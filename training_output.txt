ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-02 02:23:49 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-02 02:23:52 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_02_2025_0223_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-02 02:23:52,690 INFO: Initializing external client
2025-12-02 02:23:52,690 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-02 02:23:53,700 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.21s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9902 records
ğŸ“Š Shape: (9902, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9897 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.7504
ğŸŒ² OOB Score: 0.9977
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.0354
RMSE                : 28.7163
R2                  : 0.8955
MAPE                : 0.0237
Explained Variance  : 0.9009
ğŸ† New best model found! RÂ² = 0.8955

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5628
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4598
RMSE                : 3.1520
R2                  : 0.9988
MAPE                : 0.0225
Explained Variance  : 0.9988
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.7983
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5760
RMSE                : 3.1124
R2                  : 0.9794
MAPE                : 0.0191
Explained Variance  : 0.9808

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8871
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5772
RMSE                : 2.0540
R2                  : 0.9986
MAPE                : 0.0215
Explained Variance  : 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-01 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 124.5 - 287.9 AQI
ğŸ“ˆ Forecast average: 210.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 12_01_2025_0226_pkt
âœ… Loaded LightGBM forecasts from 12_01_2025_0226_pkt
ğŸ“Š Reference mean from 2 models: 156.07
ğŸ“ˆ Random Forest forecast mean: 210.03
ğŸ“Š Reference mean: 156.07
ğŸ“Š Ratio (RF/Reference): 1.35
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_02_2025_0223_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 1.4598
ğŸ“ˆ Best MAPE: 0.0225

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.4597942952305756, 'RMSE': 3.1520313036159124, 'R2': 0.9987919278943981, 'MAPE': 0.022481838636990072, 'Explained Variance': 0.9988373765633802}, 'randomforest_additional/12_02_2025_0223_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-02 02:24:42 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-02 02:24:42,357 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-02 02:24:42,359 INFO: Initializing external client
2025-12-02 02:24:42,360 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-02 02:24:43,374 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.95s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9902 records
ğŸ“Š Shape: (9902, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9897 records
ğŸ“Š Final dataset shape: (9897, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1585 samples
   Val:   396 samples
   Test:  1979 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 30.6475
[200]	valid_0's l2: 29.0395
[300]	valid_0's l2: 28.7219
[400]	valid_0's l2: 28.4666
Early stopping, best iteration is:
[356]	valid_0's l2: 28.4443
âœ… Model trained with 356 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.3725
RMSE                : 27.2454
R2                  : 0.9059
MAPE                : 0.0369
Explained Variance  : 0.9116
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.43617266343635
Prediction_local [164.26555016]
Right: 170.97794742431134

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3168 samples
   Val:   792 samples
   Test:  1979 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.5849
[200]	valid_0's l2: 16.0939
Early stopping, best iteration is:
[240]	valid_0's l2: 15.9884
âœ… Model trained with 240 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1120
RMSE                : 3.8870
R2                  : 0.9982
MAPE                : 0.0269
Explained Variance  : 0.9982
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4752 samples
   Val:   1187 samples
   Test:  1979 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 8.98179
[200]	valid_0's l2: 8.24172
Early stopping, best iteration is:
[228]	valid_0's l2: 8.1972
âœ… Model trained with 228 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3525
RMSE                : 2.2412
R2                  : 0.9893
MAPE                : 0.0206
Explained Variance  : 0.9893
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6335 samples
   Val:   1583 samples
   Test:  1979 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.31146
[200]	valid_0's l2: 2.17708
[300]	valid_0's l2: 2.14301
Early stopping, best iteration is:
[255]	valid_0's l2: 2.14265
âœ… Model trained with 255 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4977
RMSE                : 2.8811
R2                  : 0.9972
MAPE                : 0.0377
Explained Variance  : 0.9973
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.2412

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 73.0 - 129.0 AQI
ğŸ“Š Forecast average: 114.3 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-01 22:00:00+00:00 to 2025-12-04 21:00:00+00:00
Forecast range: 73.0 - 129.0 AQI
Average forecast: 114.3 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_02_2025_0225_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_02_2025_0225_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0837 (avg)
RMSE                : 9.0637 (avg)
R2                  : 0.9727 (avg)
MAPE                : 0.0305 (avg)
Explained Variance  : 0.9741 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-02 02:25:07 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_02_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-02 02:25:07,963 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-02 02:25:07,965 INFO: Initializing external client
2025-12-02 02:25:07,965 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-02 02:25:08,986 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.10s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9902 records
ğŸ“Š Shape: (9902, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9897 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:66.43653
[100]	validation_0-rmse:4.15925
[200]	validation_0-rmse:4.14628
[300]	validation_0-rmse:4.14543
[400]	validation_0-rmse:4.14529
[500]	validation_0-rmse:4.14527
[600]	validation_0-rmse:4.14531
[700]	validation_0-rmse:4.14525
[800]	validation_0-rmse:4.14521
[900]	validation_0-rmse:4.14522
[1000]	validation_0-rmse:4.14522
[1100]	validation_0-rmse:4.14522
[1200]	validation_0-rmse:4.14521
[1300]	validation_0-rmse:4.14521
[1400]	validation_0-rmse:4.14521
[1499]	validation_0-rmse:4.14521
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.4993
RMSE                : 27.4718
R2                  : 0.9044
MAPE                : 0.0326
Explained Variance  : 0.9109

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:118.81395
[100]	validation_0-rmse:3.53720
[200]	validation_0-rmse:3.23860
[300]	validation_0-rmse:3.23277
[400]	validation_0-rmse:3.23172
[500]	validation_0-rmse:3.23131
[600]	validation_0-rmse:3.23103
[700]	validation_0-rmse:3.23078
[800]	validation_0-rmse:3.23045
[900]	validation_0-rmse:3.23025
[1000]	validation_0-rmse:3.23017
[1100]	validation_0-rmse:3.23015
[1200]	validation_0-rmse:3.23010
[1300]	validation_0-rmse:3.23006
[1400]	validation_0-rmse:3.23007
[1499]	validation_0-rmse:3.23006
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3876
RMSE                : 4.3047
R2                  : 0.9977
MAPE                : 0.0310
Explained Variance  : 0.9978

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:85.10448
[100]	validation_0-rmse:3.90728
[200]	validation_0-rmse:3.58366
[300]	validation_0-rmse:3.55012
[400]	validation_0-rmse:3.54706
[500]	validation_0-rmse:3.55551
[600]	validation_0-rmse:3.55818
[700]	validation_0-rmse:3.55652
[800]	validation_0-rmse:3.55816
[900]	validation_0-rmse:3.55783
[1000]	validation_0-rmse:3.55815
[1100]	validation_0-rmse:3.55754
[1200]	validation_0-rmse:3.55730
[1300]	validation_0-rmse:3.55745
[1400]	validation_0-rmse:3.55763
[1499]	validation_0-rmse:3.55765
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0521
RMSE                : 3.1705
R2                  : 0.9786
MAPE                : 0.0307
Explained Variance  : 0.9794

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:72.54665
[100]	validation_0-rmse:1.31918
[200]	validation_0-rmse:1.20844
[300]	validation_0-rmse:1.20636
[400]	validation_0-rmse:1.20490
[500]	validation_0-rmse:1.20463
[600]	validation_0-rmse:1.20523
[700]	validation_0-rmse:1.20485
[800]	validation_0-rmse:1.20474
[900]	validation_0-rmse:1.20477
[1000]	validation_0-rmse:1.20478
[1100]	validation_0-rmse:1.20466
[1200]	validation_0-rmse:1.20482
[1300]	validation_0-rmse:1.20485
[1400]	validation_0-rmse:1.20502
[1499]	validation_0-rmse:1.20501
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1031
RMSE                : 2.6146
R2                  : 0.9977
MAPE                : 0.0329
Explained Variance  : 0.9977

ğŸ† Best model from split 2
ğŸ“Š Best metrics:
   MAE: 2.3876
   RMSE: 4.3047
   R2: 0.9977
   MAPE: 0.0310
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-01 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 63.8 - 87.2 AQI
ğŸ“Š Forecast average: 75.8 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/12_02_2025_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9977
ğŸ“Š Best MAE: 2.3876
ğŸ“ˆ Best MAPE: 0.0310

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 2.3876445293426514, 'RMSE': 4.3047149712712365, 'R2': 0.9977468252182007, 'MAPE': 0.030959200114011765, 'Explained Variance': 0.9977924823760986}, 'xgboost_additional/12_02_2025_0225_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-02 02:26:00 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-02 02:26:00,195 INFO: Closing external client and cleaning up certificates.
Connection closed.
