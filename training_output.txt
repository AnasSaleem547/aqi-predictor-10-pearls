ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-01 02:25:56 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-01 02:25:58 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_01_2026_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-01 02:25:58,991 INFO: Initializing external client
2026-01-01 02:25:58,991 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-01 02:26:00,085 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.13s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10611 records
ğŸ“Š Shape: (10611, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10606 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.2859
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.0120
RMSE                : 31.2105
R2                  : 0.8898
MAPE                : 0.0256
Explained Variance  : 0.8966
ğŸ† New best model found! RÂ² = 0.8898

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.1146
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9026
RMSE                : 3.6135
R2                  : 0.9972
MAPE                : 0.0268
Explained Variance  : 0.9974
ğŸ† New best model found! RÂ² = 0.9972

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.6198
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3036
RMSE                : 2.7490
R2                  : 0.9863
MAPE                : 0.0578
Explained Variance  : 0.9870

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2697
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4435
RMSE                : 2.5089
R2                  : 0.9979
MAPE                : 0.0054
Explained Variance  : 0.9979
ğŸ† New best model found! RÂ² = 0.9979

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-31 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 76.0 - 295.1 AQI
ğŸ“ˆ Forecast average: 209.8 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 209.79
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.70
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.589
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 86.20 AQI (41.1%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_01_2026_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9979
ğŸ“Š Best MAE: 0.4435
ğŸ“ˆ Best MAPE: 0.0054

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.44348790222167433, 'RMSE': 2.5088669407652247, 'R2': 0.9978931403151706, 'MAPE': 0.005368161993408881, 'Explained Variance': 0.9979294866147324}, 'randomforest_additional/01_01_2026_0225_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-01 02:27:07 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-01 02:27:07,249 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-01 02:27:07,251 INFO: Initializing external client
2026-01-01 02:27:07,251 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-01 02:27:08,142 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.08s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10611 records
ğŸ“Š Shape: (10611, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10606 records
ğŸ“Š Final dataset shape: (10606, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1698 samples
   Val:   424 samples
   Test:  2121 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 216.179
[200]	valid_0's l2: 194.671
[300]	valid_0's l2: 186.535
[400]	valid_0's l2: 182.399
[500]	valid_0's l2: 181.159
Early stopping, best iteration is:
[454]	valid_0's l2: 181.145
âœ… Model trained with 454 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.8485
RMSE                : 28.9729
R2                  : 0.9050
MAPE                : 0.0372
Explained Variance  : 0.9116
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 67.83624233382747
Prediction_local [171.08592208]
Right: 264.66032017216764

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3395 samples
   Val:   848 samples
   Test:  2121 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.2656
[200]	valid_0's l2: 16.0963
Early stopping, best iteration is:
[211]	valid_0's l2: 16.0573
âœ… Model trained with 211 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9659
RMSE                : 3.4987
R2                  : 0.9974
MAPE                : 0.0273
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5092 samples
   Val:   1272 samples
   Test:  2121 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.64416
Early stopping, best iteration is:
[76]	valid_0's l2: 8.74929
âœ… Model trained with 76 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9357
RMSE                : 3.4429
R2                  : 0.9785
MAPE                : 0.0832
Explained Variance  : 0.9812
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6788 samples
   Val:   1697 samples
   Test:  2121 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.4245
[200]	valid_0's l2: 3.26419
Early stopping, best iteration is:
[243]	valid_0's l2: 3.19287
âœ… Model trained with 243 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6197
RMSE                : 3.6982
R2                  : 0.9954
MAPE                : 0.0157
Explained Variance  : 0.9956
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.4429

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 40.3 - 55.1 AQI
ğŸ“Š Forecast average: 53.8 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-31 22:00:00+00:00 to 2026-01-03 21:00:00+00:00
Forecast range: 40.3 - 55.1 AQI
Average forecast: 53.8 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_01_2026_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_01_2026_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3424 (avg)
RMSE                : 9.9032 (avg)
R2                  : 0.9691 (avg)
MAPE                : 0.0409 (avg)
Explained Variance  : 0.9714 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-01 02:27:32 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_01_2026_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-01 02:27:32,296 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-01 02:27:32,299 INFO: Initializing external client
2026-01-01 02:27:32,299 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-01 02:27:33,252 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.03s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10611 records
ğŸ“Š Shape: (10611, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10606 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:91.30345
[100]	validation_0-rmse:13.07331
[200]	validation_0-rmse:12.86514
[300]	validation_0-rmse:12.86218
[400]	validation_0-rmse:12.86152
[500]	validation_0-rmse:12.86163
[600]	validation_0-rmse:12.86123
[700]	validation_0-rmse:12.86117
[800]	validation_0-rmse:12.86111
[900]	validation_0-rmse:12.86116
[1000]	validation_0-rmse:12.86118
[1100]	validation_0-rmse:12.86116
[1200]	validation_0-rmse:12.86117
[1300]	validation_0-rmse:12.86116
[1400]	validation_0-rmse:12.86116
[1499]	validation_0-rmse:12.86118
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.7105
RMSE                : 28.6103
R2                  : 0.9074
MAPE                : 0.0326
Explained Variance  : 0.9148

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:116.20176
[100]	validation_0-rmse:3.57909
[200]	validation_0-rmse:3.36686
[300]	validation_0-rmse:3.36359
[400]	validation_0-rmse:3.36468
[500]	validation_0-rmse:3.36385
[600]	validation_0-rmse:3.36368
[700]	validation_0-rmse:3.36348
[800]	validation_0-rmse:3.36364
[900]	validation_0-rmse:3.36366
[1000]	validation_0-rmse:3.36371
[1100]	validation_0-rmse:3.36371
[1200]	validation_0-rmse:3.36367
[1300]	validation_0-rmse:3.36364
[1400]	validation_0-rmse:3.36363
[1499]	validation_0-rmse:3.36363
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4795
RMSE                : 4.1713
R2                  : 0.9963
MAPE                : 0.0350
Explained Variance  : 0.9964

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.77641
[100]	validation_0-rmse:3.66695
[200]	validation_0-rmse:3.43670
[300]	validation_0-rmse:3.39644
[400]	validation_0-rmse:3.39438
[500]	validation_0-rmse:3.38806
[600]	validation_0-rmse:3.38570
[700]	validation_0-rmse:3.38217
[800]	validation_0-rmse:3.37855
[900]	validation_0-rmse:3.38002
[1000]	validation_0-rmse:3.37821
[1100]	validation_0-rmse:3.37851
[1200]	validation_0-rmse:3.37825
[1300]	validation_0-rmse:3.37764
[1400]	validation_0-rmse:3.37776
[1499]	validation_0-rmse:3.37753
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9875
RMSE                : 3.1549
R2                  : 0.9820
MAPE                : 0.0671
Explained Variance  : 0.9833

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.29721
[100]	validation_0-rmse:1.86598
[200]	validation_0-rmse:1.73094
[300]	validation_0-rmse:1.72728
[400]	validation_0-rmse:1.72404
[500]	validation_0-rmse:1.72655
[600]	validation_0-rmse:1.72646
[700]	validation_0-rmse:1.72580
[800]	validation_0-rmse:1.72543
[900]	validation_0-rmse:1.72658
[1000]	validation_0-rmse:1.72678
[1100]	validation_0-rmse:1.72828
[1200]	validation_0-rmse:1.72861
[1300]	validation_0-rmse:1.72861
[1400]	validation_0-rmse:1.72911
[1499]	validation_0-rmse:1.72913
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1472
RMSE                : 3.0733
R2                  : 0.9968
MAPE                : 0.0125
Explained Variance  : 0.9969

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1472
   RMSE: 3.0733
   R2: 0.9968
   MAPE: 0.0125
   Explained Variance: 0.9969

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-31 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-03 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 32.7 - 50.6 AQI
ğŸ“Š Forecast average: 40.7 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-01 02:28:33 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-01 02:28:33,961 INFO: Closing external client and cleaning up certificates.
Connection closed.
