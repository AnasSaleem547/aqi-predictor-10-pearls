ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-31 02:26:55 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-31 02:26:59 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_31_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-31 02:26:59,227 INFO: Initializing external client
2025-12-31 02:26:59,227 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-31 02:26:59,971 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.96s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10588 records
ğŸ“Š Shape: (10588, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10583 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.2850
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.6136
RMSE                : 30.2757
R2                  : 0.8937
MAPE                : 0.0248
Explained Variance  : 0.8998
ğŸ† New best model found! RÂ² = 0.8937

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.0825
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8856
RMSE                : 3.6026
R2                  : 0.9975
MAPE                : 0.0266
Explained Variance  : 0.9977
ğŸ† New best model found! RÂ² = 0.9975

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.5958
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3110
RMSE                : 2.7607
R2                  : 0.9864
MAPE                : 0.0570
Explained Variance  : 0.9871

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2678
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4398
RMSE                : 2.5000
R2                  : 0.9979
MAPE                : 0.0055
Explained Variance  : 0.9980
ğŸ† New best model found! RÂ² = 0.9979

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-30 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 153.2 - 324.9 AQI
ğŸ“ˆ Forecast average: 226.1 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_29_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 159.86
ğŸ“ˆ Random Forest forecast mean: 226.10
ğŸ“Š Reference mean: 159.86
ğŸ“Š Ratio (RF/Reference): 1.41
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_31_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9979
ğŸ“Š Best MAE: 0.4398
ğŸ“ˆ Best MAPE: 0.0055

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.43975688620217745, 'RMSE': 2.499990725930653, 'R2': 0.9979253295989662, 'MAPE': 0.0054566592819772055, 'Explained Variance': 0.9979600697678986}, 'randomforest_additional/12_31_2025_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-31 02:28:09 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-31 02:28:09,896 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-31 02:28:09,898 INFO: Initializing external client
2025-12-31 02:28:09,898 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-31 02:28:10,476 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.90s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10588 records
ğŸ“Š Shape: (10588, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10583 records
ğŸ“Š Final dataset shape: (10583, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1696 samples
   Val:   423 samples
   Test:  2116 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 218.98
[200]	valid_0's l2: 195.518
[300]	valid_0's l2: 186.866
[400]	valid_0's l2: 182.856
[500]	valid_0's l2: 181.66
Early stopping, best iteration is:
[485]	valid_0's l2: 181.66
âœ… Model trained with 485 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.3345
RMSE                : 27.8754
R2                  : 0.9099
MAPE                : 0.0361
Explained Variance  : 0.9152
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 67.62887076275516
Prediction_local [169.75188673]
Right: 184.71764771688143

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3388 samples
   Val:   847 samples
   Test:  2116 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.5214
Early stopping, best iteration is:
[140]	valid_0's l2: 16.4301
âœ… Model trained with 140 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9653
RMSE                : 3.4633
R2                  : 0.9977
MAPE                : 0.0262
Explained Variance  : 0.9977
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5081 samples
   Val:   1270 samples
   Test:  2116 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 8.06191
Early stopping, best iteration is:
[79]	valid_0's l2: 6.99345
âœ… Model trained with 79 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8202
RMSE                : 3.3608
R2                  : 0.9798
MAPE                : 0.0814
Explained Variance  : 0.9811
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6774 samples
   Val:   1693 samples
   Test:  2116 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.27294
[200]	valid_0's l2: 3.17403
Early stopping, best iteration is:
[241]	valid_0's l2: 3.1249
âœ… Model trained with 241 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8156
RMSE                : 3.8874
R2                  : 0.9950
MAPE                : 0.0170
Explained Variance  : 0.9952
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.3608

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 96.3 - 135.8 AQI
ğŸ“Š Forecast average: 123.6 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-30 22:00:00+00:00 to 2026-01-02 21:00:00+00:00
Forecast range: 96.3 - 135.8 AQI
Average forecast: 123.6 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_31_2025_0228_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_31_2025_0228_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2339 (avg)
RMSE                : 9.6467 (avg)
R2                  : 0.9706 (avg)
MAPE                : 0.0402 (avg)
Explained Variance  : 0.9723 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-31 02:28:32 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_31_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-31 02:28:32,983 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-31 02:28:32,985 INFO: Initializing external client
2025-12-31 02:28:32,985 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-31 02:28:33,554 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10588 records
ğŸ“Š Shape: (10588, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10583 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:91.09638
[100]	validation_0-rmse:13.08355
[200]	validation_0-rmse:12.87530
[300]	validation_0-rmse:12.87120
[400]	validation_0-rmse:12.87003
[500]	validation_0-rmse:12.86908
[600]	validation_0-rmse:12.86926
[700]	validation_0-rmse:12.86918
[800]	validation_0-rmse:12.86914
[900]	validation_0-rmse:12.86916
[1000]	validation_0-rmse:12.86917
[1100]	validation_0-rmse:12.86914
[1200]	validation_0-rmse:12.86918
[1300]	validation_0-rmse:12.86916
[1400]	validation_0-rmse:12.86916
[1499]	validation_0-rmse:12.86916
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.4132
RMSE                : 27.9149
R2                  : 0.9096
MAPE                : 0.0323
Explained Variance  : 0.9163

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:112.76354
[100]	validation_0-rmse:3.31399
[200]	validation_0-rmse:3.12042
[300]	validation_0-rmse:3.11967
[400]	validation_0-rmse:3.11966
[500]	validation_0-rmse:3.11984
[600]	validation_0-rmse:3.11990
[700]	validation_0-rmse:3.11995
[800]	validation_0-rmse:3.12018
[900]	validation_0-rmse:3.12025
[1000]	validation_0-rmse:3.12029
[1100]	validation_0-rmse:3.12036
[1200]	validation_0-rmse:3.12036
[1300]	validation_0-rmse:3.12036
[1400]	validation_0-rmse:3.12039
[1499]	validation_0-rmse:3.12041
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6431
RMSE                : 4.3099
R2                  : 0.9964
MAPE                : 0.0360
Explained Variance  : 0.9966

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.26169
[100]	validation_0-rmse:3.91792
[200]	validation_0-rmse:3.73609
[300]	validation_0-rmse:3.70015
[400]	validation_0-rmse:3.69648
[500]	validation_0-rmse:3.69421
[600]	validation_0-rmse:3.69238
[700]	validation_0-rmse:3.69662
[800]	validation_0-rmse:3.69434
[900]	validation_0-rmse:3.69324
[1000]	validation_0-rmse:3.69441
[1100]	validation_0-rmse:3.69394
[1200]	validation_0-rmse:3.69451
[1300]	validation_0-rmse:3.69382
[1400]	validation_0-rmse:3.69380
[1499]	validation_0-rmse:3.69395
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2577
RMSE                : 3.4186
R2                  : 0.9791
MAPE                : 0.0713
Explained Variance  : 0.9794

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.88805
[100]	validation_0-rmse:1.84940
[200]	validation_0-rmse:1.72098
[300]	validation_0-rmse:1.71572
[400]	validation_0-rmse:1.71719
[500]	validation_0-rmse:1.71971
[600]	validation_0-rmse:1.72135
[700]	validation_0-rmse:1.72043
[800]	validation_0-rmse:1.72027
[900]	validation_0-rmse:1.72038
[1000]	validation_0-rmse:1.72038
[1100]	validation_0-rmse:1.72078
[1200]	validation_0-rmse:1.72019
[1300]	validation_0-rmse:1.72026
[1400]	validation_0-rmse:1.72055
[1499]	validation_0-rmse:1.72050
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1159
RMSE                : 3.0476
R2                  : 0.9969
MAPE                : 0.0123
Explained Variance  : 0.9970

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1159
   RMSE: 3.0476
   R2: 0.9969
   MAPE: 0.0123
   Explained Variance: 0.9970

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-30 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-31 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-01 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-02 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 58.8 - 84.9 AQI
ğŸ“Š Forecast average: 72.3 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-31 02:29:34 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-31 02:29:34,120 INFO: Closing external client and cleaning up certificates.
Connection closed.
