ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-12 02:26:09 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-12 02:26:12 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_12_2026_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-12 02:26:12,926 INFO: Initializing external client
2026-01-12 02:26:12,926 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-12 02:26:13,561 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.66s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10874 records
ğŸ“Š Shape: (10874, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10869 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 13.7589
ğŸŒ² OOB Score: 0.9981
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.0449
RMSE                : 31.5209
R2                  : 0.8924
MAPE                : 0.0253
Explained Variance  : 0.8995
ğŸ† New best model found! RÂ² = 0.8924

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.1024
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9903
RMSE                : 3.6683
R2                  : 0.9963
MAPE                : 0.0277
Explained Variance  : 0.9966
ğŸ† New best model found! RÂ² = 0.9963

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.4785
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0593
RMSE                : 2.0626
R2                  : 0.9915
MAPE                : 0.0397
Explained Variance  : 0.9918

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.1401
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.3960
RMSE                : 2.5015
R2                  : 0.9972
MAPE                : 0.0041
Explained Variance  : 0.9973
ğŸ† New best model found! RÂ² = 0.9972

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2026-01-11 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 159.0 - 283.7 AQI
ğŸ“ˆ Forecast average: 199.5 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 199.46
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.61
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.620
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 75.87 AQI (38.0%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_12_2026_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9972
ğŸ“Š Best MAE: 0.3960
ğŸ“ˆ Best MAPE: 0.0041

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.39597941122639807, 'RMSE': 2.501451844939077, 'R2': 0.9972402809943703, 'MAPE': 0.004149783447363737, 'Explained Variance': 0.9972937298565229}, 'randomforest_additional/01_12_2026_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-12 02:27:23 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-12 02:27:23,778 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-12 02:27:23,781 INFO: Initializing external client
2026-01-12 02:27:23,781 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-12 02:27:24,418 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.65s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10874 records
ğŸ“Š Shape: (10874, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10869 records
ğŸ“Š Final dataset shape: (10869, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1742 samples
   Val:   435 samples
   Test:  2173 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 202.493
[200]	valid_0's l2: 177.024
[300]	valid_0's l2: 169.07
[400]	valid_0's l2: 164
[500]	valid_0's l2: 161.351
Early stopping, best iteration is:
[502]	valid_0's l2: 161.345
âœ… Model trained with 502 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 12.0631
RMSE                : 29.9432
R2                  : 0.9029
MAPE                : 0.0375
Explained Variance  : 0.9091
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 94.08596126468794
Prediction_local [95.24098506]
Right: 87.12094544515887

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3480 samples
   Val:   870 samples
   Test:  2173 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 15.5859
Early stopping, best iteration is:
[142]	valid_0's l2: 15.3416
âœ… Model trained with 142 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1569
RMSE                : 3.6383
R2                  : 0.9964
MAPE                : 0.0296
Explained Variance  : 0.9965
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5219 samples
   Val:   1304 samples
   Test:  2173 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 5.73342
[200]	valid_0's l2: 5.58105
Early stopping, best iteration is:
[246]	valid_0's l2: 5.52
âœ… Model trained with 246 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5158
RMSE                : 2.6117
R2                  : 0.9863
MAPE                : 0.0620
Explained Variance  : 0.9864
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6957 samples
   Val:   1739 samples
   Test:  2173 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.42446
[200]	valid_0's l2: 3.18849
[300]	valid_0's l2: 3.14349
Early stopping, best iteration is:
[252]	valid_0's l2: 3.14253
âœ… Model trained with 252 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6652
RMSE                : 3.6740
R2                  : 0.9940
MAPE                : 0.0146
Explained Variance  : 0.9942
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.6117

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 92.1 - 154.3 AQI
ğŸ“Š Forecast average: 116.4 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2026-01-11 22:00:00+00:00 to 2026-01-14 21:00:00+00:00
Forecast range: 92.1 - 154.3 AQI
Average forecast: 116.4 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_12_2026_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_12_2026_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3503 (avg)
RMSE                : 9.9668 (avg)
R2                  : 0.9699 (avg)
MAPE                : 0.0359 (avg)
Explained Variance  : 0.9715 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-12 02:27:49 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_12_2026_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-12 02:27:49,032 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-12 02:27:49,035 INFO: Initializing external client
2026-01-12 02:27:49,035 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-12 02:27:49,803 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.62s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10874 records
ğŸ“Š Shape: (10874, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10869 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:88.14878
[100]	validation_0-rmse:12.61765
[200]	validation_0-rmse:12.39034
[300]	validation_0-rmse:12.39034
[400]	validation_0-rmse:12.38955
[500]	validation_0-rmse:12.38906
[600]	validation_0-rmse:12.38876
[700]	validation_0-rmse:12.38851
[800]	validation_0-rmse:12.38837
[900]	validation_0-rmse:12.38831
[1000]	validation_0-rmse:12.38828
[1100]	validation_0-rmse:12.38826
[1200]	validation_0-rmse:12.38826
[1300]	validation_0-rmse:12.38826
[1400]	validation_0-rmse:12.38826
[1499]	validation_0-rmse:12.38826
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.8034
RMSE                : 28.7319
R2                  : 0.9106
MAPE                : 0.0329
Explained Variance  : 0.9168

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:116.34716
[100]	validation_0-rmse:3.46060
[200]	validation_0-rmse:3.28814
[300]	validation_0-rmse:3.28661
[400]	validation_0-rmse:3.28602
[500]	validation_0-rmse:3.28606
[600]	validation_0-rmse:3.28618
[700]	validation_0-rmse:3.28658
[800]	validation_0-rmse:3.28666
[900]	validation_0-rmse:3.28682
[1000]	validation_0-rmse:3.28707
[1100]	validation_0-rmse:3.28721
[1200]	validation_0-rmse:3.28729
[1300]	validation_0-rmse:3.28730
[1400]	validation_0-rmse:3.28735
[1499]	validation_0-rmse:3.28735
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6824
RMSE                : 4.3628
R2                  : 0.9948
MAPE                : 0.0377
Explained Variance  : 0.9951

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:77.33896
[100]	validation_0-rmse:3.36695
[200]	validation_0-rmse:3.20798
[300]	validation_0-rmse:3.21223
[400]	validation_0-rmse:3.21158
[500]	validation_0-rmse:3.21467
[600]	validation_0-rmse:3.21602
[700]	validation_0-rmse:3.21751
[800]	validation_0-rmse:3.21760
[900]	validation_0-rmse:3.21846
[1000]	validation_0-rmse:3.21758
[1100]	validation_0-rmse:3.21731
[1200]	validation_0-rmse:3.21656
[1300]	validation_0-rmse:3.21569
[1400]	validation_0-rmse:3.21558
[1499]	validation_0-rmse:3.21536
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6785
RMSE                : 2.8761
R2                  : 0.9834
MAPE                : 0.0644
Explained Variance  : 0.9839

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:86.48701
[100]	validation_0-rmse:1.78981
[200]	validation_0-rmse:1.61634
[300]	validation_0-rmse:1.61385
[400]	validation_0-rmse:1.60945
[500]	validation_0-rmse:1.61035
[600]	validation_0-rmse:1.60867
[700]	validation_0-rmse:1.60979
[800]	validation_0-rmse:1.61182
[900]	validation_0-rmse:1.61103
[1000]	validation_0-rmse:1.61164
[1100]	validation_0-rmse:1.61136
[1200]	validation_0-rmse:1.61172
[1300]	validation_0-rmse:1.61154
[1400]	validation_0-rmse:1.61180
[1499]	validation_0-rmse:1.61188
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4830
RMSE                : 3.3965
R2                  : 0.9949
MAPE                : 0.0140
Explained Variance  : 0.9952

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.4830
   RMSE: 3.3965
   R2: 0.9949
   MAPE: 0.0140
   Explained Variance: 0.9952

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2026-01-11 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-13 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-14 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 104.4 - 130.0 AQI
ğŸ“Š Forecast average: 117.1 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/01_12_2026_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9949
ğŸ“Š Best MAE: 1.4830
ğŸ“ˆ Best MAPE: 0.0140

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 1.482985258102417, 'RMSE': 3.396526632545133, 'R2': 0.9949119687080383, 'MAPE': 0.014042844995856285, 'Explained Variance': 0.9951927661895752}, 'xgboost_additional/01_12_2026_0227_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-12 02:28:50 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-12 02:28:50,704 INFO: Closing external client and cleaning up certificates.
Connection closed.
