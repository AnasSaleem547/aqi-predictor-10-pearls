ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-18 02:27:51 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-18 02:27:54 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_18_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-18 02:27:54,469 INFO: Initializing external client
2025-12-18 02:27:54,469 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-18 02:27:55,200 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.72s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10281 records
ğŸ“Š Shape: (10281, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10276 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.9207
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 8.9760
RMSE                : 28.8532
R2                  : 0.8943
MAPE                : 0.0232
Explained Variance  : 0.9000
ğŸ† New best model found! RÂ² = 0.8943

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5057
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8799
RMSE                : 3.7031
R2                  : 0.9981
MAPE                : 0.0265
Explained Variance  : 0.9982
ğŸ† New best model found! RÂ² = 0.9981

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.6099
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5004
RMSE                : 3.0312
R2                  : 0.9834
MAPE                : 0.0501
Explained Variance  : 0.9848

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.1793
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4555
RMSE                : 1.9329
R2                  : 0.9988
MAPE                : 0.0090
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-17 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 181.7 - 287.3 AQI
ğŸ“ˆ Forecast average: 206.2 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_17_2025_0230_pkt
ğŸ“Š Reference mean from 1 models: 245.38
ğŸ“ˆ Random Forest forecast mean: 206.25
ğŸ“Š Reference mean: 245.38
ğŸ“Š Ratio (RF/Reference): 0.84
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_18_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 0.4555
ğŸ“ˆ Best MAPE: 0.0090

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.455530780275417, 'RMSE': 1.9329355735014317, 'R2': 0.998823103079201, 'MAPE': 0.009020635401200952, 'Explained Variance': 0.9988500279734842}, 'randomforest_additional/12_18_2025_0227_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-18 02:29:01 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-18 02:29:01,709 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-18 02:29:01,711 INFO: Initializing external client
2025-12-18 02:29:01,711 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-18 02:29:02,586 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.60s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10281 records
ğŸ“Š Shape: (10281, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10276 records
ğŸ“Š Final dataset shape: (10276, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1645 samples
   Val:   411 samples
   Test:  2055 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 23.2379
Early stopping, best iteration is:
[88]	valid_0's l2: 23.1139
âœ… Model trained with 88 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.5358
RMSE                : 29.7490
R2                  : 0.8877
MAPE                : 0.0362
Explained Variance  : 0.8943
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.81758953437001
Prediction_local [174.94967708]
Right: 332.3940780596434

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3289 samples
   Val:   822 samples
   Test:  2055 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 14.6041
[200]	valid_0's l2: 14.1384
[300]	valid_0's l2: 14.1217
Early stopping, best iteration is:
[255]	valid_0's l2: 14.1015
âœ… Model trained with 255 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1421
RMSE                : 3.9456
R2                  : 0.9978
MAPE                : 0.0271
Explained Variance  : 0.9978
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4933 samples
   Val:   1233 samples
   Test:  2055 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.7305
[200]	valid_0's l2: 12.5921
Early stopping, best iteration is:
[203]	valid_0's l2: 12.5343
âœ… Model trained with 203 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7731
RMSE                : 3.1540
R2                  : 0.9820
MAPE                : 0.0669
Explained Variance  : 0.9829
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6577 samples
   Val:   1644 samples
   Test:  2055 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.22735
[200]	valid_0's l2: 3.08824
Early stopping, best iteration is:
[235]	valid_0's l2: 3.0722
âœ… Model trained with 235 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3977
RMSE                : 2.9555
R2                  : 0.9972
MAPE                : 0.0195
Explained Variance  : 0.9973
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 2.9555

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 170.4 - 346.6 AQI
ğŸ“Š Forecast average: 260.1 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-17 22:00:00+00:00 to 2025-12-20 21:00:00+00:00
Forecast range: 170.4 - 346.6 AQI
Average forecast: 260.1 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_18_2025_0229_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_18_2025_0229_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2122 (avg)
RMSE                : 9.9510 (avg)
R2                  : 0.9662 (avg)
MAPE                : 0.0374 (avg)
Explained Variance  : 0.9681 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-18 02:29:25 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_18_2025_0229_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-18 02:29:25,800 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-18 02:29:25,802 INFO: Initializing external client
2025-12-18 02:29:25,802 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-18 02:29:26,483 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.60s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10281 records
ğŸ“Š Shape: (10281, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10276 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:68.39472
[100]	validation_0-rmse:3.53185
[200]	validation_0-rmse:3.40781
[300]	validation_0-rmse:3.40491
[400]	validation_0-rmse:3.40458
[500]	validation_0-rmse:3.40403
[600]	validation_0-rmse:3.40376
[700]	validation_0-rmse:3.40370
[800]	validation_0-rmse:3.40371
[900]	validation_0-rmse:3.40369
[1000]	validation_0-rmse:3.40368
[1100]	validation_0-rmse:3.40368
[1200]	validation_0-rmse:3.40368
[1300]	validation_0-rmse:3.40368
[1400]	validation_0-rmse:3.40368
[1499]	validation_0-rmse:3.40369
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.9330
RMSE                : 26.7526
R2                  : 0.9092
MAPE                : 0.0310
Explained Variance  : 0.9153

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:114.95866
[100]	validation_0-rmse:3.25664
[200]	validation_0-rmse:2.91703
[300]	validation_0-rmse:2.91366
[400]	validation_0-rmse:2.91367
[500]	validation_0-rmse:2.91303
[600]	validation_0-rmse:2.91316
[700]	validation_0-rmse:2.91292
[800]	validation_0-rmse:2.91298
[900]	validation_0-rmse:2.91300
[1000]	validation_0-rmse:2.91297
[1100]	validation_0-rmse:2.91297
[1200]	validation_0-rmse:2.91300
[1300]	validation_0-rmse:2.91305
[1400]	validation_0-rmse:2.91307
[1499]	validation_0-rmse:2.91308
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6585
RMSE                : 4.5153
R2                  : 0.9971
MAPE                : 0.0350
Explained Variance  : 0.9973

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.71788
[100]	validation_0-rmse:4.57928
[200]	validation_0-rmse:4.33206
[300]	validation_0-rmse:4.32141
[400]	validation_0-rmse:4.31131
[500]	validation_0-rmse:4.30908
[600]	validation_0-rmse:4.30893
[700]	validation_0-rmse:4.30962
[800]	validation_0-rmse:4.30691
[900]	validation_0-rmse:4.30473
[1000]	validation_0-rmse:4.30432
[1100]	validation_0-rmse:4.30404
[1200]	validation_0-rmse:4.30433
[1300]	validation_0-rmse:4.30429
[1400]	validation_0-rmse:4.30443
[1499]	validation_0-rmse:4.30456
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2696
RMSE                : 3.5782
R2                  : 0.9768
MAPE                : 0.0639
Explained Variance  : 0.9777

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:80.85750
[100]	validation_0-rmse:1.72331
[200]	validation_0-rmse:1.58443
[300]	validation_0-rmse:1.58252
[400]	validation_0-rmse:1.57998
[500]	validation_0-rmse:1.57996
[600]	validation_0-rmse:1.57927
[700]	validation_0-rmse:1.57823
[800]	validation_0-rmse:1.57744
[900]	validation_0-rmse:1.57751
[1000]	validation_0-rmse:1.57708
[1100]	validation_0-rmse:1.57683
[1200]	validation_0-rmse:1.57725
[1300]	validation_0-rmse:1.57774
[1400]	validation_0-rmse:1.57771
[1499]	validation_0-rmse:1.57781
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1830
RMSE                : 2.6748
R2                  : 0.9977
MAPE                : 0.0179
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1830
   RMSE: 2.6748
   R2: 0.9977
   MAPE: 0.0179
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-17 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 189.8 - 232.7 AQI
ğŸ“Š Forecast average: 206.8 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-18 02:30:24 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-18 02:30:24,806 INFO: Closing external client and cleaning up certificates.
Connection closed.
