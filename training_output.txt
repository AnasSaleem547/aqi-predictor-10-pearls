ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-20 02:26:08 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-20 02:26:10 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_20_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-20 02:26:10,952 INFO: Initializing external client
2025-12-20 02:26:10,953 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-20 02:26:11,609 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.64s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10328 records
ğŸ“Š Shape: (10328, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10323 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 10.9630
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.4021
RMSE                : 29.7633
R2                  : 0.8908
MAPE                : 0.0241
Explained Variance  : 0.8970
ğŸ† New best model found! RÂ² = 0.8908

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5095
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8625
RMSE                : 3.6844
R2                  : 0.9978
MAPE                : 0.0265
Explained Variance  : 0.9980
ğŸ† New best model found! RÂ² = 0.9978

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5569
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5951
RMSE                : 3.1496
R2                  : 0.9830
MAPE                : 0.0596
Explained Variance  : 0.9849

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2447
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4398
RMSE                : 1.9048
R2                  : 0.9989
MAPE                : 0.0060
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9989

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-19 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 165.0 - 310.8 AQI
ğŸ“ˆ Forecast average: 195.9 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_18_2025_0229_pkt
ğŸ“Š Reference mean from 1 models: 260.14
ğŸ“ˆ Random Forest forecast mean: 195.88
ğŸ“Š Reference mean: 260.14
ğŸ“Š Ratio (RF/Reference): 0.75
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_20_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9989
ğŸ“Š Best MAE: 0.4398
ğŸ“ˆ Best MAPE: 0.0060

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.4398419991950924, 'RMSE': 1.9047676465663939, 'R2': 0.9989136732576871, 'MAPE': 0.005969424573422367, 'Explained Variance': 0.9989365723913884}, 'randomforest_additional/12_20_2025_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-20 02:27:18 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-20 02:27:18,223 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-20 02:27:18,226 INFO: Initializing external client
2025-12-20 02:27:18,226 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-20 02:27:18,767 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.99s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10328 records
ğŸ“Š Shape: (10328, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10323 records
ğŸ“Š Final dataset shape: (10323, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1654 samples
   Val:   413 samples
   Test:  2064 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 130.747
[200]	valid_0's l2: 116.672
[300]	valid_0's l2: 112.208
[400]	valid_0's l2: 109.371
[500]	valid_0's l2: 108.376
Early stopping, best iteration is:
[490]	valid_0's l2: 108.376
âœ… Model trained with 487 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.3627
RMSE                : 27.5371
R2                  : 0.9065
MAPE                : 0.0364
Explained Variance  : 0.9113
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.68665909939956
Prediction_local [175.76630761]
Right: 366.99619945994976

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3305 samples
   Val:   826 samples
   Test:  2064 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.0591
[200]	valid_0's l2: 15.7302
Early stopping, best iteration is:
[184]	valid_0's l2: 15.6794
âœ… Model trained with 184 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2220
RMSE                : 4.0781
R2                  : 0.9974
MAPE                : 0.0278
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4956 samples
   Val:   1239 samples
   Test:  2064 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.0138
[200]	valid_0's l2: 12.0754
Early stopping, best iteration is:
[176]	valid_0's l2: 11.8976
âœ… Model trained with 176 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7794
RMSE                : 3.2353
R2                  : 0.9821
MAPE                : 0.0742
Explained Variance  : 0.9832
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6608 samples
   Val:   1651 samples
   Test:  2064 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.15866
[200]	valid_0's l2: 2.9509
Early stopping, best iteration is:
[209]	valid_0's l2: 2.93676
âœ… Model trained with 209 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7379
RMSE                : 3.6044
R2                  : 0.9961
MAPE                : 0.0173
Explained Variance  : 0.9964
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.2353

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 175.8 - 322.9 AQI
ğŸ“Š Forecast average: 231.1 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-19 22:00:00+00:00 to 2025-12-22 21:00:00+00:00
Forecast range: 175.8 - 322.9 AQI
Average forecast: 231.1 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_20_2025_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_20_2025_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2755 (avg)
RMSE                : 9.6137 (avg)
R2                  : 0.9705 (avg)
MAPE                : 0.0389 (avg)
Explained Variance  : 0.9721 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-20 02:27:42 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_20_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-20 02:27:42,235 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-20 02:27:42,237 INFO: Initializing external client
2025-12-20 02:27:42,237 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-20 02:27:42,796 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.86s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10328 records
ğŸ“Š Shape: (10328, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10323 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:77.54134
[100]	validation_0-rmse:10.49987
[200]	validation_0-rmse:10.34651
[300]	validation_0-rmse:10.34708
[400]	validation_0-rmse:10.34637
[500]	validation_0-rmse:10.34597
[600]	validation_0-rmse:10.34586
[700]	validation_0-rmse:10.34581
[800]	validation_0-rmse:10.34579
[900]	validation_0-rmse:10.34584
[1000]	validation_0-rmse:10.34586
[1100]	validation_0-rmse:10.34586
[1200]	validation_0-rmse:10.34588
[1300]	validation_0-rmse:10.34588
[1400]	validation_0-rmse:10.34587
[1499]	validation_0-rmse:10.34588
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.2282
RMSE                : 27.3925
R2                  : 0.9075
MAPE                : 0.0314
Explained Variance  : 0.9137

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:118.15154
[100]	validation_0-rmse:3.33861
[200]	validation_0-rmse:2.99240
[300]	validation_0-rmse:2.98660
[400]	validation_0-rmse:2.98450
[500]	validation_0-rmse:2.98443
[600]	validation_0-rmse:2.98421
[700]	validation_0-rmse:2.98436
[800]	validation_0-rmse:2.98437
[900]	validation_0-rmse:2.98454
[1000]	validation_0-rmse:2.98463
[1100]	validation_0-rmse:2.98458
[1200]	validation_0-rmse:2.98458
[1300]	validation_0-rmse:2.98462
[1400]	validation_0-rmse:2.98460
[1499]	validation_0-rmse:2.98460
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.7539
RMSE                : 4.6500
R2                  : 0.9966
MAPE                : 0.0365
Explained Variance  : 0.9968

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.99552
[100]	validation_0-rmse:4.95848
[200]	validation_0-rmse:4.78834
[300]	validation_0-rmse:4.74021
[400]	validation_0-rmse:4.73261
[500]	validation_0-rmse:4.72132
[600]	validation_0-rmse:4.72017
[700]	validation_0-rmse:4.72005
[800]	validation_0-rmse:4.71822
[900]	validation_0-rmse:4.71877
[1000]	validation_0-rmse:4.71906
[1100]	validation_0-rmse:4.71889
[1200]	validation_0-rmse:4.71867
[1300]	validation_0-rmse:4.71866
[1400]	validation_0-rmse:4.71852
[1499]	validation_0-rmse:4.71856
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2761
RMSE                : 3.5156
R2                  : 0.9789
MAPE                : 0.0681
Explained Variance  : 0.9798

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.05619
[100]	validation_0-rmse:1.76781
[200]	validation_0-rmse:1.60851
[300]	validation_0-rmse:1.60480
[400]	validation_0-rmse:1.60463
[500]	validation_0-rmse:1.60934
[600]	validation_0-rmse:1.60910
[700]	validation_0-rmse:1.60885
[800]	validation_0-rmse:1.61261
[900]	validation_0-rmse:1.61569
[1000]	validation_0-rmse:1.61673
[1100]	validation_0-rmse:1.61728
[1200]	validation_0-rmse:1.61717
[1300]	validation_0-rmse:1.61711
[1400]	validation_0-rmse:1.61712
[1499]	validation_0-rmse:1.61711
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2733
RMSE                : 2.8494
R2                  : 0.9976
MAPE                : 0.0145
Explained Variance  : 0.9977

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.2733
   RMSE: 2.8494
   R2: 0.9976
   MAPE: 0.0145
   Explained Variance: 0.9977

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-19 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-20 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 190.3 - 235.0 AQI
ğŸ“Š Forecast average: 204.9 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-20 02:28:41 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-20 02:28:41,386 INFO: Closing external client and cleaning up certificates.
Connection closed.
