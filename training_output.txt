ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-01 02:25:03 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-01 02:25:06 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_01_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-01 02:25:06,627 INFO: Initializing external client
2025-12-01 02:25:06,627 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-01 02:25:07,256 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.66s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9879 records
ğŸ“Š Shape: (9879, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9874 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.3195
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 8.8454
RMSE                : 28.5683
R2                  : 0.8961
MAPE                : 0.0229
Explained Variance  : 0.9016
ğŸ† New best model found! RÂ² = 0.8961

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5602
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4640
RMSE                : 3.1524
R2                  : 0.9988
MAPE                : 0.0225
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.7954
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5869
RMSE                : 3.1339
R2                  : 0.9788
MAPE                : 0.0192
Explained Variance  : 0.9803

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.9033
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5773
RMSE                : 2.0400
R2                  : 0.9986
MAPE                : 0.0215
Explained Variance  : 0.9987

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-11-30 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 170.0 - 294.7 AQI
ğŸ“ˆ Forecast average: 214.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 11_29_2025_0224_pkt
âœ… Loaded LightGBM forecasts from 11_29_2025_0224_pkt
ğŸ“Š Reference mean from 2 models: 150.39
ğŸ“ˆ Random Forest forecast mean: 214.04
ğŸ“Š Reference mean: 150.39
ğŸ“Š Ratio (RF/Reference): 1.42
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_01_2025_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 1.4640
ğŸ“ˆ Best MAPE: 0.0225

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.4640240753108276, 'RMSE': 3.152405286487896, 'R2': 0.9988101698285843, 'MAPE': 0.022505676393287294, 'Explained Variance': 0.9988547293841146}, 'randomforest_additional/12_01_2025_0225_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-01 02:25:53 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-01 02:25:53,640 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-01 02:25:53,643 INFO: Initializing external client
2025-12-01 02:25:53,643 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-01 02:25:54,251 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.65s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9879 records
ğŸ“Š Shape: (9879, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9874 records
ğŸ“Š Final dataset shape: (9874, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1583 samples
   Val:   395 samples
   Test:  1974 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 32.0307
Early stopping, best iteration is:
[139]	valid_0's l2: 31.6084
âœ… Model trained with 139 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 12.2737
RMSE                : 29.2887
R2                  : 0.8908
MAPE                : 0.0398
Explained Variance  : 0.8970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.05477892404116
Prediction_local [167.4905877]
Right: 177.7791609443078

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3162 samples
   Val:   790 samples
   Test:  1974 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.2009
[200]	valid_0's l2: 16.5705
[300]	valid_0's l2: 16.5488
Early stopping, best iteration is:
[256]	valid_0's l2: 16.5187
âœ… Model trained with 256 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1828
RMSE                : 3.9400
R2                  : 0.9981
MAPE                : 0.0273
Explained Variance  : 0.9981
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4741 samples
   Val:   1185 samples
   Test:  1974 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.21916
[200]	valid_0's l2: 8.33515
Early stopping, best iteration is:
[219]	valid_0's l2: 8.22656
âœ… Model trained with 219 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3852
RMSE                : 2.2425
R2                  : 0.9892
MAPE                : 0.0215
Explained Variance  : 0.9892
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6320 samples
   Val:   1580 samples
   Test:  1974 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.16132
[200]	valid_0's l2: 2.0789
Early stopping, best iteration is:
[217]	valid_0's l2: 2.05526
âœ… Model trained with 217 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3526
RMSE                : 2.6698
R2                  : 0.9976
MAPE                : 0.0352
Explained Variance  : 0.9976
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.2425

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 139.0 - 163.7 AQI
ğŸ“Š Forecast average: 157.3 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-11-30 22:00:00+00:00 to 2025-12-03 21:00:00+00:00
Forecast range: 139.0 - 163.7 AQI
Average forecast: 157.3 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_01_2025_0226_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_01_2025_0226_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2986 (avg)
RMSE                : 9.5353 (avg)
R2                  : 0.9689 (avg)
MAPE                : 0.0309 (avg)
Explained Variance  : 0.9705 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-01 02:26:17 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_01_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-01 02:26:17,081 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-01 02:26:17,083 INFO: Initializing external client
2025-12-01 02:26:17,083 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-01 02:26:17,694 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.60s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9879 records
ğŸ“Š Shape: (9879, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9874 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:66.47686
[100]	validation_0-rmse:4.38907
[200]	validation_0-rmse:4.33795
[300]	validation_0-rmse:4.33738
[400]	validation_0-rmse:4.33739
[500]	validation_0-rmse:4.33719
[600]	validation_0-rmse:4.33728
[700]	validation_0-rmse:4.33731
[800]	validation_0-rmse:4.33733
[900]	validation_0-rmse:4.33735
[1000]	validation_0-rmse:4.33735
[1100]	validation_0-rmse:4.33735
[1200]	validation_0-rmse:4.33735
[1300]	validation_0-rmse:4.33735
[1400]	validation_0-rmse:4.33735
[1499]	validation_0-rmse:4.33735
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.1948
RMSE                : 27.0164
R2                  : 0.9071
MAPE                : 0.0317
Explained Variance  : 0.9130

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:118.21213
[100]	validation_0-rmse:3.50226
[200]	validation_0-rmse:3.20487
[300]	validation_0-rmse:3.20296
[400]	validation_0-rmse:3.20211
[500]	validation_0-rmse:3.20135
[600]	validation_0-rmse:3.20113
[700]	validation_0-rmse:3.20028
[800]	validation_0-rmse:3.20014
[900]	validation_0-rmse:3.19997
[1000]	validation_0-rmse:3.19996
[1100]	validation_0-rmse:3.20000
[1200]	validation_0-rmse:3.19996
[1300]	validation_0-rmse:3.19991
[1400]	validation_0-rmse:3.19987
[1499]	validation_0-rmse:3.19986
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4368
RMSE                : 4.3785
R2                  : 0.9977
MAPE                : 0.0314
Explained Variance  : 0.9978

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:85.06146
[100]	validation_0-rmse:4.17998
[200]	validation_0-rmse:3.88003
[300]	validation_0-rmse:3.85694
[400]	validation_0-rmse:3.84814
[500]	validation_0-rmse:3.84450
[600]	validation_0-rmse:3.83620
[700]	validation_0-rmse:3.83536
[800]	validation_0-rmse:3.83413
[900]	validation_0-rmse:3.83409
[1000]	validation_0-rmse:3.83372
[1100]	validation_0-rmse:3.83326
[1200]	validation_0-rmse:3.83308
[1300]	validation_0-rmse:3.83337
[1400]	validation_0-rmse:3.83324
[1499]	validation_0-rmse:3.83322
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2366
RMSE                : 3.4352
R2                  : 0.9746
MAPE                : 0.0324
Explained Variance  : 0.9760

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:72.13709
[100]	validation_0-rmse:1.29309
[200]	validation_0-rmse:1.16646
[300]	validation_0-rmse:1.16387
[400]	validation_0-rmse:1.16476
[500]	validation_0-rmse:1.16428
[600]	validation_0-rmse:1.16445
[700]	validation_0-rmse:1.16319
[800]	validation_0-rmse:1.16308
[900]	validation_0-rmse:1.16275
[1000]	validation_0-rmse:1.16243
[1100]	validation_0-rmse:1.16243
[1200]	validation_0-rmse:1.16221
[1300]	validation_0-rmse:1.16230
[1400]	validation_0-rmse:1.16224
[1499]	validation_0-rmse:1.16227
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0665
RMSE                : 2.4268
R2                  : 0.9980
MAPE                : 0.0330
Explained Variance  : 0.9980

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0665
   RMSE: 2.4268
   R2: 0.9980
   MAPE: 0.0330
   Explained Variance: 0.9980

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-11-30 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 151.8 - 160.0 AQI
ğŸ“Š Forecast average: 154.9 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/12_01_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9980
ğŸ“Š Best MAE: 1.0665
ğŸ“ˆ Best MAPE: 0.0330

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 1.0665143728256226, 'RMSE': 2.4268385691417196, 'R2': 0.9980362057685852, 'MAPE': 0.03302757441997528, 'Explained Variance': 0.99803626537323}, 'xgboost_additional/12_01_2025_0226_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-01 02:27:12 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-01 02:27:12,618 INFO: Closing external client and cleaning up certificates.
Connection closed.
