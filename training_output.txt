ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-16 02:27:38 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-16 02:27:40 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_16_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-16 02:27:40,598 INFO: Initializing external client
2025-12-16 02:27:40,598 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-16 02:27:41,170 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.64s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10234 records
ğŸ“Š Shape: (10234, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10229 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.4799
ğŸŒ² OOB Score: 0.9977
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.1139
RMSE                : 28.9965
R2                  : 0.8933
MAPE                : 0.0237
Explained Variance  : 0.8989
ğŸ† New best model found! RÂ² = 0.8933

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5929
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8696
RMSE                : 3.7056
R2                  : 0.9981
MAPE                : 0.0263
Explained Variance  : 0.9982
ğŸ† New best model found! RÂ² = 0.9981

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5662
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2923
RMSE                : 2.5863
R2                  : 0.9870
MAPE                : 0.0292
Explained Variance  : 0.9879

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8509
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5880
RMSE                : 2.0935
R2                  : 0.9985
MAPE                : 0.0193
Explained Variance  : 0.9985
ğŸ† New best model found! RÂ² = 0.9985

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-15 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 180.9 - 301.9 AQI
ğŸ“ˆ Forecast average: 224.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_14_2025_0225_pkt
ğŸ“Š Reference mean from 1 models: 47.85
ğŸ“ˆ Random Forest forecast mean: 223.99
ğŸ“Š Reference mean: 47.85
ğŸ“Š Ratio (RF/Reference): 4.68
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.214
ğŸ“Š Corrected mean: 47.85
ğŸ“Š Reduction: 176.14 AQI (78.6%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_16_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9985
ğŸ“Š Best MAE: 0.5880
ğŸ“ˆ Best MAPE: 0.0193

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.5879666151330369, 'RMSE': 2.093474501084625, 'R2': 0.9984771210454906, 'MAPE': 0.019278066588650163, 'Explained Variance': 0.9985399196255819}, 'randomforest_additional/12_16_2025_0227_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-16 02:28:44 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-16 02:28:44,427 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-16 02:28:44,428 INFO: Initializing external client
2025-12-16 02:28:44,429 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-16 02:28:44,927 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.23s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10234 records
ğŸ“Š Shape: (10234, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10229 records
ğŸ“Š Final dataset shape: (10229, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1640 samples
   Val:   409 samples
   Test:  2045 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.0143
[200]	valid_0's l2: 19.0633
Early stopping, best iteration is:
[157]	valid_0's l2: 18.8541
âœ… Model trained with 157 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.4150
RMSE                : 28.3867
R2                  : 0.8977
MAPE                : 0.0364
Explained Variance  : 0.9032
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.76455884420557
Prediction_local [171.64957624]
Right: 183.35127457297997

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3276 samples
   Val:   818 samples
   Test:  2045 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.254
[200]	valid_0's l2: 18.5553
Early stopping, best iteration is:
[161]	valid_0's l2: 18.4537
âœ… Model trained with 161 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3535
RMSE                : 4.0325
R2                  : 0.9977
MAPE                : 0.0291
Explained Variance  : 0.9977
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4912 samples
   Val:   1227 samples
   Test:  2045 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 12.9606
[200]	valid_0's l2: 11.9475
Early stopping, best iteration is:
[219]	valid_0's l2: 11.7651
âœ… Model trained with 219 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3831
RMSE                : 2.4208
R2                  : 0.9886
MAPE                : 0.0389
Explained Variance  : 0.9889
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6548 samples
   Val:   1636 samples
   Test:  2045 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.03961
[200]	valid_0's l2: 1.90533
Early stopping, best iteration is:
[244]	valid_0's l2: 1.87024
âœ… Model trained with 244 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3589
RMSE                : 2.8291
R2                  : 0.9972
MAPE                : 0.0304
Explained Variance  : 0.9972
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.4208

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 160.7 - 213.8 AQI
ğŸ“Š Forecast average: 179.2 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-15 22:00:00+00:00 to 2025-12-18 21:00:00+00:00
Forecast range: 160.7 - 213.8 AQI
Average forecast: 179.2 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_16_2025_0229_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_16_2025_0229_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.1276 (avg)
RMSE                : 9.4173 (avg)
R2                  : 0.9703 (avg)
MAPE                : 0.0337 (avg)
Explained Variance  : 0.9718 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-16 02:29:08 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_16_2025_0229_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-16 02:29:08,636 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-16 02:29:08,638 INFO: Initializing external client
2025-12-16 02:29:08,638 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-16 02:29:09,167 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.57s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10234 records
ğŸ“Š Shape: (10234, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10229 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:66.66099
[100]	validation_0-rmse:3.63499
[200]	validation_0-rmse:3.55991
[300]	validation_0-rmse:3.55841
[400]	validation_0-rmse:3.55778
[500]	validation_0-rmse:3.55720
[600]	validation_0-rmse:3.55700
[700]	validation_0-rmse:3.55690
[800]	validation_0-rmse:3.55687
[900]	validation_0-rmse:3.55686
[1000]	validation_0-rmse:3.55686
[1100]	validation_0-rmse:3.55685
[1200]	validation_0-rmse:3.55685
[1300]	validation_0-rmse:3.55686
[1400]	validation_0-rmse:3.55687
[1499]	validation_0-rmse:3.55687
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.0520
RMSE                : 26.9327
R2                  : 0.9079
MAPE                : 0.0315
Explained Variance  : 0.9140

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:119.09913
[100]	validation_0-rmse:3.70430
[200]	validation_0-rmse:3.42556
[300]	validation_0-rmse:3.42037
[400]	validation_0-rmse:3.41941
[500]	validation_0-rmse:3.41812
[600]	validation_0-rmse:3.41844
[700]	validation_0-rmse:3.41847
[800]	validation_0-rmse:3.41833
[900]	validation_0-rmse:3.41830
[1000]	validation_0-rmse:3.41820
[1100]	validation_0-rmse:3.41815
[1200]	validation_0-rmse:3.41816
[1300]	validation_0-rmse:3.41817
[1400]	validation_0-rmse:3.41816
[1499]	validation_0-rmse:3.41815
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6722
RMSE                : 4.4911
R2                  : 0.9972
MAPE                : 0.0345
Explained Variance  : 0.9973

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.19528
[100]	validation_0-rmse:4.76251
[200]	validation_0-rmse:4.48909
[300]	validation_0-rmse:4.45271
[400]	validation_0-rmse:4.44704
[500]	validation_0-rmse:4.44704
[600]	validation_0-rmse:4.44259
[700]	validation_0-rmse:4.44298
[800]	validation_0-rmse:4.44254
[900]	validation_0-rmse:4.44259
[1000]	validation_0-rmse:4.44072
[1100]	validation_0-rmse:4.44086
[1200]	validation_0-rmse:4.44060
[1300]	validation_0-rmse:4.44061
[1400]	validation_0-rmse:4.44046
[1499]	validation_0-rmse:4.44048
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0893
RMSE                : 3.2367
R2                  : 0.9796
MAPE                : 0.0472
Explained Variance  : 0.9802

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:79.49849
[100]	validation_0-rmse:1.43174
[200]	validation_0-rmse:1.32013
[300]	validation_0-rmse:1.31898
[400]	validation_0-rmse:1.31846
[500]	validation_0-rmse:1.31785
[600]	validation_0-rmse:1.31746
[700]	validation_0-rmse:1.31748
[800]	validation_0-rmse:1.31769
[900]	validation_0-rmse:1.31788
[1000]	validation_0-rmse:1.31753
[1100]	validation_0-rmse:1.31758
[1200]	validation_0-rmse:1.31740
[1300]	validation_0-rmse:1.31732
[1400]	validation_0-rmse:1.31733
[1499]	validation_0-rmse:1.31719
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0515
RMSE                : 2.5321
R2                  : 0.9978
MAPE                : 0.0274
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0515
   RMSE: 2.5321
   R2: 0.9978
   MAPE: 0.0274
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-15 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 185.6 - 231.2 AQI
ğŸ“Š Forecast average: 206.6 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-16 02:30:06 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-16 02:30:06,099 INFO: Closing external client and cleaning up certificates.
Connection closed.
