ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-12 02:28:12 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-12 02:28:15 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_12_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-12 02:28:15,161 INFO: Initializing external client
2025-12-12 02:28:15,161 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-12 02:28:16,222 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.03s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10138 records
ğŸ“Š Shape: (10138, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10133 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.4465
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.2083
RMSE                : 29.1778
R2                  : 0.8920
MAPE                : 0.0239
Explained Variance  : 0.8978
ğŸ† New best model found! RÂ² = 0.8920

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.6995
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7331
RMSE                : 3.4138
R2                  : 0.9984
MAPE                : 0.0253
Explained Variance  : 0.9985
ğŸ† New best model found! RÂ² = 0.9984

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.0625
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3272
RMSE                : 2.8132
R2                  : 0.9856
MAPE                : 0.0233
Explained Variance  : 0.9862

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8241
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5679
RMSE                : 2.0265
R2                  : 0.9986
MAPE                : 0.0197
Explained Variance  : 0.9987
ğŸ† New best model found! RÂ² = 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-11 20:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 114.9 - 268.3 AQI
ğŸ“ˆ Forecast average: 197.1 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_11_2025_0233_pkt
ğŸ“Š Reference mean from 1 models: 156.99
ğŸ“ˆ Random Forest forecast mean: 197.06
ğŸ“Š Reference mean: 156.99
ğŸ“Š Ratio (RF/Reference): 1.26
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_12_2025_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9986
ğŸ“Š Best MAE: 0.5679
ğŸ“ˆ Best MAPE: 0.0197

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.567850342192361, 'RMSE': 2.0265412029343457, 'R2': 0.9986027024200798, 'MAPE': 0.01973123333980226, 'Explained Variance': 0.9986567847330006}, 'randomforest_additional/12_12_2025_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-12 02:29:21 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-12 02:29:21,377 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-12 02:29:21,379 INFO: Initializing external client
2025-12-12 02:29:21,380 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-12 02:29:22,214 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.18s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10138 records
ğŸ“Š Shape: (10138, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10133 records
ğŸ“Š Final dataset shape: (10133, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1624 samples
   Val:   405 samples
   Test:  2026 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 18.1885
Early stopping, best iteration is:
[132]	valid_0's l2: 18.0079
âœ… Model trained with 132 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.3441
RMSE                : 28.3583
R2                  : 0.8980
MAPE                : 0.0361
Explained Variance  : 0.9028
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.62449928699444
Prediction_local [171.29436783]
Right: 222.98786507044747

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3244 samples
   Val:   811 samples
   Test:  2026 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.6224
[200]	valid_0's l2: 18.9784
[300]	valid_0's l2: 18.948
Early stopping, best iteration is:
[264]	valid_0's l2: 18.9411
âœ… Model trained with 264 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1812
RMSE                : 3.8640
R2                  : 0.9979
MAPE                : 0.0283
Explained Variance  : 0.9979
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4865 samples
   Val:   1216 samples
   Test:  2026 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 11.9279
[200]	valid_0's l2: 10.5267
Early stopping, best iteration is:
[217]	valid_0's l2: 10.3955
âœ… Model trained with 217 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3857
RMSE                : 2.3731
R2                  : 0.9897
MAPE                : 0.0313
Explained Variance  : 0.9898
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6486 samples
   Val:   1621 samples
   Test:  2026 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.00748
[200]	valid_0's l2: 1.78886
[300]	valid_0's l2: 1.72715
Early stopping, best iteration is:
[258]	valid_0's l2: 1.72715
âœ… Model trained with 258 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3951
RMSE                : 2.8251
R2                  : 0.9973
MAPE                : 0.0314
Explained Variance  : 0.9973
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.3731

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 68.6 - 124.6 AQI
ğŸ“Š Forecast average: 93.9 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-11 21:00:00+00:00 to 2025-12-14 20:00:00+00:00
Forecast range: 68.6 - 124.6 AQI
Average forecast: 93.9 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_12_2025_0229_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_12_2025_0229_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0765 (avg)
RMSE                : 9.3551 (avg)
R2                  : 0.9707 (avg)
MAPE                : 0.0318 (avg)
Explained Variance  : 0.9720 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-12 02:29:46 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_12_2025_0229_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-12 02:29:46,760 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-12 02:29:46,761 INFO: Initializing external client
2025-12-12 02:29:46,762 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-12 02:29:47,815 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
2025-12-12 02:30:45,190 ERROR: Flight returned timeout error, with message: Deadline Exceeded
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 209, in __init__
    self._health_check()
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 55, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 289, in call
    raise attempt.get()
          ^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 326, in get
    raise exc.with_traceback(tb)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 273, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
                      ^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 307, in _health_check
    list(self._connection.do_action(action, options=options))
  File "pyarrow/_flight.pyx", line 1668, in _do_action_response
  File "pyarrow/_flight.pyx", line 60, in pyarrow._flight.check_flight_status
pyarrow._flight.FlightTimedOutError: Flight returned timeout error, with message: Deadline Exceeded
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   2025-12-12 02:30:50,194 ERROR: Flight returned timeout error, with message: Deadline Exceeded
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 394, in afs_error_handler_wrapper
    return func(instance, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 459, in read_query
    return self._get_dataset(
           ^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 55, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 279, in call
    return attempt.get(self._wrap_exception)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 326, in get
    raise exc.with_traceback(tb)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 273, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
                      ^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 440, in _get_dataset
    info = self.get_flight_info(descriptor)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 55, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 279, in call
    return attempt.get(self._wrap_exception)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 326, in get
    raise exc.with_traceback(tb)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/retrying.py", line 273, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
                      ^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py", line 427, in get_flight_info
    return self._connection.get_flight_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/_flight.pyx", line 1712, in pyarrow._flight.FlightClient.get_flight_info
  File "pyarrow/_flight.pyx", line 60, in pyarrow._flight.check_flight_status
pyarrow._flight.FlightTimedOutError: Flight returned timeout error, with message: Deadline Exceeded
Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           
âŒ Failed to retrieve features from Hopsworks: 'ArrowFlightClient' object has no attribute '_server_version'
âš ï¸ Trying local CSV files...
ğŸ“ Loading data from local file: retrieved_karachi_aqi_features15.csv
âœ… Loaded 9704 records from retrieved_karachi_aqi_features15.csv

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9699 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:65.57539
[100]	validation_0-rmse:4.76386
[200]	validation_0-rmse:4.79904
[300]	validation_0-rmse:4.79993
[400]	validation_0-rmse:4.79928
[500]	validation_0-rmse:4.79900
[600]	validation_0-rmse:4.79888
[700]	validation_0-rmse:4.79881
[800]	validation_0-rmse:4.79877
[900]	validation_0-rmse:4.79875
[1000]	validation_0-rmse:4.79875
[1100]	validation_0-rmse:4.79874
[1200]	validation_0-rmse:4.79873
[1300]	validation_0-rmse:4.79873
[1400]	validation_0-rmse:4.79873
[1499]	validation_0-rmse:4.79872
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.1606
RMSE                : 27.1597
R2                  : 0.9039
MAPE                : 0.0311
Explained Variance  : 0.9105

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:128.39112
[100]	validation_0-rmse:4.40688
[200]	validation_0-rmse:4.06046
[300]	validation_0-rmse:4.05572
[400]	validation_0-rmse:4.05492
[500]	validation_0-rmse:4.05432
[600]	validation_0-rmse:4.05423
[700]	validation_0-rmse:4.05414
[800]	validation_0-rmse:4.05408
[900]	validation_0-rmse:4.05413
[1000]	validation_0-rmse:4.05414
[1100]	validation_0-rmse:4.05419
[1200]	validation_0-rmse:4.05419
[1300]	validation_0-rmse:4.05417
[1400]	validation_0-rmse:4.05418
[1499]	validation_0-rmse:4.05417
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4234
RMSE                : 4.6362
R2                  : 0.9975
MAPE                : 0.0300
Explained Variance  : 0.9976

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:87.83636
[100]	validation_0-rmse:3.99644
[200]	validation_0-rmse:3.76432
[300]	validation_0-rmse:3.74226
[400]	validation_0-rmse:3.74144
[500]	validation_0-rmse:3.74427
[600]	validation_0-rmse:3.74428
[700]	validation_0-rmse:3.74595
[800]	validation_0-rmse:3.74603
[900]	validation_0-rmse:3.74665
[1000]	validation_0-rmse:3.74681
[1100]	validation_0-rmse:3.74681
[1200]	validation_0-rmse:3.74700
[1300]	validation_0-rmse:3.74699
[1400]	validation_0-rmse:3.74709
[1499]	validation_0-rmse:3.74715
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2681
RMSE                : 3.4171
R2                  : 0.9762
MAPE                : 0.0334
Explained Variance  : 0.9771

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:72.33806
[100]	validation_0-rmse:1.34497
[200]	validation_0-rmse:1.24458
[300]	validation_0-rmse:1.24159
[400]	validation_0-rmse:1.24230
[500]	validation_0-rmse:1.24474
[600]	validation_0-rmse:1.24509
[700]	validation_0-rmse:1.24570
[800]	validation_0-rmse:1.24512
[900]	validation_0-rmse:1.24547
[1000]	validation_0-rmse:1.24558
[1100]	validation_0-rmse:1.24568
[1200]	validation_0-rmse:1.24577
[1300]	validation_0-rmse:1.24594
[1400]	validation_0-rmse:1.24599
[1499]	validation_0-rmse:1.24586
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.9344
RMSE                : 2.4123
R2                  : 0.9977
MAPE                : 0.0305
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 0.9344
   RMSE: 2.4123
   R2: 0.9977
   MAPE: 0.0305
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-11-23 18:00:00...
   ğŸ“… Forecasting for 2025-11-23 19:00:00...
   ğŸ“… Forecasting for 2025-11-23 20:00:00...
   ğŸ“… Forecasting for 2025-11-23 21:00:00...
   ğŸ“… Forecasting for 2025-11-23 22:00:00...
   ğŸ“… Forecasting for 2025-11-23 23:00:00...
   ğŸ“… Forecasting for 2025-11-24 00:00:00...
   ğŸ“… Forecasting for 2025-11-24 01:00:00...
   ğŸ“… Forecasting for 2025-11-24 02:00:00...
   ğŸ“… Forecasting for 2025-11-24 03:00:00...
   ğŸ“… Forecasting for 2025-11-24 04:00:00...
   ğŸ“… Forecasting for 2025-11-24 05:00:00...
   ğŸ“… Forecasting for 2025-11-24 06:00:00...
   ğŸ“… Forecasting for 2025-11-24 07:00:00...
   ğŸ“… Forecasting for 2025-11-24 08:00:00...
   ğŸ“… Forecasting for 2025-11-24 09:00:00...
   ğŸ“… Forecasting for 2025-11-24 10:00:00...
   ğŸ“… Forecasting for 2025-11-24 11:00:00...
   ğŸ“… Forecasting for 2025-11-24 12:00:00...
   ğŸ“… Forecasting for 2025-11-24 13:00:00...
   ğŸ“… Forecasting for 2025-11-24 14:00:00...
   ğŸ“… Forecasting for 2025-11-24 15:00:00...
   ğŸ“… Forecasting for 2025-11-24 16:00:00...
   ğŸ“… Forecasting for 2025-11-24 17:00:00...
   ğŸ“… Forecasting for 2025-11-24 18:00:00...
   ğŸ“… Forecasting for 2025-11-24 19:00:00...
   ğŸ“… Forecasting for 2025-11-24 20:00:00...
   ğŸ“… Forecasting for 2025-11-24 21:00:00...
   ğŸ“… Forecasting for 2025-11-24 22:00:00...
   ğŸ“… Forecasting for 2025-11-24 23:00:00...
   ğŸ“… Forecasting for 2025-11-25 00:00:00...
   ğŸ“… Forecasting for 2025-11-25 01:00:00...
   ğŸ“… Forecasting for 2025-11-25 02:00:00...
   ğŸ“… Forecasting for 2025-11-25 03:00:00...
   ğŸ“… Forecasting for 2025-11-25 04:00:00...
   ğŸ“… Forecasting for 2025-11-25 05:00:00...
   ğŸ“… Forecasting for 2025-11-25 06:00:00...
   ğŸ“… Forecasting for 2025-11-25 07:00:00...
   ğŸ“… Forecasting for 2025-11-25 08:00:00...
   ğŸ“… Forecasting for 2025-11-25 09:00:00...
   ğŸ“… Forecasting for 2025-11-25 10:00:00...
   ğŸ“… Forecasting for 2025-11-25 11:00:00...
   ğŸ“… Forecasting for 2025-11-25 12:00:00...
   ğŸ“… Forecasting for 2025-11-25 13:00:00...
   ğŸ“… Forecasting for 2025-11-25 14:00:00...
   ğŸ“… Forecasting for 2025-11-25 15:00:00...
   ğŸ“… Forecasting for 2025-11-25 16:00:00...
   ğŸ“… Forecasting for 2025-11-25 17:00:00...
   ğŸ“… Forecasting for 2025-11-25 18:00:00...
   ğŸ“… Forecasting for 2025-11-25 19:00:00...
   ğŸ“… Forecasting for 2025-11-25 20:00:00...
   ğŸ“… Forecasting for 2025-11-25 21:00:00...
   ğŸ“… Forecasting for 2025-11-25 22:00:00...
   ğŸ“… Forecasting for 2025-11-25 23:00:00...
   ğŸ“… Forecasting for 2025-11-26 00:00:00...
   ğŸ“… Forecasting for 2025-11-26 01:00:00...
   ğŸ“… Forecasting for 2025-11-26 02:00:00...
   ğŸ“… Forecasting for 2025-11-26 03:00:00...
   ğŸ“… Forecasting for 2025-11-26 04:00:00...
   ğŸ“… Forecasting for 2025-11-26 05:00:00...
   ğŸ“… Forecasting for 2025-11-26 06:00:00...
   ğŸ“… Forecasting for 2025-11-26 07:00:00...
   ğŸ“… Forecasting for 2025-11-26 08:00:00...
   ğŸ“… Forecasting for 2025-11-26 09:00:00...
   ğŸ“… Forecasting for 2025-11-26 10:00:00...
   ğŸ“… Forecasting for 2025-11-26 11:00:00...
   ğŸ“… Forecasting for 2025-11-26 12:00:00...
   ğŸ“… Forecasting for 2025-11-26 13:00:00...
   ğŸ“… Forecasting for 2025-11-26 14:00:00...
   ğŸ“… Forecasting for 2025-11-26 15:00:00...
   ğŸ“… Forecasting for 2025-11-26 16:00:00...
   ğŸ“… Forecasting for 2025-11-26 17:00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 168.7 - 183.8 AQI
ğŸ“Š Forecast average: 175.0 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-12 02:31:42 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-12 02:31:42,582 INFO: Closing external client and cleaning up certificates.
Connection closed.
