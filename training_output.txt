ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-08 02:27:58 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-08 02:28:01 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_08_2026_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-08 02:28:01,673 INFO: Initializing external client
2026-01-08 02:28:01,673 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-08 02:28:02,995 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.14s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10779 records
ğŸ“Š Shape: (10779, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10774 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 13.9342
ğŸŒ² OOB Score: 0.9981
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.5860
RMSE                : 30.5329
R2                  : 0.8962
MAPE                : 0.0243
Explained Variance  : 0.9028
ğŸ† New best model found! RÂ² = 0.8962

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.1125
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0531
RMSE                : 3.7361
R2                  : 0.9969
MAPE                : 0.0283
Explained Variance  : 0.9971
ğŸ† New best model found! RÂ² = 0.9969

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.5834
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3343
RMSE                : 2.7262
R2                  : 0.9850
MAPE                : 0.0592
Explained Variance  : 0.9856

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.1923
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4513
RMSE                : 2.4881
R2                  : 0.9976
MAPE                : 0.0046
Explained Variance  : 0.9976
ğŸ† New best model found! RÂ² = 0.9976

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2026-01-07 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 149.2 - 270.0 AQI
ğŸ“ˆ Forecast average: 197.7 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 197.68
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.60
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.625
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 74.08 AQI (37.5%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_08_2026_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9976
ğŸ“Š Best MAE: 0.4513
ğŸ“ˆ Best MAPE: 0.0046

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.45126389454711396, 'RMSE': 2.4881181159653747, 'R2': 0.9975616696188008, 'MAPE': 0.004555533528682346, 'Explained Variance': 0.9976268284038122}, 'randomforest_additional/01_08_2026_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-08 02:29:13 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-08 02:29:13,449 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-08 02:29:13,452 INFO: Initializing external client
2026-01-08 02:29:13,452 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-08 02:29:14,417 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.21s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10779 records
ğŸ“Š Shape: (10779, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10774 records
ğŸ“Š Final dataset shape: (10774, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1727 samples
   Val:   431 samples
   Test:  2154 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 213.007
[200]	valid_0's l2: 187.003
[300]	valid_0's l2: 179.126
[400]	valid_0's l2: 175.929
[500]	valid_0's l2: 173.373
Early stopping, best iteration is:
[504]	valid_0's l2: 173.357
âœ… Model trained with 503 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.4838
RMSE                : 28.5578
R2                  : 0.9092
MAPE                : 0.0362
Explained Variance  : 0.9156
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 92.27154976226393
Prediction_local [102.10697118]
Right: 104.6071320084357

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3450 samples
   Val:   862 samples
   Test:  2154 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.22
Early stopping, best iteration is:
[101]	valid_0's l2: 17.2132
âœ… Model trained with 101 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9977
RMSE                : 3.4484
R2                  : 0.9973
MAPE                : 0.0280
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5173 samples
   Val:   1293 samples
   Test:  2154 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 6.63555
Early stopping, best iteration is:
[140]	valid_0's l2: 6.48903
âœ… Model trained with 140 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8196
RMSE                : 3.3360
R2                  : 0.9776
MAPE                : 0.0837
Explained Variance  : 0.9791
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6896 samples
   Val:   1724 samples
   Test:  2154 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.72399
[200]	valid_0's l2: 3.53245
Early stopping, best iteration is:
[240]	valid_0's l2: 3.49189
âœ… Model trained with 240 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4706
RMSE                : 3.4114
R2                  : 0.9954
MAPE                : 0.0137
Explained Variance  : 0.9954
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.3360

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 95.5 - 136.8 AQI
ğŸ“Š Forecast average: 124.5 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2026-01-07 22:00:00+00:00 to 2026-01-10 21:00:00+00:00
Forecast range: 95.5 - 136.8 AQI
Average forecast: 124.5 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_08_2026_0229_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_08_2026_0229_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.1929 (avg)
RMSE                : 9.6884 (avg)
R2                  : 0.9699 (avg)
MAPE                : 0.0404 (avg)
Explained Variance  : 0.9719 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-08 02:29:39 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_08_2026_0229_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-08 02:29:39,298 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-08 02:29:39,300 INFO: Initializing external client
2026-01-08 02:29:39,300 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-08 02:29:40,239 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.35s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10779 records
ğŸ“Š Shape: (10779, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10774 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:89.93372
[100]	validation_0-rmse:12.87767
[200]	validation_0-rmse:12.63952
[300]	validation_0-rmse:12.63626
[400]	validation_0-rmse:12.63526
[500]	validation_0-rmse:12.63475
[600]	validation_0-rmse:12.63426
[700]	validation_0-rmse:12.63397
[800]	validation_0-rmse:12.63384
[900]	validation_0-rmse:12.63380
[1000]	validation_0-rmse:12.63378
[1100]	validation_0-rmse:12.63378
[1200]	validation_0-rmse:12.63378
[1300]	validation_0-rmse:12.63378
[1400]	validation_0-rmse:12.63377
[1499]	validation_0-rmse:12.63377
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.5412
RMSE                : 28.0470
R2                  : 0.9124
MAPE                : 0.0323
Explained Variance  : 0.9197

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:115.80031
[100]	validation_0-rmse:3.50147
[200]	validation_0-rmse:3.26944
[300]	validation_0-rmse:3.26676
[400]	validation_0-rmse:3.26624
[500]	validation_0-rmse:3.26494
[600]	validation_0-rmse:3.26513
[700]	validation_0-rmse:3.26512
[800]	validation_0-rmse:3.26505
[900]	validation_0-rmse:3.26517
[1000]	validation_0-rmse:3.26529
[1100]	validation_0-rmse:3.26538
[1200]	validation_0-rmse:3.26543
[1300]	validation_0-rmse:3.26547
[1400]	validation_0-rmse:3.26545
[1499]	validation_0-rmse:3.26547
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5219
RMSE                : 4.0148
R2                  : 0.9964
MAPE                : 0.0353
Explained Variance  : 0.9965

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:79.76845
[100]	validation_0-rmse:3.46906
[200]	validation_0-rmse:3.25963
[300]	validation_0-rmse:3.25654
[400]	validation_0-rmse:3.25145
[500]	validation_0-rmse:3.24601
[600]	validation_0-rmse:3.24546
[700]	validation_0-rmse:3.24618
[800]	validation_0-rmse:3.24525
[900]	validation_0-rmse:3.24629
[1000]	validation_0-rmse:3.24847
[1100]	validation_0-rmse:3.24939
[1200]	validation_0-rmse:3.24917
[1300]	validation_0-rmse:3.25007
[1400]	validation_0-rmse:3.25021
[1499]	validation_0-rmse:3.25030
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8568
RMSE                : 3.0870
R2                  : 0.9808
MAPE                : 0.0714
Explained Variance  : 0.9814

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:85.34134
[100]	validation_0-rmse:1.79526
[200]	validation_0-rmse:1.64866
[300]	validation_0-rmse:1.64618
[400]	validation_0-rmse:1.65251
[500]	validation_0-rmse:1.65344
[600]	validation_0-rmse:1.65379
[700]	validation_0-rmse:1.65541
[800]	validation_0-rmse:1.65746
[900]	validation_0-rmse:1.65742
[1000]	validation_0-rmse:1.65842
[1100]	validation_0-rmse:1.65904
[1200]	validation_0-rmse:1.65893
[1300]	validation_0-rmse:1.65949
[1400]	validation_0-rmse:1.65960
[1499]	validation_0-rmse:1.65937
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1649
RMSE                : 2.9876
R2                  : 0.9965
MAPE                : 0.0117
Explained Variance  : 0.9965

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1649
   RMSE: 2.9876
   R2: 0.9965
   MAPE: 0.0117
   Explained Variance: 0.9965

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2026-01-07 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 84.0 - 116.8 AQI
ğŸ“Š Forecast average: 99.0 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-08 02:30:42 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-08 02:30:42,342 INFO: Closing external client and cleaning up certificates.
Connection closed.
