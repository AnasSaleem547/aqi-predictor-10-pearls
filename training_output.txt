ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-11-29 02:23:39 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-11-29 02:23:41 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/11_29_2025_0223_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-29 02:23:41,831 INFO: Initializing external client
2025-11-29 02:23:41,831 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-29 02:23:42,427 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.57s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9831 records
ğŸ“Š Shape: (9831, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9826 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.3290
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 8.7388
RMSE                : 28.3501
R2                  : 0.8961
MAPE                : 0.0227
Explained Variance  : 0.9014
ğŸ† New best model found! RÂ² = 0.8961

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5525
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4751
RMSE                : 3.1895
R2                  : 0.9988
MAPE                : 0.0225
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.8331
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6265
RMSE                : 3.1806
R2                  : 0.9784
MAPE                : 0.0197
Explained Variance  : 0.9800

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.9148
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5544
RMSE                : 2.0440
R2                  : 0.9986
MAPE                : 0.0215
Explained Variance  : 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-11-28 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 166.0 - 300.9 AQI
ğŸ“ˆ Forecast average: 201.7 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 11_28_2025_0227_pkt
âœ… Loaded LightGBM forecasts from 11_28_2025_0227_pkt
ğŸ“Š Reference mean from 2 models: 149.08
ğŸ“ˆ Random Forest forecast mean: 201.68
ğŸ“Š Reference mean: 149.08
ğŸ“Š Ratio (RF/Reference): 1.35
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/11_29_2025_0223_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 1.4751
ğŸ“ˆ Best MAPE: 0.0225

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.475131927024562, 'RMSE': 3.1894865207552328, 'R2': 0.9988170257530224, 'MAPE': 0.022543065084323684, 'Explained Variance': 0.9988616929343959}, 'randomforest_additional/11_29_2025_0223_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-11-29 02:24:31 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-29 02:24:31,147 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-29 02:24:31,150 INFO: Initializing external client
2025-11-29 02:24:31,150 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-29 02:24:31,617 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.83s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9831 records
ğŸ“Š Shape: (9831, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9826 records
ğŸ“Š Final dataset shape: (9826, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1573 samples
   Val:   393 samples
   Test:  1965 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 36.896
Early stopping, best iteration is:
[85]	valid_0's l2: 36.4426
âœ… Model trained with 85 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 12.6073
RMSE                : 31.2998
R2                  : 0.8733
MAPE                : 0.0399
Explained Variance  : 0.8810
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 64.70782031171953
Prediction_local [166.06387388]
Right: 160.05066725361493

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3145 samples
   Val:   786 samples
   Test:  1965 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.4936
[200]	valid_0's l2: 18.9416
[300]	valid_0's l2: 18.8727
Early stopping, best iteration is:
[254]	valid_0's l2: 18.8679
âœ… Model trained with 254 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1912
RMSE                : 3.9923
R2                  : 0.9981
MAPE                : 0.0267
Explained Variance  : 0.9981
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4717 samples
   Val:   1179 samples
   Test:  1965 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.1548
Early stopping, best iteration is:
[134]	valid_0's l2: 12.3118
âœ… Model trained with 134 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6553
RMSE                : 2.7722
R2                  : 0.9836
MAPE                : 0.0239
Explained Variance  : 0.9849
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6289 samples
   Val:   1572 samples
   Test:  1965 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.22164
Early stopping, best iteration is:
[136]	valid_0's l2: 2.18352
âœ… Model trained with 136 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3510
RMSE                : 2.7380
R2                  : 0.9974
MAPE                : 0.0365
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 2.7380

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 100.6 - 166.3 AQI
ğŸ“Š Forecast average: 149.1 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-11-28 22:00:00+00:00 to 2025-12-01 21:00:00+00:00
Forecast range: 100.6 - 166.3 AQI
Average forecast: 149.1 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/11_29_2025_0224_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/11_29_2025_0224_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.4512 (avg)
RMSE                : 10.2006 (avg)
R2                  : 0.9631 (avg)
MAPE                : 0.0317 (avg)
Explained Variance  : 0.9654 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-11-29 02:24:52 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/11_29_2025_0224_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-29 02:24:52,001 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-29 02:24:52,004 INFO: Initializing external client
2025-11-29 02:24:52,004 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-29 02:24:52,497 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.97s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9831 records
ğŸ“Š Shape: (9831, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9826 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:66.71184
[100]	validation_0-rmse:4.55240
[200]	validation_0-rmse:4.52832
[300]	validation_0-rmse:4.52677
[400]	validation_0-rmse:4.52599
[500]	validation_0-rmse:4.52546
[600]	validation_0-rmse:4.52521
[700]	validation_0-rmse:4.52510
[800]	validation_0-rmse:4.52504
[900]	validation_0-rmse:4.52501
[1000]	validation_0-rmse:4.52498
[1100]	validation_0-rmse:4.52497
[1200]	validation_0-rmse:4.52496
[1300]	validation_0-rmse:4.52495
[1400]	validation_0-rmse:4.52495
[1499]	validation_0-rmse:4.52495
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.1565
RMSE                : 27.1241
R2                  : 0.9049
MAPE                : 0.0316
Explained Variance  : 0.9112

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:122.04707
[100]	validation_0-rmse:4.04235
[200]	validation_0-rmse:3.68259
[300]	validation_0-rmse:3.67661
[400]	validation_0-rmse:3.67500
[500]	validation_0-rmse:3.67316
[600]	validation_0-rmse:3.67296
[700]	validation_0-rmse:3.67272
[800]	validation_0-rmse:3.67251
[900]	validation_0-rmse:3.67252
[1000]	validation_0-rmse:3.67255
[1100]	validation_0-rmse:3.67256
[1200]	validation_0-rmse:3.67254
[1300]	validation_0-rmse:3.67252
[1400]	validation_0-rmse:3.67250
[1499]	validation_0-rmse:3.67251
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2737
RMSE                : 4.0938
R2                  : 0.9981
MAPE                : 0.0293
Explained Variance  : 0.9981

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:86.21929
[100]	validation_0-rmse:4.64109
[200]	validation_0-rmse:4.43381
[300]	validation_0-rmse:4.41948
[400]	validation_0-rmse:4.41114
[500]	validation_0-rmse:4.40661
[600]	validation_0-rmse:4.40117
[700]	validation_0-rmse:4.40316
[800]	validation_0-rmse:4.40115
[900]	validation_0-rmse:4.40024
[1000]	validation_0-rmse:4.40045
[1100]	validation_0-rmse:4.39911
[1200]	validation_0-rmse:4.39877
[1300]	validation_0-rmse:4.39874
[1400]	validation_0-rmse:4.39889
[1499]	validation_0-rmse:4.39884
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3780
RMSE                : 3.7639
R2                  : 0.9697
MAPE                : 0.0329
Explained Variance  : 0.9744

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:71.86987
[100]	validation_0-rmse:1.37712
[200]	validation_0-rmse:1.26797
[300]	validation_0-rmse:1.26678
[400]	validation_0-rmse:1.27043
[500]	validation_0-rmse:1.26775
[600]	validation_0-rmse:1.26345
[700]	validation_0-rmse:1.26360
[800]	validation_0-rmse:1.26247
[900]	validation_0-rmse:1.26221
[1000]	validation_0-rmse:1.26222
[1100]	validation_0-rmse:1.26205
[1200]	validation_0-rmse:1.26195
[1300]	validation_0-rmse:1.26180
[1400]	validation_0-rmse:1.26186
[1499]	validation_0-rmse:1.26185
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0220
RMSE                : 2.3801
R2                  : 0.9980
MAPE                : 0.0316
Explained Variance  : 0.9980

ğŸ† Best model from split 2
ğŸ“Š Best metrics:
   MAE: 2.2737
   RMSE: 4.0938
   R2: 0.9981
   MAPE: 0.0293
   Explained Variance: 0.9981

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-11-28 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-01 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 142.7 - 158.3 AQI
ğŸ“Š Forecast average: 151.6 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/11_29_2025_0224_pkt
ğŸ¯ Best RÂ² Score: 0.9981
ğŸ“Š Best MAE: 2.2737
ğŸ“ˆ Best MAPE: 0.0293

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 2.2736799716949463, 'RMSE': 4.093808006283339, 'R2': 0.998051106929779, 'MAPE': 0.029345618560910225, 'Explained Variance': 0.9980723857879639}, 'xgboost_additional/11_29_2025_0224_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-11-29 02:25:42 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-11-29 02:25:42,863 INFO: Closing external client and cleaning up certificates.
Connection closed.
