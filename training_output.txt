ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-05 02:26:41 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-05 02:26:44 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_05_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-05 02:26:44,534 INFO: Initializing external client
2025-12-05 02:26:44,534 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-05 02:26:45,107 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.85s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9974 records
ğŸ“Š Shape: (9974, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9969 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.6420
ğŸŒ² OOB Score: 0.9977
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.0201
RMSE                : 28.6167
R2                  : 0.8951
MAPE                : 0.0238
Explained Variance  : 0.9005
ğŸ† New best model found! RÂ² = 0.8951

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5109
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5192
RMSE                : 3.1816
R2                  : 0.9987
MAPE                : 0.0232
Explained Variance  : 0.9988
ğŸ† New best model found! RÂ² = 0.9987

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.8523
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4791
RMSE                : 3.0257
R2                  : 0.9818
MAPE                : 0.0182
Explained Variance  : 0.9829

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8551
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5628
RMSE                : 2.0445
R2                  : 0.9986
MAPE                : 0.0212
Explained Variance  : 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-04 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 171.0 - 306.5 AQI
ğŸ“ˆ Forecast average: 204.4 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 12_03_2025_0228_pkt
âœ… Loaded LightGBM forecasts from 12_03_2025_0228_pkt
ğŸ“Š Reference mean from 2 models: 124.07
ğŸ“ˆ Random Forest forecast mean: 204.39
ğŸ“Š Reference mean: 124.07
ğŸ“Š Ratio (RF/Reference): 1.65
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.607
ğŸ“Š Corrected mean: 124.07
ğŸ“Š Reduction: 80.32 AQI (39.3%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_05_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9987
ğŸ“Š Best MAE: 1.5192
ğŸ“ˆ Best MAPE: 0.0232

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.5191524795890237, 'RMSE': 3.1816333624476645, 'R2': 0.9987426543657579, 'MAPE': 0.02315413148009752, 'Explained Variance': 0.9987938260711555}, 'randomforest_additional/12_05_2025_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-05 02:27:32 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-05 02:27:32,498 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-05 02:27:32,500 INFO: Initializing external client
2025-12-05 02:27:32,500 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-05 02:27:32,980 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9974 records
ğŸ“Š Shape: (9974, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9969 records
ğŸ“Š Final dataset shape: (9969, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1598 samples
   Val:   399 samples
   Test:  1993 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.2189
Early stopping, best iteration is:
[137]	valid_0's l2: 18.6406
âœ… Model trained with 137 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.3617
RMSE                : 28.2827
R2                  : 0.8975
MAPE                : 0.0365
Explained Variance  : 0.9030
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 77.473450971753
Prediction_local [131.26080042]
Right: 108.98327984698841

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3192 samples
   Val:   798 samples
   Test:  1993 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.8011
[200]	valid_0's l2: 16.3891
[300]	valid_0's l2: 16.3172
Early stopping, best iteration is:
[258]	valid_0's l2: 16.3062
âœ… Model trained with 258 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9938
RMSE                : 3.7552
R2                  : 0.9982
MAPE                : 0.0257
Explained Variance  : 0.9983
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4787 samples
   Val:   1196 samples
   Test:  1993 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 10.6475
[200]	valid_0's l2: 9.97658
Early stopping, best iteration is:
[213]	valid_0's l2: 9.9107
âœ… Model trained with 213 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5108
RMSE                : 2.5099
R2                  : 0.9874
MAPE                : 0.0227
Explained Variance  : 0.9878
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6381 samples
   Val:   1595 samples
   Test:  1993 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 1.99557
[200]	valid_0's l2: 1.74738
[300]	valid_0's l2: 1.71856
Early stopping, best iteration is:
[256]	valid_0's l2: 1.71855
âœ… Model trained with 256 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2416
RMSE                : 2.7487
R2                  : 0.9975
MAPE                : 0.0346
Explained Variance  : 0.9975
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.5099

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 145.1 - 172.0 AQI
ğŸ“Š Forecast average: 154.1 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-04 22:00:00+00:00 to 2025-12-07 21:00:00+00:00
Forecast range: 145.1 - 172.0 AQI
Average forecast: 154.1 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_05_2025_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_05_2025_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0270 (avg)
RMSE                : 9.3241 (avg)
R2                  : 0.9702 (avg)
MAPE                : 0.0299 (avg)
Explained Variance  : 0.9716 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-05 02:27:55 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_05_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-05 02:27:55,855 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-05 02:27:55,857 INFO: Initializing external client
2025-12-05 02:27:55,857 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-05 02:27:56,373 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9974 records
ğŸ“Š Shape: (9974, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9969 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 4 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:64.36859
[100]	validation_0-rmse:3.64435
[200]	validation_0-rmse:3.57719
[300]	validation_0-rmse:3.57556
[400]	validation_0-rmse:3.57443
[500]	validation_0-rmse:3.57411
[600]	validation_0-rmse:3.57378
[700]	validation_0-rmse:3.57362
[800]	validation_0-rmse:3.57355
[900]	validation_0-rmse:3.57355
[1000]	validation_0-rmse:3.57354
[1100]	validation_0-rmse:3.57353
[1200]	validation_0-rmse:3.57351
[1300]	validation_0-rmse:3.57350
[1400]	validation_0-rmse:3.57350
[1499]	validation_0-rmse:3.57350
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.8288
RMSE                : 26.3937
R2                  : 0.9107
MAPE                : 0.0308
Explained Variance  : 0.9170

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:118.93320
[100]	validation_0-rmse:3.57681
[200]	validation_0-rmse:3.28324
[300]	validation_0-rmse:3.28060
[400]	validation_0-rmse:3.27924
[500]	validation_0-rmse:3.27855
[600]	validation_0-rmse:3.27839
[700]	validation_0-rmse:3.27810
[800]	validation_0-rmse:3.27787
[900]	validation_0-rmse:3.27784
[1000]	validation_0-rmse:3.27779
[1100]	validation_0-rmse:3.27778
[1200]	validation_0-rmse:3.27783
[1300]	validation_0-rmse:3.27783
[1400]	validation_0-rmse:3.27783
[1499]	validation_0-rmse:3.27784
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3945
RMSE                : 4.2983
R2                  : 0.9977
MAPE                : 0.0310
Explained Variance  : 0.9977

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.69822
[100]	validation_0-rmse:4.10184
[200]	validation_0-rmse:3.86489
[300]	validation_0-rmse:3.86145
[400]	validation_0-rmse:3.85883
[500]	validation_0-rmse:3.85360
[600]	validation_0-rmse:3.85266
[700]	validation_0-rmse:3.85446
[800]	validation_0-rmse:3.85421
[900]	validation_0-rmse:3.85471
[1000]	validation_0-rmse:3.85512
[1100]	validation_0-rmse:3.85538
[1200]	validation_0-rmse:3.85562
[1300]	validation_0-rmse:3.85556
[1400]	validation_0-rmse:3.85543
[1499]	validation_0-rmse:3.85531
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1260
RMSE                : 3.2650
R2                  : 0.9788
MAPE                : 0.0321
Explained Variance  : 0.9799

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:74.44646
[100]	validation_0-rmse:1.41384
[200]	validation_0-rmse:1.29197
[300]	validation_0-rmse:1.29249
[400]	validation_0-rmse:1.29553
[500]	validation_0-rmse:1.29714
[600]	validation_0-rmse:1.29697
[700]	validation_0-rmse:1.29684
[800]	validation_0-rmse:1.29670
[900]	validation_0-rmse:1.29667
[1000]	validation_0-rmse:1.29664
[1100]	validation_0-rmse:1.29658
[1200]	validation_0-rmse:1.29635
[1300]	validation_0-rmse:1.29646
[1400]	validation_0-rmse:1.29635
[1499]	validation_0-rmse:1.29635
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1900
RMSE                : 2.6322
R2                  : 0.9977
MAPE                : 0.0323
Explained Variance  : 0.9977

ğŸ† Best model from split 2
ğŸ“Š Best metrics:
   MAE: 2.3945
   RMSE: 4.2983
   R2: 0.9977
   MAPE: 0.0310
   Explained Variance: 0.9977

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-04 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-06 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-07 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 133.1 - 152.8 AQI
ğŸ“Š Forecast average: 146.0 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/12_05_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9977
ğŸ“Š Best MAE: 2.3945
ğŸ“ˆ Best MAPE: 0.0310

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 2.394531726837158, 'RMSE': 4.298291669872681, 'R2': 0.9977052211761475, 'MAPE': 0.030960291624069214, 'Explained Variance': 0.9977344870567322}, 'xgboost_additional/12_05_2025_0227_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-05 02:28:45 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-05 02:28:45,738 INFO: Closing external client and cleaning up certificates.
Connection closed.
