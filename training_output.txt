ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-11-25 02:26:31 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-11-25 02:26:34 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/11_25_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-25 02:26:34,394 INFO: Initializing external client
2025-11-25 02:26:34,394 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-25 02:26:35,201 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.80s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9737 records
ğŸ“Š Shape: (9737, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9732 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.6169
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 8.7909
RMSE                : 28.4510
R2                  : 0.8943
MAPE                : 0.0228
Explained Variance  : 0.8998
ğŸ† New best model found! RÂ² = 0.8943

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5424
ğŸŒ² OOB Score: 0.9993
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4031
RMSE                : 3.1310
R2                  : 0.9989
MAPE                : 0.0217
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9989

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.3251
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8084
RMSE                : 3.4079
R2                  : 0.9763
MAPE                : 0.0215
Explained Variance  : 0.9785

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.9284
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5470
RMSE                : 2.0440
R2                  : 0.9985
MAPE                : 0.0217
Explained Variance  : 0.9985

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-11-24 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 178.3 - 307.0 AQI
ğŸ“ˆ Forecast average: 221.7 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 11_24_2025_0224_pkt
âœ… Loaded LightGBM forecasts from 11_24_2025_0224_pkt
ğŸ“Š Reference mean from 2 models: 173.19
ğŸ“ˆ Random Forest forecast mean: 221.72
ğŸ“Š Reference mean: 173.19
ğŸ“Š Ratio (RF/Reference): 1.28
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/11_25_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9989
ğŸ“Š Best MAE: 1.4031
ğŸ“ˆ Best MAPE: 0.0217

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.4030962732203376, 'RMSE': 3.1310291499116754, 'R2': 0.9988759733137765, 'MAPE': 0.021672834688014952, 'Explained Variance': 0.9989041653981945}, 'randomforest_additional/11_25_2025_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-11-25 02:27:22 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-25 02:27:22,321 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-25 02:27:22,323 INFO: Initializing external client
2025-11-25 02:27:22,323 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-25 02:27:23,131 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.98s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9737 records
ğŸ“Š Shape: (9737, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9732 records
ğŸ“Š Final dataset shape: (9732, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1559 samples
   Val:   389 samples
   Test:  1946 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 42.5164
Early stopping, best iteration is:
[51]	valid_0's l2: 39.2864
âœ… Model trained with 51 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 13.8723
RMSE                : 34.6165
R2                  : 0.8435
MAPE                : 0.0433
Explained Variance  : 0.8568
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 63.94125053422562
Prediction_local [163.95712921]
Right: 160.67669839059354

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3116 samples
   Val:   778 samples
   Test:  1946 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.3033
[200]	valid_0's l2: 18.6057
Early stopping, best iteration is:
[228]	valid_0's l2: 18.5197
âœ… Model trained with 228 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2224
RMSE                : 4.1992
R2                  : 0.9980
MAPE                : 0.0264
Explained Variance  : 0.9980
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4672 samples
   Val:   1168 samples
   Test:  1946 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.77833
[200]	valid_0's l2: 9.51666
Early stopping, best iteration is:
[223]	valid_0's l2: 9.41619
âœ… Model trained with 223 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4286
RMSE                : 2.3255
R2                  : 0.9890
MAPE                : 0.0213
Explained Variance  : 0.9894
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6229 samples
   Val:   1557 samples
   Test:  1946 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.31641
[200]	valid_0's l2: 2.21924
Early stopping, best iteration is:
[221]	valid_0's l2: 2.20525
âœ… Model trained with 221 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2415
RMSE                : 2.7160
R2                  : 0.9973
MAPE                : 0.0346
Explained Variance  : 0.9973
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.3255

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 158.3 - 179.8 AQI
ğŸ“Š Forecast average: 167.3 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-11-24 22:00:00+00:00 to 2025-11-27 21:00:00+00:00
Forecast range: 158.3 - 179.8 AQI
Average forecast: 167.3 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/11_25_2025_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/11_25_2025_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.6912 (avg)
RMSE                : 10.9643 (avg)
R2                  : 0.9569 (avg)
MAPE                : 0.0314 (avg)
Explained Variance  : 0.9604 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-11-25 02:27:45 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/11_25_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-25 02:27:45,676 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-25 02:27:45,679 INFO: Initializing external client
2025-11-25 02:27:45,680 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-25 02:27:46,483 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.76s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9737 records
ğŸ“Š Shape: (9737, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9732 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:65.52136
[100]	validation_0-rmse:4.68929
[200]	validation_0-rmse:4.70170
[300]	validation_0-rmse:4.70005
[400]	validation_0-rmse:4.69984
[500]	validation_0-rmse:4.69932
[600]	validation_0-rmse:4.69921
[700]	validation_0-rmse:4.69910
[800]	validation_0-rmse:4.69902
[900]	validation_0-rmse:4.69899
[1000]	validation_0-rmse:4.69899
[1100]	validation_0-rmse:4.69899
[1200]	validation_0-rmse:4.69898
[1300]	validation_0-rmse:4.69898
[1400]	validation_0-rmse:4.69898
[1499]	validation_0-rmse:4.69898
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.2181
RMSE                : 27.1446
R2                  : 0.9038
MAPE                : 0.0316
Explained Variance  : 0.9102

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:127.91728
[100]	validation_0-rmse:4.32961
[200]	validation_0-rmse:3.94264
[300]	validation_0-rmse:3.93850
[400]	validation_0-rmse:3.93731
[500]	validation_0-rmse:3.93728
[600]	validation_0-rmse:3.93712
[700]	validation_0-rmse:3.93721
[800]	validation_0-rmse:3.93708
[900]	validation_0-rmse:3.93715
[1000]	validation_0-rmse:3.93712
[1100]	validation_0-rmse:3.93714
[1200]	validation_0-rmse:3.93713
[1300]	validation_0-rmse:3.93713
[1400]	validation_0-rmse:3.93713
[1499]	validation_0-rmse:3.93713
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4506
RMSE                : 4.5959
R2                  : 0.9976
MAPE                : 0.0306
Explained Variance  : 0.9976

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:87.14057
[100]	validation_0-rmse:3.86032
[200]	validation_0-rmse:3.66447
[300]	validation_0-rmse:3.64228
[400]	validation_0-rmse:3.63372
[500]	validation_0-rmse:3.63260
[600]	validation_0-rmse:3.63066
[700]	validation_0-rmse:3.63179
[800]	validation_0-rmse:3.63314
[900]	validation_0-rmse:3.63372
[1000]	validation_0-rmse:3.63361
[1100]	validation_0-rmse:3.63381
[1200]	validation_0-rmse:3.63415
[1300]	validation_0-rmse:3.63434
[1400]	validation_0-rmse:3.63458
[1499]	validation_0-rmse:3.63453
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4431
RMSE                : 3.7456
R2                  : 0.9714
MAPE                : 0.0351
Explained Variance  : 0.9728

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:72.22332
[100]	validation_0-rmse:1.45117
[200]	validation_0-rmse:1.35505
[300]	validation_0-rmse:1.35487
[400]	validation_0-rmse:1.35476
[500]	validation_0-rmse:1.35453
[600]	validation_0-rmse:1.35550
[700]	validation_0-rmse:1.35559
[800]	validation_0-rmse:1.35567
[900]	validation_0-rmse:1.35622
[1000]	validation_0-rmse:1.35647
[1100]	validation_0-rmse:1.35665
[1200]	validation_0-rmse:1.35661
[1300]	validation_0-rmse:1.35672
[1400]	validation_0-rmse:1.35678
[1499]	validation_0-rmse:1.35688
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0120
RMSE                : 2.4844
R2                  : 0.9977
MAPE                : 0.0313
Explained Variance  : 0.9977

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0120
   RMSE: 2.4844
   R2: 0.9977
   MAPE: 0.0313
   Explained Variance: 0.9977

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-11-24 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-24 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-25 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-26 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 157.4 - 169.1 AQI
ğŸ“Š Forecast average: 163.4 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/11_25_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9977
ğŸ“Š Best MAE: 1.0120
ğŸ“ˆ Best MAPE: 0.0313

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 1.0119599103927612, 'RMSE': 2.484374712098303, 'R2': 0.9977225065231323, 'MAPE': 0.03126680478453636, 'Explained Variance': 0.9977226853370667}, 'xgboost_additional/11_25_2025_0227_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-11-25 02:28:41 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-11-25 02:28:41,930 INFO: Closing external client and cleaning up certificates.
Connection closed.
