ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-13 02:25:28 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-13 02:25:30 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_13_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-13 02:25:30,610 INFO: Initializing external client
2025-12-13 02:25:30,610 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-13 02:25:31,245 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.05s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10162 records
ğŸ“Š Shape: (10162, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10157 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.4005
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.1206
RMSE                : 29.0583
R2                  : 0.8928
MAPE                : 0.0236
Explained Variance  : 0.8985
ğŸ† New best model found! RÂ² = 0.8928

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.6892
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7408
RMSE                : 3.4069
R2                  : 0.9984
MAPE                : 0.0253
Explained Variance  : 0.9985
ğŸ† New best model found! RÂ² = 0.9984

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.1515
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3260
RMSE                : 2.8470
R2                  : 0.9847
MAPE                : 0.0234
Explained Variance  : 0.9854

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8271
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5717
RMSE                : 2.0803
R2                  : 0.9985
MAPE                : 0.0202
Explained Variance  : 0.9986
ğŸ† New best model found! RÂ² = 0.9985

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-12 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 96.7 - 268.3 AQI
ğŸ“ˆ Forecast average: 180.4 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_12_2025_0229_pkt
ğŸ“Š Reference mean from 1 models: 93.88
ğŸ“ˆ Random Forest forecast mean: 180.41
ğŸ“Š Reference mean: 93.88
ğŸ“Š Ratio (RF/Reference): 1.92
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.520
ğŸ“Š Corrected mean: 93.88
ğŸ“Š Reduction: 86.52 AQI (48.0%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_13_2025_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9985
ğŸ“Š Best MAE: 0.5717
ğŸ“ˆ Best MAPE: 0.0202

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.5716555031174334, 'RMSE': 2.0802757808417898, 'R2': 0.998513885118025, 'MAPE': 0.02018779667890115, 'Explained Variance': 0.9985700366775537}, 'randomforest_additional/12_13_2025_0225_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-13 02:26:32 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-13 02:26:32,401 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-13 02:26:32,404 INFO: Initializing external client
2025-12-13 02:26:32,404 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-13 02:26:33,031 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.01s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10162 records
ğŸ“Š Shape: (10162, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10157 records
ğŸ“Š Final dataset shape: (10157, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1627 samples
   Val:   406 samples
   Test:  2031 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 20.4127
Early stopping, best iteration is:
[81]	valid_0's l2: 20.3819
âœ… Model trained with 81 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.8489
RMSE                : 30.4072
R2                  : 0.8826
MAPE                : 0.0369
Explained Variance  : 0.8896
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.56733305749782
Prediction_local [172.79590221]
Right: 214.68006304331905

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3252 samples
   Val:   812 samples
   Test:  2031 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 18.6667
[200]	valid_0's l2: 18.0338
[300]	valid_0's l2: 18.0022
Early stopping, best iteration is:
[257]	valid_0's l2: 17.9657
âœ… Model trained with 257 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2886
RMSE                : 3.8987
R2                  : 0.9979
MAPE                : 0.0296
Explained Variance  : 0.9979
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4876 samples
   Val:   1219 samples
   Test:  2031 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.2662
[200]	valid_0's l2: 12.0841
Early stopping, best iteration is:
[232]	valid_0's l2: 11.8412
âœ… Model trained with 232 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4478
RMSE                : 2.5194
R2                  : 0.9880
MAPE                : 0.0321
Explained Variance  : 0.9884
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6501 samples
   Val:   1625 samples
   Test:  2031 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 1.97819
[200]	valid_0's l2: 1.88188
[300]	valid_0's l2: 1.86737
Early stopping, best iteration is:
[252]	valid_0's l2: 1.8663
âœ… Model trained with 252 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5213
RMSE                : 2.9616
R2                  : 0.9970
MAPE                : 0.0339
Explained Variance  : 0.9970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.5194

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 42.5 - 116.1 AQI
ğŸ“Š Forecast average: 60.2 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-12 22:00:00+00:00 to 2025-12-15 21:00:00+00:00
Forecast range: 42.5 - 116.1 AQI
Average forecast: 60.2 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_13_2025_0226_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_13_2025_0226_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2767 (avg)
RMSE                : 9.9467 (avg)
R2                  : 0.9664 (avg)
MAPE                : 0.0331 (avg)
Explained Variance  : 0.9682 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-13 02:26:56 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_13_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-13 02:26:56,428 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-13 02:26:56,430 INFO: Initializing external client
2025-12-13 02:26:56,430 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-13 02:26:56,983 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.57s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10162 records
ğŸ“Š Shape: (10162, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10157 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:64.49049
[100]	validation_0-rmse:3.48143
[200]	validation_0-rmse:3.39930
[300]	validation_0-rmse:3.39721
[400]	validation_0-rmse:3.39673
[500]	validation_0-rmse:3.39624
[600]	validation_0-rmse:3.39609
[700]	validation_0-rmse:3.39612
[800]	validation_0-rmse:3.39608
[900]	validation_0-rmse:3.39609
[1000]	validation_0-rmse:3.39609
[1100]	validation_0-rmse:3.39608
[1200]	validation_0-rmse:3.39608
[1300]	validation_0-rmse:3.39608
[1400]	validation_0-rmse:3.39607
[1499]	validation_0-rmse:3.39606
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.9821
RMSE                : 26.8937
R2                  : 0.9082
MAPE                : 0.0309
Explained Variance  : 0.9145

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:121.26147
[100]	validation_0-rmse:3.70639
[200]	validation_0-rmse:3.43138
[300]	validation_0-rmse:3.42956
[400]	validation_0-rmse:3.42894
[500]	validation_0-rmse:3.42903
[600]	validation_0-rmse:3.42914
[700]	validation_0-rmse:3.42894
[800]	validation_0-rmse:3.42882
[900]	validation_0-rmse:3.42875
[1000]	validation_0-rmse:3.42869
[1100]	validation_0-rmse:3.42872
[1200]	validation_0-rmse:3.42869
[1300]	validation_0-rmse:3.42872
[1400]	validation_0-rmse:3.42872
[1499]	validation_0-rmse:3.42872
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5874
RMSE                : 4.3612
R2                  : 0.9973
MAPE                : 0.0336
Explained Variance  : 0.9975

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.20581
[100]	validation_0-rmse:4.99777
[200]	validation_0-rmse:4.71396
[300]	validation_0-rmse:4.67833
[400]	validation_0-rmse:4.66812
[500]	validation_0-rmse:4.67110
[600]	validation_0-rmse:4.66794
[700]	validation_0-rmse:4.66486
[800]	validation_0-rmse:4.66153
[900]	validation_0-rmse:4.66206
[1000]	validation_0-rmse:4.66021
[1100]	validation_0-rmse:4.65996
[1200]	validation_0-rmse:4.66016
[1300]	validation_0-rmse:4.66010
[1400]	validation_0-rmse:4.66047
[1499]	validation_0-rmse:4.66059
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0466
RMSE                : 3.3436
R2                  : 0.9789
MAPE                : 0.0390
Explained Variance  : 0.9808

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:77.99092
[100]	validation_0-rmse:1.41896
[200]	validation_0-rmse:1.32623
[300]	validation_0-rmse:1.32632
[400]	validation_0-rmse:1.32421
[500]	validation_0-rmse:1.32380
[600]	validation_0-rmse:1.32218
[700]	validation_0-rmse:1.32250
[800]	validation_0-rmse:1.32175
[900]	validation_0-rmse:1.32201
[1000]	validation_0-rmse:1.32184
[1100]	validation_0-rmse:1.32171
[1200]	validation_0-rmse:1.32150
[1300]	validation_0-rmse:1.32146
[1400]	validation_0-rmse:1.32151
[1499]	validation_0-rmse:1.32157
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0459
RMSE                : 2.5323
R2                  : 0.9978
MAPE                : 0.0286
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0459
   RMSE: 2.5323
   R2: 0.9978
   MAPE: 0.0286
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-12 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 37.1 - 55.1 AQI
ğŸ“Š Forecast average: 46.9 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-13 02:27:53 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-13 02:27:53,489 INFO: Closing external client and cleaning up certificates.
Connection closed.
