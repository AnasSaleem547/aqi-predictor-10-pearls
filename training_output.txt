ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-28 02:25:37 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-28 02:25:39 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_28_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-28 02:25:39,797 INFO: Initializing external client
2025-12-28 02:25:39,797 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-28 02:25:40,559 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.70s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10517 records
ğŸ“Š Shape: (10517, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10512 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.3154
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.6600
RMSE                : 30.3378
R2                  : 0.8921
MAPE                : 0.0249
Explained Variance  : 0.8984
ğŸ† New best model found! RÂ² = 0.8921

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.0911
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8572
RMSE                : 3.6096
R2                  : 0.9975
MAPE                : 0.0264
Explained Variance  : 0.9977
ğŸ† New best model found! RÂ² = 0.9975

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.9124
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4212
RMSE                : 2.8668
R2                  : 0.9863
MAPE                : 0.0582
Explained Variance  : 0.9873

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2732
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4420
RMSE                : 2.5103
R2                  : 0.9980
MAPE                : 0.0056
Explained Variance  : 0.9980
ğŸ† New best model found! RÂ² = 0.9980

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-27 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 165.0 - 281.1 AQI
ğŸ“ˆ Forecast average: 188.5 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_27_2025_0230_pkt
ğŸ“Š Reference mean from 1 models: 84.44
ğŸ“ˆ Random Forest forecast mean: 188.45
ğŸ“Š Reference mean: 84.44
ğŸ“Š Ratio (RF/Reference): 2.23
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.448
ğŸ“Š Corrected mean: 84.44
ğŸ“Š Reduction: 104.01 AQI (55.2%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_28_2025_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9980
ğŸ“Š Best MAE: 0.4420
ğŸ“ˆ Best MAPE: 0.0056

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.4419616084056045, 'RMSE': 2.510257110940923, 'R2': 0.9979799238214564, 'MAPE': 0.005612756480011519, 'Explained Variance': 0.9980137115950185}, 'randomforest_additional/12_28_2025_0225_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-28 02:26:46 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-28 02:26:46,377 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-28 02:26:46,379 INFO: Initializing external client
2025-12-28 02:26:46,379 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-28 02:26:47,150 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.29s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10517 records
ğŸ“Š Shape: (10517, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10512 records
ğŸ“Š Final dataset shape: (10512, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1684 samples
   Val:   420 samples
   Test:  2102 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 217.657
[200]	valid_0's l2: 190.725
[300]	valid_0's l2: 181.64
[400]	valid_0's l2: 177.812
[500]	valid_0's l2: 176.806
Early stopping, best iteration is:
[453]	valid_0's l2: 176.764
âœ… Model trained with 453 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.5776
RMSE                : 28.4965
R2                  : 0.9048
MAPE                : 0.0365
Explained Variance  : 0.9111
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 67.1978727683402
Prediction_local [169.94115766]
Right: 171.95900564563814

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3365 samples
   Val:   841 samples
   Test:  2102 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.037
Early stopping, best iteration is:
[142]	valid_0's l2: 16.8236
âœ… Model trained with 142 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2131
RMSE                : 3.7050
R2                  : 0.9974
MAPE                : 0.0302
Explained Variance  : 0.9975
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5047 samples
   Val:   1261 samples
   Test:  2102 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.51383
Early stopping, best iteration is:
[79]	valid_0's l2: 9.1311
âœ… Model trained with 79 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9353
RMSE                : 3.4795
R2                  : 0.9799
MAPE                : 0.0824
Explained Variance  : 0.9818
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6728 samples
   Val:   1682 samples
   Test:  2102 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.43788
Early stopping, best iteration is:
[137]	valid_0's l2: 3.34198
âœ… Model trained with 137 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7459
RMSE                : 3.7756
R2                  : 0.9954
MAPE                : 0.0173
Explained Variance  : 0.9956
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.4795

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 97.8 - 168.9 AQI
ğŸ“Š Forecast average: 129.1 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-27 22:00:00+00:00 to 2025-12-30 21:00:00+00:00
Forecast range: 97.8 - 168.9 AQI
Average forecast: 129.1 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_28_2025_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_28_2025_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3680 (avg)
RMSE                : 9.8642 (avg)
R2                  : 0.9694 (avg)
MAPE                : 0.0416 (avg)
Explained Variance  : 0.9715 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-28 02:27:07 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_28_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-28 02:27:07,986 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-28 02:27:07,988 INFO: Initializing external client
2025-12-28 02:27:07,989 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-28 02:27:08,729 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.59s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10517 records
ğŸ“Š Shape: (10517, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10512 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:91.23663
[100]	validation_0-rmse:13.10326
[200]	validation_0-rmse:12.90116
[300]	validation_0-rmse:12.89673
[400]	validation_0-rmse:12.89503
[500]	validation_0-rmse:12.89418
[600]	validation_0-rmse:12.89419
[700]	validation_0-rmse:12.89421
[800]	validation_0-rmse:12.89413
[900]	validation_0-rmse:12.89415
[1000]	validation_0-rmse:12.89413
[1100]	validation_0-rmse:12.89414
[1200]	validation_0-rmse:12.89414
[1300]	validation_0-rmse:12.89413
[1400]	validation_0-rmse:12.89412
[1499]	validation_0-rmse:12.89412
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.3170
RMSE                : 27.8865
R2                  : 0.9088
MAPE                : 0.0317
Explained Variance  : 0.9153

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:113.40889
[100]	validation_0-rmse:3.58640
[200]	validation_0-rmse:3.34857
[300]	validation_0-rmse:3.34654
[400]	validation_0-rmse:3.34599
[500]	validation_0-rmse:3.34590
[600]	validation_0-rmse:3.34597
[700]	validation_0-rmse:3.34567
[800]	validation_0-rmse:3.34559
[900]	validation_0-rmse:3.34558
[1000]	validation_0-rmse:3.34550
[1100]	validation_0-rmse:3.34551
[1200]	validation_0-rmse:3.34553
[1300]	validation_0-rmse:3.34559
[1400]	validation_0-rmse:3.34561
[1499]	validation_0-rmse:3.34561
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.7441
RMSE                : 4.5067
R2                  : 0.9962
MAPE                : 0.0375
Explained Variance  : 0.9963

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.14962
[100]	validation_0-rmse:5.59214
[200]	validation_0-rmse:5.28739
[300]	validation_0-rmse:5.27960
[400]	validation_0-rmse:5.27380
[500]	validation_0-rmse:5.27504
[600]	validation_0-rmse:5.27172
[700]	validation_0-rmse:5.27470
[800]	validation_0-rmse:5.27294
[900]	validation_0-rmse:5.27547
[1000]	validation_0-rmse:5.27273
[1100]	validation_0-rmse:5.27104
[1200]	validation_0-rmse:5.26987
[1300]	validation_0-rmse:5.27002
[1400]	validation_0-rmse:5.27004
[1499]	validation_0-rmse:5.26991
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 3.2251
RMSE                : 4.6600
R2                  : 0.9639
MAPE                : 0.0876
Explained Variance  : 0.9704

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.89398
[100]	validation_0-rmse:1.87762
[200]	validation_0-rmse:1.74672
[300]	validation_0-rmse:1.73903
[400]	validation_0-rmse:1.73515
[500]	validation_0-rmse:1.73495
[600]	validation_0-rmse:1.73444
[700]	validation_0-rmse:1.73329
[800]	validation_0-rmse:1.73282
[900]	validation_0-rmse:1.73243
[1000]	validation_0-rmse:1.73210
[1100]	validation_0-rmse:1.73193
[1200]	validation_0-rmse:1.73100
[1300]	validation_0-rmse:1.73113
[1400]	validation_0-rmse:1.73149
[1499]	validation_0-rmse:1.73170
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2842
RMSE                : 3.2166
R2                  : 0.9967
MAPE                : 0.0146
Explained Variance  : 0.9968

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.2842
   RMSE: 3.2166
   R2: 0.9967
   MAPE: 0.0146
   Explained Variance: 0.9968

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-27 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-30 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 151.4 - 164.9 AQI
ğŸ“Š Forecast average: 156.2 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-28 02:28:07 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-28 02:28:07,848 INFO: Closing external client and cleaning up certificates.
Connection closed.
