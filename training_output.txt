ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-17 02:28:33 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-17 02:28:36 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_17_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-17 02:28:36,043 INFO: Initializing external client
2025-12-17 02:28:36,043 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-17 02:28:37,091 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.67s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10257 records
ğŸ“Š Shape: (10257, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10252 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.5504
ğŸŒ² OOB Score: 0.9977
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.1261
RMSE                : 28.9882
R2                  : 0.8935
MAPE                : 0.0237
Explained Variance  : 0.8992
ğŸ† New best model found! RÂ² = 0.8935

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5104
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8816
RMSE                : 3.7057
R2                  : 0.9981
MAPE                : 0.0265
Explained Variance  : 0.9982
ğŸ† New best model found! RÂ² = 0.9981

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5479
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3934
RMSE                : 2.7977
R2                  : 0.9853
MAPE                : 0.0394
Explained Variance  : 0.9865

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.0185
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5299
RMSE                : 2.0250
R2                  : 0.9987
MAPE                : 0.0144
Explained Variance  : 0.9987
ğŸ† New best model found! RÂ² = 0.9987

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-16 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 183.2 - 292.0 AQI
ğŸ“ˆ Forecast average: 224.3 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_16_2025_0229_pkt
ğŸ“Š Reference mean from 1 models: 179.15
ğŸ“ˆ Random Forest forecast mean: 224.32
ğŸ“Š Reference mean: 179.15
ğŸ“Š Ratio (RF/Reference): 1.25
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_17_2025_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9987
ğŸ“Š Best MAE: 0.5299
ğŸ“ˆ Best MAPE: 0.0144

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.5298694726804402, 'RMSE': 2.025031649327922, 'R2': 0.9986682074134667, 'MAPE': 0.014437620569985439, 'Explained Variance': 0.9987117848850746}, 'randomforest_additional/12_17_2025_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-17 02:29:42 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-17 02:29:42,748 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-17 02:29:42,750 INFO: Initializing external client
2025-12-17 02:29:42,750 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-17 02:29:43,840 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.15s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10257 records
ğŸ“Š Shape: (10257, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10252 records
ğŸ“Š Final dataset shape: (10252, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1642 samples
   Val:   410 samples
   Test:  2050 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.3539
[200]	valid_0's l2: 16.9733
Early stopping, best iteration is:
[151]	valid_0's l2: 16.8542
âœ… Model trained with 151 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.5516
RMSE                : 28.4117
R2                  : 0.8977
MAPE                : 0.0373
Explained Variance  : 0.9021
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 64.59209138338645
Prediction_local [173.8848146]
Right: 258.9223356276745

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3282 samples
   Val:   820 samples
   Test:  2050 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.0123
[200]	valid_0's l2: 12.555
[300]	valid_0's l2: 12.4879
Early stopping, best iteration is:
[265]	valid_0's l2: 12.4879
âœ… Model trained with 264 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2035
RMSE                : 3.8796
R2                  : 0.9979
MAPE                : 0.0281
Explained Variance  : 0.9979
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4922 samples
   Val:   1230 samples
   Test:  2050 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 12.19
[200]	valid_0's l2: 10.9512
Early stopping, best iteration is:
[234]	valid_0's l2: 10.8135
âœ… Model trained with 234 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5794
RMSE                : 2.7389
R2                  : 0.9859
MAPE                : 0.0513
Explained Variance  : 0.9865
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6562 samples
   Val:   1640 samples
   Test:  2050 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.54536
[200]	valid_0's l2: 2.40815
Early stopping, best iteration is:
[243]	valid_0's l2: 2.35264
âœ… Model trained with 243 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3091
RMSE                : 2.9179
R2                  : 0.9972
MAPE                : 0.0253
Explained Variance  : 0.9972
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.7389

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 175.1 - 342.9 AQI
ğŸ“Š Forecast average: 245.4 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-16 22:00:00+00:00 to 2025-12-19 21:00:00+00:00
Forecast range: 175.1 - 342.9 AQI
Average forecast: 245.4 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_17_2025_0230_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_17_2025_0230_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.1609 (avg)
RMSE                : 9.4870 (avg)
R2                  : 0.9697 (avg)
MAPE                : 0.0355 (avg)
Explained Variance  : 0.9709 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-17 02:30:08 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_17_2025_0230_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-17 02:30:08,707 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-17 02:30:08,709 INFO: Initializing external client
2025-12-17 02:30:08,709 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-17 02:30:09,699 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.21s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10257 records
ğŸ“Š Shape: (10257, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10252 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:67.08606
[100]	validation_0-rmse:3.70979
[200]	validation_0-rmse:3.61534
[300]	validation_0-rmse:3.61314
[400]	validation_0-rmse:3.61270
[500]	validation_0-rmse:3.61233
[600]	validation_0-rmse:3.61227
[700]	validation_0-rmse:3.61224
[800]	validation_0-rmse:3.61225
[900]	validation_0-rmse:3.61227
[1000]	validation_0-rmse:3.61229
[1100]	validation_0-rmse:3.61228
[1200]	validation_0-rmse:3.61228
[1300]	validation_0-rmse:3.61228
[1400]	validation_0-rmse:3.61228
[1499]	validation_0-rmse:3.61228
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.9606
RMSE                : 26.7768
R2                  : 0.9092
MAPE                : 0.0312
Explained Variance  : 0.9152

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:115.47449
[100]	validation_0-rmse:3.33473
[200]	validation_0-rmse:2.98059
[300]	validation_0-rmse:2.97516
[400]	validation_0-rmse:2.97266
[500]	validation_0-rmse:2.97182
[600]	validation_0-rmse:2.97181
[700]	validation_0-rmse:2.97127
[800]	validation_0-rmse:2.97096
[900]	validation_0-rmse:2.97092
[1000]	validation_0-rmse:2.97093
[1100]	validation_0-rmse:2.97096
[1200]	validation_0-rmse:2.97094
[1300]	validation_0-rmse:2.97096
[1400]	validation_0-rmse:2.97097
[1499]	validation_0-rmse:2.97100
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6721
RMSE                : 4.5539
R2                  : 0.9971
MAPE                : 0.0347
Explained Variance  : 0.9972

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.40395
[100]	validation_0-rmse:5.12460
[200]	validation_0-rmse:4.91026
[300]	validation_0-rmse:4.89745
[400]	validation_0-rmse:4.89007
[500]	validation_0-rmse:4.88624
[600]	validation_0-rmse:4.88427
[700]	validation_0-rmse:4.88261
[800]	validation_0-rmse:4.88050
[900]	validation_0-rmse:4.87959
[1000]	validation_0-rmse:4.87906
[1100]	validation_0-rmse:4.87923
[1200]	validation_0-rmse:4.87833
[1300]	validation_0-rmse:4.87796
[1400]	validation_0-rmse:4.87805
[1499]	validation_0-rmse:4.87806
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2145
RMSE                : 3.4831
R2                  : 0.9772
MAPE                : 0.0552
Explained Variance  : 0.9782

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:80.14172
[100]	validation_0-rmse:1.56711
[200]	validation_0-rmse:1.42975
[300]	validation_0-rmse:1.42809
[400]	validation_0-rmse:1.42702
[500]	validation_0-rmse:1.42582
[600]	validation_0-rmse:1.42611
[700]	validation_0-rmse:1.42555
[800]	validation_0-rmse:1.42505
[900]	validation_0-rmse:1.42470
[1000]	validation_0-rmse:1.42453
[1100]	validation_0-rmse:1.42464
[1200]	validation_0-rmse:1.42482
[1300]	validation_0-rmse:1.42470
[1400]	validation_0-rmse:1.42466
[1499]	validation_0-rmse:1.42435
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0366
RMSE                : 2.5826
R2                  : 0.9978
MAPE                : 0.0221
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0366
   RMSE: 2.5826
   R2: 0.9978
   MAPE: 0.0221
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-16 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-17 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-18 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-19 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 198.3 - 260.7 AQI
ğŸ“Š Forecast average: 225.2 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-17 02:31:08 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-17 02:31:08,741 INFO: Closing external client and cleaning up certificates.
Connection closed.
