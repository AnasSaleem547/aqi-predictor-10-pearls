ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-24 02:26:47 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-24 02:26:49 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_24_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-24 02:26:49,504 INFO: Initializing external client
2025-12-24 02:26:49,504 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-24 02:26:50,724 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.16s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10423 records
ğŸ“Š Shape: (10423, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10418 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.3889
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.8108
RMSE                : 30.6177
R2                  : 0.8881
MAPE                : 0.0249
Explained Variance  : 0.8949
ğŸ† New best model found! RÂ² = 0.8881

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.6343
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9062
RMSE                : 3.7572
R2                  : 0.9973
MAPE                : 0.0271
Explained Variance  : 0.9975
ğŸ† New best model found! RÂ² = 0.9973

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5125
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5533
RMSE                : 3.1187
R2                  : 0.9841
MAPE                : 0.0592
Explained Variance  : 0.9857

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2391
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4229
RMSE                : 1.9104
R2                  : 0.9989
MAPE                : 0.0057
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9989

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-23 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 169.0 - 305.7 AQI
ğŸ“ˆ Forecast average: 218.7 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_22_2025_0226_pkt
ğŸ“Š Reference mean from 1 models: 105.78
ğŸ“ˆ Random Forest forecast mean: 218.67
ğŸ“Š Reference mean: 105.78
ğŸ“Š Ratio (RF/Reference): 2.07
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.484
ğŸ“Š Corrected mean: 105.78
ğŸ“Š Reduction: 112.89 AQI (51.6%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_24_2025_0226_pkt
ğŸ¯ Best RÂ² Score: 0.9989
ğŸ“Š Best MAE: 0.4229
ğŸ“ˆ Best MAPE: 0.0057

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.42293021593411945, 'RMSE': 1.9104247511081658, 'R2': 0.9988835937643057, 'MAPE': 0.005656067998685755, 'Explained Variance': 0.9989042914905266}, 'randomforest_additional/12_24_2025_0226_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-24 02:27:50 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-24 02:27:50,309 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-24 02:27:50,311 INFO: Initializing external client
2025-12-24 02:27:50,312 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-24 02:27:51,468 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.10s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10423 records
ğŸ“Š Shape: (10423, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10418 records
ğŸ“Š Final dataset shape: (10418, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1669 samples
   Val:   417 samples
   Test:  2083 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 217.543
[200]	valid_0's l2: 188.936
[300]	valid_0's l2: 179.448
[400]	valid_0's l2: 175.403
[500]	valid_0's l2: 174.229
Early stopping, best iteration is:
[460]	valid_0's l2: 174.126
âœ… Model trained with 460 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.4894
RMSE                : 28.1872
R2                  : 0.9052
MAPE                : 0.0360
Explained Variance  : 0.9114
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 66.97937074683239
Prediction_local [171.20196035]
Right: 154.1291821103513

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3336 samples
   Val:   833 samples
   Test:  2083 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.0227
[200]	valid_0's l2: 15.6576
Early stopping, best iteration is:
[225]	valid_0's l2: 15.6411
âœ… Model trained with 225 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3324
RMSE                : 3.9942
R2                  : 0.9970
MAPE                : 0.0306
Explained Variance  : 0.9970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5002 samples
   Val:   1250 samples
   Test:  2083 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 10.5639
Early stopping, best iteration is:
[97]	valid_0's l2: 10.5343
âœ… Model trained with 97 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1086
RMSE                : 3.7096
R2                  : 0.9775
MAPE                : 0.0889
Explained Variance  : 0.9812
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6668 samples
   Val:   1667 samples
   Test:  2083 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.26776
[200]	valid_0's l2: 3.20103
Early stopping, best iteration is:
[234]	valid_0's l2: 3.17796
âœ… Model trained with 234 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5056
RMSE                : 3.1760
R2                  : 0.9969
MAPE                : 0.0156
Explained Variance  : 0.9970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 3.1760

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 102.0 - 178.0 AQI
ğŸ“Š Forecast average: 136.7 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-23 22:00:00+00:00 to 2025-12-26 21:00:00+00:00
Forecast range: 102.0 - 178.0 AQI
Average forecast: 136.7 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_24_2025_0228_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_24_2025_0228_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3590 (avg)
RMSE                : 9.7668 (avg)
R2                  : 0.9691 (avg)
MAPE                : 0.0428 (avg)
Explained Variance  : 0.9717 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-24 02:28:14 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_24_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-24 02:28:14,224 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-24 02:28:14,226 INFO: Initializing external client
2025-12-24 02:28:14,227 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-24 02:28:15,420 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.05s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10423 records
ğŸ“Š Shape: (10423, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10418 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:87.65033
[100]	validation_0-rmse:13.12261
[200]	validation_0-rmse:12.92033
[300]	validation_0-rmse:12.91811
[400]	validation_0-rmse:12.91722
[500]	validation_0-rmse:12.91706
[600]	validation_0-rmse:12.91697
[700]	validation_0-rmse:12.91693
[800]	validation_0-rmse:12.91681
[900]	validation_0-rmse:12.91686
[1000]	validation_0-rmse:12.91683
[1100]	validation_0-rmse:12.91684
[1200]	validation_0-rmse:12.91685
[1300]	validation_0-rmse:12.91685
[1400]	validation_0-rmse:12.91684
[1499]	validation_0-rmse:12.91685
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.3941
RMSE                : 27.9764
R2                  : 0.9066
MAPE                : 0.0316
Explained Variance  : 0.9135

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:117.36224
[100]	validation_0-rmse:3.15107
[200]	validation_0-rmse:2.90247
[300]	validation_0-rmse:2.90080
[400]	validation_0-rmse:2.89935
[500]	validation_0-rmse:2.89869
[600]	validation_0-rmse:2.89867
[700]	validation_0-rmse:2.89875
[800]	validation_0-rmse:2.89882
[900]	validation_0-rmse:2.89877
[1000]	validation_0-rmse:2.89880
[1100]	validation_0-rmse:2.89879
[1200]	validation_0-rmse:2.89876
[1300]	validation_0-rmse:2.89879
[1400]	validation_0-rmse:2.89880
[1499]	validation_0-rmse:2.89880
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5990
RMSE                : 4.3904
R2                  : 0.9964
MAPE                : 0.0356
Explained Variance  : 0.9966

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.17928
[100]	validation_0-rmse:4.36731
[200]	validation_0-rmse:4.16169
[300]	validation_0-rmse:4.14716
[400]	validation_0-rmse:4.14733
[500]	validation_0-rmse:4.14495
[600]	validation_0-rmse:4.14360
[700]	validation_0-rmse:4.14769
[800]	validation_0-rmse:4.14487
[900]	validation_0-rmse:4.14397
[1000]	validation_0-rmse:4.14532
[1100]	validation_0-rmse:4.14538
[1200]	validation_0-rmse:4.14553
[1300]	validation_0-rmse:4.14566
[1400]	validation_0-rmse:4.14570
[1499]	validation_0-rmse:4.14577
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3776
RMSE                : 3.6723
R2                  : 0.9779
MAPE                : 0.0760
Explained Variance  : 0.9795

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.98914
[100]	validation_0-rmse:1.75097
[200]	validation_0-rmse:1.59553
[300]	validation_0-rmse:1.59622
[400]	validation_0-rmse:1.59713
[500]	validation_0-rmse:1.59589
[600]	validation_0-rmse:1.59605
[700]	validation_0-rmse:1.59765
[800]	validation_0-rmse:1.59845
[900]	validation_0-rmse:1.60085
[1000]	validation_0-rmse:1.60066
[1100]	validation_0-rmse:1.60055
[1200]	validation_0-rmse:1.60062
[1300]	validation_0-rmse:1.60042
[1400]	validation_0-rmse:1.60036
[1499]	validation_0-rmse:1.60118
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1958
RMSE                : 2.8654
R2                  : 0.9975
MAPE                : 0.0138
Explained Variance  : 0.9976

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1958
   RMSE: 2.8654
   R2: 0.9975
   MAPE: 0.0138
   Explained Variance: 0.9976

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-23 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-25 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 129.5 - 165.2 AQI
ğŸ“Š Forecast average: 155.7 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-24 02:29:14 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-24 02:29:14,275 INFO: Closing external client and cleaning up certificates.
Connection closed.
