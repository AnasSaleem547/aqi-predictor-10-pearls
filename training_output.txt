ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-10 02:29:47 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-10 02:29:50 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_10_2025_0229_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-10 02:29:50,386 INFO: Initializing external client
2025-12-10 02:29:50,386 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-10 02:29:51,211 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.63s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10093 records
ğŸ“Š Shape: (10093, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10088 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.7425
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.0230
RMSE                : 28.5072
R2                  : 0.8947
MAPE                : 0.0238
Explained Variance  : 0.9001
ğŸ† New best model found! RÂ² = 0.8947

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5322
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6689
RMSE                : 3.3433
R2                  : 0.9985
MAPE                : 0.0247
Explained Variance  : 0.9986
ğŸ† New best model found! RÂ² = 0.9985

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.9589
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4262
RMSE                : 2.9736
R2                  : 0.9844
MAPE                : 0.0241
Explained Variance  : 0.9852

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8210
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5711
RMSE                : 2.0351
R2                  : 0.9986
MAPE                : 0.0199
Explained Variance  : 0.9987
ğŸ† New best model found! RÂ² = 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-09 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 172.2 - 295.0 AQI
ğŸ“ˆ Forecast average: 218.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 12_09_2025_0223_pkt
âœ… Loaded LightGBM forecasts from 12_09_2025_0223_pkt
ğŸ“Š Reference mean from 2 models: 183.24
ğŸ“ˆ Random Forest forecast mean: 218.03
ğŸ“Š Reference mean: 183.24
ğŸ“Š Ratio (RF/Reference): 1.19
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_10_2025_0229_pkt
ğŸ¯ Best RÂ² Score: 0.9986
ğŸ“Š Best MAE: 0.5711
ğŸ“ˆ Best MAPE: 0.0199

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.5711266260470682, 'RMSE': 2.035121862196403, 'R2': 0.9986080668817907, 'MAPE': 0.019947643149085124, 'Explained Variance': 0.9986624229286616}, 'randomforest_additional/12_10_2025_0229_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-10 02:30:56 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-10 02:30:56,137 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-10 02:30:56,139 INFO: Initializing external client
2025-12-10 02:30:56,139 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-10 02:30:56,622 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.54s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10093 records
ğŸ“Š Shape: (10093, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10088 records
ğŸ“Š Final dataset shape: (10088, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1616 samples
   Val:   404 samples
   Test:  2017 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 20.986
Early stopping, best iteration is:
[141]	valid_0's l2: 20.8334
âœ… Model trained with 141 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.0882
RMSE                : 27.7248
R2                  : 0.9004
MAPE                : 0.0355
Explained Variance  : 0.9049
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.01052146440952
Prediction_local [168.97266162]
Right: 166.69366473182848

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3230 samples
   Val:   807 samples
   Test:  2017 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 15.9415
[200]	valid_0's l2: 15.3914
[300]	valid_0's l2: 15.3663
Early stopping, best iteration is:
[258]	valid_0's l2: 15.3264
âœ… Model trained with 258 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1632
RMSE                : 3.8937
R2                  : 0.9980
MAPE                : 0.0282
Explained Variance  : 0.9980
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4844 samples
   Val:   1210 samples
   Test:  2017 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 11.5035
[200]	valid_0's l2: 10.7165
Early stopping, best iteration is:
[215]	valid_0's l2: 10.5644
âœ… Model trained with 215 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5302
RMSE                : 2.6150
R2                  : 0.9879
MAPE                : 0.0326
Explained Variance  : 0.9882
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6457 samples
   Val:   1614 samples
   Test:  2017 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 1.83318
[200]	valid_0's l2: 1.7143
Early stopping, best iteration is:
[221]	valid_0's l2: 1.6968
âœ… Model trained with 221 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3156
RMSE                : 2.7525
R2                  : 0.9975
MAPE                : 0.0325
Explained Variance  : 0.9975
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.6150

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 150.6 - 169.3 AQI
ğŸ“Š Forecast average: 158.9 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-09 22:00:00+00:00 to 2025-12-12 21:00:00+00:00
Forecast range: 150.6 - 169.3 AQI
Average forecast: 158.9 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_10_2025_0231_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_10_2025_0231_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0243 (avg)
RMSE                : 9.2465 (avg)
R2                  : 0.9710 (avg)
MAPE                : 0.0322 (avg)
Explained Variance  : 0.9722 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-10 02:31:19 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_10_2025_0231_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-10 02:31:19,659 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-10 02:31:19,661 INFO: Initializing external client
2025-12-10 02:31:19,662 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-10 02:31:20,180 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.59s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10093 records
ğŸ“Š Shape: (10093, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10088 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:63.22446
[100]	validation_0-rmse:3.54098
[200]	validation_0-rmse:3.49581
[300]	validation_0-rmse:3.49500
[400]	validation_0-rmse:3.49466
[500]	validation_0-rmse:3.49442
[600]	validation_0-rmse:3.49415
[700]	validation_0-rmse:3.49404
[800]	validation_0-rmse:3.49404
[900]	validation_0-rmse:3.49404
[1000]	validation_0-rmse:3.49405
[1100]	validation_0-rmse:3.49404
[1200]	validation_0-rmse:3.49404
[1300]	validation_0-rmse:3.49404
[1400]	validation_0-rmse:3.49403
[1499]	validation_0-rmse:3.49404
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.6513
RMSE                : 26.0367
R2                  : 0.9122
MAPE                : 0.0304
Explained Variance  : 0.9173

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:118.24450
[100]	validation_0-rmse:3.36861
[200]	validation_0-rmse:3.13352
[300]	validation_0-rmse:3.12979
[400]	validation_0-rmse:3.12822
[500]	validation_0-rmse:3.12715
[600]	validation_0-rmse:3.12696
[700]	validation_0-rmse:3.12664
[800]	validation_0-rmse:3.12648
[900]	validation_0-rmse:3.12637
[1000]	validation_0-rmse:3.12632
[1100]	validation_0-rmse:3.12628
[1200]	validation_0-rmse:3.12631
[1300]	validation_0-rmse:3.12633
[1400]	validation_0-rmse:3.12633
[1499]	validation_0-rmse:3.12632
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5847
RMSE                : 4.4070
R2                  : 0.9975
MAPE                : 0.0339
Explained Variance  : 0.9976

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.65615
[100]	validation_0-rmse:4.30038
[200]	validation_0-rmse:4.09213
[300]	validation_0-rmse:4.06648
[400]	validation_0-rmse:4.05600
[500]	validation_0-rmse:4.06523
[600]	validation_0-rmse:4.06829
[700]	validation_0-rmse:4.07319
[800]	validation_0-rmse:4.07361
[900]	validation_0-rmse:4.07311
[1000]	validation_0-rmse:4.07319
[1100]	validation_0-rmse:4.07366
[1200]	validation_0-rmse:4.07297
[1300]	validation_0-rmse:4.07343
[1400]	validation_0-rmse:4.07336
[1499]	validation_0-rmse:4.07360
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9412
RMSE                : 3.1157
R2                  : 0.9828
MAPE                : 0.0360
Explained Variance  : 0.9844

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:77.59317
[100]	validation_0-rmse:1.39281
[200]	validation_0-rmse:1.28886
[300]	validation_0-rmse:1.29040
[400]	validation_0-rmse:1.29115
[500]	validation_0-rmse:1.29144
[600]	validation_0-rmse:1.28990
[700]	validation_0-rmse:1.28969
[800]	validation_0-rmse:1.28969
[900]	validation_0-rmse:1.28966
[1000]	validation_0-rmse:1.28975
[1100]	validation_0-rmse:1.28974
[1200]	validation_0-rmse:1.28983
[1300]	validation_0-rmse:1.28984
[1400]	validation_0-rmse:1.28984
[1499]	validation_0-rmse:1.28992
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0481
RMSE                : 2.4535
R2                  : 0.9980
MAPE                : 0.0273
Explained Variance  : 0.9980

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0481
   RMSE: 2.4535
   R2: 0.9980
   MAPE: 0.0273
   Explained Variance: 0.9980

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-09 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-09 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-10 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-11 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-12 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 125.9 - 149.6 AQI
ğŸ“Š Forecast average: 140.1 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/12_10_2025_0231_pkt
ğŸ¯ Best RÂ² Score: 0.9980
ğŸ“Š Best MAE: 1.0481
ğŸ“ˆ Best MAPE: 0.0273

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 1.0480515956878662, 'RMSE': 2.453507314642728, 'R2': 0.9979768991470337, 'MAPE': 0.027293946593999863, 'Explained Variance': 0.9979836344718933}, 'xgboost_additional/12_10_2025_0231_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-10 02:32:17 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-10 02:32:17,380 INFO: Closing external client and cleaning up certificates.
Connection closed.
