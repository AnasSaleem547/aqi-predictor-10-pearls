ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-14 02:24:08 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-14 02:24:12 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_14_2025_0224_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-14 02:24:12,157 INFO: Initializing external client
2025-12-14 02:24:12,157 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-14 02:24:13,070 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.05s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10186 records
ğŸ“Š Shape: (10186, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10181 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.4436
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.1737
RMSE                : 29.0846
R2                  : 0.8924
MAPE                : 0.0238
Explained Variance  : 0.8981
ğŸ† New best model found! RÂ² = 0.8924

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.6916
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7984
RMSE                : 3.4920
R2                  : 0.9983
MAPE                : 0.0258
Explained Variance  : 0.9984
ğŸ† New best model found! RÂ² = 0.9983

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.2146
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2798
RMSE                : 2.7658
R2                  : 0.9852
MAPE                : 0.0230
Explained Variance  : 0.9858

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8287
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5814
RMSE                : 2.0934
R2                  : 0.9985
MAPE                : 0.0201
Explained Variance  : 0.9985
ğŸ† New best model found! RÂ² = 0.9985

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-13 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 63.8 - 269.9 AQI
ğŸ“ˆ Forecast average: 177.5 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_13_2025_0226_pkt
ğŸ“Š Reference mean from 1 models: 60.19
ğŸ“ˆ Random Forest forecast mean: 177.48
ğŸ“Š Reference mean: 60.19
ğŸ“Š Ratio (RF/Reference): 2.95
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.339
ğŸ“Š Corrected mean: 60.19
ğŸ“Š Reduction: 117.29 AQI (66.1%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_14_2025_0224_pkt
ğŸ¯ Best RÂ² Score: 0.9985
ğŸ“Š Best MAE: 0.5814
ğŸ“ˆ Best MAPE: 0.0201

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.5814201170514178, 'RMSE': 2.09340537497502, 'R2': 0.9984848230389258, 'MAPE': 0.02012399577750673, 'Explained Variance': 0.9985447573939584}, 'randomforest_additional/12_14_2025_0224_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-14 02:25:17 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-14 02:25:17,748 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-14 02:25:17,750 INFO: Initializing external client
2025-12-14 02:25:17,750 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-14 02:25:18,569 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.34s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10186 records
ğŸ“Š Shape: (10186, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10181 records
ğŸ“Š Final dataset shape: (10181, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1630 samples
   Val:   407 samples
   Test:  2036 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.7005
Early stopping, best iteration is:
[122]	valid_0's l2: 17.5948
âœ… Model trained with 122 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.2550
RMSE                : 28.4709
R2                  : 0.8969
MAPE                : 0.0358
Explained Variance  : 0.9017
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 66.59703846168382
Prediction_local [170.27604777]
Right: 161.75429528978978

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3259 samples
   Val:   814 samples
   Test:  2036 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.5551
[200]	valid_0's l2: 18.8555
Early stopping, best iteration is:
[240]	valid_0's l2: 18.7948
âœ… Model trained with 240 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1113
RMSE                : 3.7512
R2                  : 0.9980
MAPE                : 0.0279
Explained Variance  : 0.9980
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4888 samples
   Val:   1221 samples
   Test:  2036 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 12.7639
[200]	valid_0's l2: 11.5704
Early stopping, best iteration is:
[198]	valid_0's l2: 11.5655
âœ… Model trained with 198 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4862
RMSE                : 2.5217
R2                  : 0.9877
MAPE                : 0.0344
Explained Variance  : 0.9877
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6516 samples
   Val:   1629 samples
   Test:  2036 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 1.85757
[200]	valid_0's l2: 1.71394
Early stopping, best iteration is:
[246]	valid_0's l2: 1.68292
âœ… Model trained with 246 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3156
RMSE                : 2.7347
R2                  : 0.9974
MAPE                : 0.0320
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.5217

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 37.3 - 111.3 AQI
ğŸ“Š Forecast average: 47.8 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-13 22:00:00+00:00 to 2025-12-16 21:00:00+00:00
Forecast range: 37.3 - 111.3 AQI
Average forecast: 47.8 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_14_2025_0225_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_14_2025_0225_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0420 (avg)
RMSE                : 9.3696 (avg)
R2                  : 0.9700 (avg)
MAPE                : 0.0325 (avg)
Explained Variance  : 0.9712 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-14 02:25:43 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_14_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-14 02:25:43,649 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-14 02:25:43,651 INFO: Initializing external client
2025-12-14 02:25:43,652 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-14 02:25:44,608 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.98s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10186 records
ğŸ“Š Shape: (10186, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10181 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:64.78963
[100]	validation_0-rmse:3.49556
[200]	validation_0-rmse:3.42147
[300]	validation_0-rmse:3.41952
[400]	validation_0-rmse:3.41897
[500]	validation_0-rmse:3.41832
[600]	validation_0-rmse:3.41817
[700]	validation_0-rmse:3.41815
[800]	validation_0-rmse:3.41815
[900]	validation_0-rmse:3.41815
[1000]	validation_0-rmse:3.41815
[1100]	validation_0-rmse:3.41814
[1200]	validation_0-rmse:3.41814
[1300]	validation_0-rmse:3.41814
[1400]	validation_0-rmse:3.41815
[1499]	validation_0-rmse:3.41814
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.9795
RMSE                : 26.8162
R2                  : 0.9085
MAPE                : 0.0310
Explained Variance  : 0.9147

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:121.11400
[100]	validation_0-rmse:3.85400
[200]	validation_0-rmse:3.56400
[300]	validation_0-rmse:3.56064
[400]	validation_0-rmse:3.55960
[500]	validation_0-rmse:3.55864
[600]	validation_0-rmse:3.55829
[700]	validation_0-rmse:3.55799
[800]	validation_0-rmse:3.55771
[900]	validation_0-rmse:3.55775
[1000]	validation_0-rmse:3.55778
[1100]	validation_0-rmse:3.55779
[1200]	validation_0-rmse:3.55780
[1300]	validation_0-rmse:3.55780
[1400]	validation_0-rmse:3.55781
[1499]	validation_0-rmse:3.55782
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5204
RMSE                : 4.3811
R2                  : 0.9973
MAPE                : 0.0337
Explained Variance  : 0.9974

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.16777
[100]	validation_0-rmse:4.95986
[200]	validation_0-rmse:4.72237
[300]	validation_0-rmse:4.71163
[400]	validation_0-rmse:4.70539
[500]	validation_0-rmse:4.69686
[600]	validation_0-rmse:4.69044
[700]	validation_0-rmse:4.68892
[800]	validation_0-rmse:4.68917
[900]	validation_0-rmse:4.68995
[1000]	validation_0-rmse:4.68918
[1100]	validation_0-rmse:4.68933
[1200]	validation_0-rmse:4.68893
[1300]	validation_0-rmse:4.68902
[1400]	validation_0-rmse:4.68917
[1499]	validation_0-rmse:4.68897
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0147
RMSE                : 3.2765
R2                  : 0.9793
MAPE                : 0.0388
Explained Variance  : 0.9807

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:78.32592
[100]	validation_0-rmse:1.36791
[200]	validation_0-rmse:1.25822
[300]	validation_0-rmse:1.25738
[400]	validation_0-rmse:1.25792
[500]	validation_0-rmse:1.25627
[600]	validation_0-rmse:1.25665
[700]	validation_0-rmse:1.25596
[800]	validation_0-rmse:1.25577
[900]	validation_0-rmse:1.25598
[1000]	validation_0-rmse:1.25613
[1100]	validation_0-rmse:1.25602
[1200]	validation_0-rmse:1.25619
[1300]	validation_0-rmse:1.25630
[1400]	validation_0-rmse:1.25621
[1499]	validation_0-rmse:1.25634
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0713
RMSE                : 2.5839
R2                  : 0.9977
MAPE                : 0.0283
Explained Variance  : 0.9977

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0713
   RMSE: 2.5839
   R2: 0.9977
   MAPE: 0.0283
   Explained Variance: 0.9977

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-13 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-13 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-14 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-15 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-16 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 33.2 - 43.8 AQI
ğŸ“Š Forecast average: 36.9 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-14 02:26:46 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-14 02:26:46,684 INFO: Closing external client and cleaning up certificates.
Connection closed.
