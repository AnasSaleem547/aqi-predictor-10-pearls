ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-03 02:27:20 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-03 02:27:22 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_03_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-03 02:27:22,246 INFO: Initializing external client
2025-12-03 02:27:22,246 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-03 02:27:22,811 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Reading data from Hopsworks, using Hopsworks Feature Query Service   Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (19.43s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9926 records
ğŸ“Š Shape: (9926, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9921 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.7190
ğŸŒ² OOB Score: 0.9977
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.1144
RMSE                : 28.7659
R2                  : 0.8949
MAPE                : 0.0241
Explained Variance  : 0.9005
ğŸ† New best model found! RÂ² = 0.8949

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5367
ğŸŒ² OOB Score: 0.9994
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4742
RMSE                : 3.1542
R2                  : 0.9988
MAPE                : 0.0227
Explained Variance  : 0.9988
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.8819
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5810
RMSE                : 3.1498
R2                  : 0.9792
MAPE                : 0.0192
Explained Variance  : 0.9806

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.8678
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5851
RMSE                : 2.0565
R2                  : 0.9986
MAPE                : 0.0215
Explained Variance  : 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-02 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 170.0 - 299.7 AQI
ğŸ“ˆ Forecast average: 211.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 12_02_2025_0225_pkt
âœ… Loaded LightGBM forecasts from 12_02_2025_0225_pkt
ğŸ“Š Reference mean from 2 models: 95.05
ğŸ“ˆ Random Forest forecast mean: 211.04
ğŸ“Š Reference mean: 95.05
ğŸ“Š Ratio (RF/Reference): 2.22
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.450
ğŸ“Š Corrected mean: 95.05
ğŸ“Š Reduction: 115.99 AQI (55.0%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_03_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 1.4742
ğŸ“ˆ Best MAPE: 0.0227

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.4741619805314612, 'RMSE': 3.1542473984510675, 'R2': 0.9987771900606792, 'MAPE': 0.022676877364738227, 'Explained Variance': 0.9988257192727947}, 'randomforest_additional/12_03_2025_0227_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-03 02:28:34 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-03 02:28:34,898 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-03 02:28:34,900 INFO: Initializing external client
2025-12-03 02:28:34,900 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-03 02:28:35,422 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.53s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9926 records
ğŸ“Š Shape: (9926, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9921 records
ğŸ“Š Final dataset shape: (9921, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1588 samples
   Val:   397 samples
   Test:  1984 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 19.9857
Early stopping, best iteration is:
[120]	valid_0's l2: 19.9035
âœ… Model trained with 120 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.1537
RMSE                : 28.1127
R2                  : 0.8997
MAPE                : 0.0358
Explained Variance  : 0.9050
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 67.5497116249247
Prediction_local [162.15370437]
Right: 117.757723986641

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3176 samples
   Val:   793 samples
   Test:  1984 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 15.7885
[200]	valid_0's l2: 15.1786
[300]	valid_0's l2: 15.0973
Early stopping, best iteration is:
[264]	valid_0's l2: 15.0957
âœ… Model trained with 264 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1440
RMSE                : 3.8589
R2                  : 0.9982
MAPE                : 0.0295
Explained Variance  : 0.9982
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4763 samples
   Val:   1190 samples
   Test:  1984 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.67399
[200]	valid_0's l2: 8.44063
Early stopping, best iteration is:
[248]	valid_0's l2: 8.39326
âœ… Model trained with 248 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3724
RMSE                : 2.2860
R2                  : 0.9890
MAPE                : 0.0206
Explained Variance  : 0.9891
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6350 samples
   Val:   1587 samples
   Test:  1984 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.00129
[200]	valid_0's l2: 1.87159
Early stopping, best iteration is:
[245]	valid_0's l2: 1.84979
âœ… Model trained with 245 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4572
RMSE                : 2.8246
R2                  : 0.9973
MAPE                : 0.0365
Explained Variance  : 0.9973
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.2860

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 90.1 - 161.9 AQI
ğŸ“Š Forecast average: 119.6 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-02 22:00:00+00:00 to 2025-12-05 21:00:00+00:00
Forecast range: 90.1 - 161.9 AQI
Average forecast: 119.6 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_03_2025_0228_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_03_2025_0228_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.0318 (avg)
RMSE                : 9.2706 (avg)
R2                  : 0.9710 (avg)
MAPE                : 0.0306 (avg)
Explained Variance  : 0.9724 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-03 02:28:57 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_03_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-03 02:28:57,534 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-03 02:28:57,537 INFO: Initializing external client
2025-12-03 02:28:57,537 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-03 02:28:58,076 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.56s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9926 records
ğŸ“Š Shape: (9926, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9921 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:65.91148
[100]	validation_0-rmse:3.77172
[200]	validation_0-rmse:3.73170
[300]	validation_0-rmse:3.72950
[400]	validation_0-rmse:3.72851
[500]	validation_0-rmse:3.72831
[600]	validation_0-rmse:3.72800
[700]	validation_0-rmse:3.72804
[800]	validation_0-rmse:3.72802
[900]	validation_0-rmse:3.72798
[1000]	validation_0-rmse:3.72797
[1100]	validation_0-rmse:3.72797
[1200]	validation_0-rmse:3.72797
[1300]	validation_0-rmse:3.72797
[1400]	validation_0-rmse:3.72796
[1499]	validation_0-rmse:3.72796
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.7159
RMSE                : 26.3799
R2                  : 0.9116
MAPE                : 0.0303
Explained Variance  : 0.9177

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:119.09525
[100]	validation_0-rmse:3.62510
[200]	validation_0-rmse:3.31796
[300]	validation_0-rmse:3.31251
[400]	validation_0-rmse:3.31089
[500]	validation_0-rmse:3.31050
[600]	validation_0-rmse:3.30987
[700]	validation_0-rmse:3.30921
[800]	validation_0-rmse:3.30885
[900]	validation_0-rmse:3.30854
[1000]	validation_0-rmse:3.30838
[1100]	validation_0-rmse:3.30826
[1200]	validation_0-rmse:3.30819
[1300]	validation_0-rmse:3.30814
[1400]	validation_0-rmse:3.30812
[1499]	validation_0-rmse:3.30810
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.3615
RMSE                : 4.2095
R2                  : 0.9978
MAPE                : 0.0310
Explained Variance  : 0.9979

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.90824
[100]	validation_0-rmse:4.00730
[200]	validation_0-rmse:3.70621
[300]	validation_0-rmse:3.68395
[400]	validation_0-rmse:3.67737
[500]	validation_0-rmse:3.68054
[600]	validation_0-rmse:3.67829
[700]	validation_0-rmse:3.67869
[800]	validation_0-rmse:3.67777
[900]	validation_0-rmse:3.67901
[1000]	validation_0-rmse:3.67838
[1100]	validation_0-rmse:3.67855
[1200]	validation_0-rmse:3.67796
[1300]	validation_0-rmse:3.67810
[1400]	validation_0-rmse:3.67812
[1499]	validation_0-rmse:3.67827
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2032
RMSE                : 3.3489
R2                  : 0.9764
MAPE                : 0.0326
Explained Variance  : 0.9772

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:73.05505
[100]	validation_0-rmse:1.41749
[200]	validation_0-rmse:1.32989
[300]	validation_0-rmse:1.32468
[400]	validation_0-rmse:1.32320
[500]	validation_0-rmse:1.32328
[600]	validation_0-rmse:1.32315
[700]	validation_0-rmse:1.32315
[800]	validation_0-rmse:1.32290
[900]	validation_0-rmse:1.32273
[1000]	validation_0-rmse:1.32251
[1100]	validation_0-rmse:1.32237
[1200]	validation_0-rmse:1.32236
[1300]	validation_0-rmse:1.32251
[1400]	validation_0-rmse:1.32239
[1499]	validation_0-rmse:1.32236
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0922
RMSE                : 2.6266
R2                  : 0.9977
MAPE                : 0.0324
Explained Variance  : 0.9977

ğŸ† Best model from split 2
ğŸ“Š Best metrics:
   MAE: 2.3615
   RMSE: 4.2095
   R2: 0.9978
   MAPE: 0.0310
   Explained Variance: 0.9979

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-02 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-02 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-03 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-04 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-05 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 119.0 - 146.8 AQI
ğŸ“Š Forecast average: 128.6 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/12_03_2025_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9978
ğŸ“Š Best MAE: 2.3615
ğŸ“ˆ Best MAPE: 0.0310

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 2.3614747524261475, 'RMSE': 4.209517712004047, 'R2': 0.997822105884552, 'MAPE': 0.030986163765192032, 'Explained Variance': 0.9978779554367065}, 'xgboost_additional/12_03_2025_0228_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-03 02:29:48 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-03 02:29:48,220 INFO: Closing external client and cleaning up certificates.
Connection closed.
