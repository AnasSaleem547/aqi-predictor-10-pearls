ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-06 02:28:30 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-06 02:28:33 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_06_2026_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-06 02:28:33,327 INFO: Initializing external client
2026-01-06 02:28:33,327 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-06 02:28:34,052 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.94s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10731 records
ğŸ“Š Shape: (10731, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10726 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.2184
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.9066
RMSE                : 31.0767
R2                  : 0.8917
MAPE                : 0.0253
Explained Variance  : 0.8983
ğŸ† New best model found! RÂ² = 0.8917

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.1026
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0379
RMSE                : 3.7373
R2                  : 0.9970
MAPE                : 0.0281
Explained Variance  : 0.9972
ğŸ† New best model found! RÂ² = 0.9970

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.6624
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.3284
RMSE                : 2.7329
R2                  : 0.9850
MAPE                : 0.0587
Explained Variance  : 0.9856

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2022
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4266
RMSE                : 2.4866
R2                  : 0.9977
MAPE                : 0.0048
Explained Variance  : 0.9977
ğŸ† New best model found! RÂ² = 0.9977

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2026-01-05 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 163.0 - 285.6 AQI
ğŸ“ˆ Forecast average: 190.6 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 190.64
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.54
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.648
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 67.04 AQI (35.2%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_06_2026_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9977
ğŸ“Š Best MAE: 0.4266
ğŸ“ˆ Best MAPE: 0.0048

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.42658914812978027, 'RMSE': 2.486622960563375, 'R2': 0.9976910484604853, 'MAPE': 0.004838273662138539, 'Explained Variance': 0.9977340188853434}, 'randomforest_additional/01_06_2026_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-06 02:29:40 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-06 02:29:40,324 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-06 02:29:40,326 INFO: Initializing external client
2026-01-06 02:29:40,327 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-06 02:29:41,169 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.70s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10731 records
ğŸ“Š Shape: (10731, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10726 records
ğŸ“Š Final dataset shape: (10726, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1717 samples
   Val:   429 samples
   Test:  2145 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 217.861
[200]	valid_0's l2: 197.853
[300]	valid_0's l2: 190.37
[400]	valid_0's l2: 186.596
[500]	valid_0's l2: 184.545
Early stopping, best iteration is:
[480]	valid_0's l2: 184.535
âœ… Model trained with 480 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.8919
RMSE                : 28.7825
R2                  : 0.9071
MAPE                : 0.0379
Explained Variance  : 0.9131
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 67.27397077974925
Prediction_local [174.21762859]
Right: 220.08262724616816

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3433 samples
   Val:   858 samples
   Test:  2145 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.5136
Early stopping, best iteration is:
[96]	valid_0's l2: 17.4572
âœ… Model trained with 96 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0637
RMSE                : 3.5009
R2                  : 0.9973
MAPE                : 0.0287
Explained Variance  : 0.9974
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5149 samples
   Val:   1287 samples
   Test:  2145 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 8.51649
Early stopping, best iteration is:
[72]	valid_0's l2: 7.93933
âœ… Model trained with 72 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9394
RMSE                : 3.5266
R2                  : 0.9750
MAPE                : 0.0867
Explained Variance  : 0.9772
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6865 samples
   Val:   1716 samples
   Test:  2145 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.57432
[200]	valid_0's l2: 3.37936
Early stopping, best iteration is:
[242]	valid_0's l2: 3.33616
âœ… Model trained with 242 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5926
RMSE                : 3.4642
R2                  : 0.9955
MAPE                : 0.0148
Explained Variance  : 0.9956
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 3.4642

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 84.5 - 156.1 AQI
ğŸ“Š Forecast average: 128.3 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2026-01-05 22:00:00+00:00 to 2026-01-08 21:00:00+00:00
Forecast range: 84.5 - 156.1 AQI
Average forecast: 128.3 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_06_2026_0230_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_06_2026_0230_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3719 (avg)
RMSE                : 9.8185 (avg)
R2                  : 0.9687 (avg)
MAPE                : 0.0420 (avg)
Explained Variance  : 0.9708 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-06 02:30:03 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_06_2026_0230_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-06 02:30:03,246 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-06 02:30:03,248 INFO: Initializing external client
2026-01-06 02:30:03,248 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-06 02:30:03,925 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.86s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10731 records
ğŸ“Š Shape: (10731, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10726 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:91.20499
[100]	validation_0-rmse:13.00420
[200]	validation_0-rmse:12.77993
[300]	validation_0-rmse:12.77816
[400]	validation_0-rmse:12.77779
[500]	validation_0-rmse:12.77794
[600]	validation_0-rmse:12.77795
[700]	validation_0-rmse:12.77790
[800]	validation_0-rmse:12.77785
[900]	validation_0-rmse:12.77789
[1000]	validation_0-rmse:12.77790
[1100]	validation_0-rmse:12.77790
[1200]	validation_0-rmse:12.77790
[1300]	validation_0-rmse:12.77791
[1400]	validation_0-rmse:12.77791
[1499]	validation_0-rmse:12.77792
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.6779
RMSE                : 28.5590
R2                  : 0.9085
MAPE                : 0.0327
Explained Variance  : 0.9152

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:115.77866
[100]	validation_0-rmse:3.58507
[200]	validation_0-rmse:3.36896
[300]	validation_0-rmse:3.36705
[400]	validation_0-rmse:3.36674
[500]	validation_0-rmse:3.36703
[600]	validation_0-rmse:3.36742
[700]	validation_0-rmse:3.36770
[800]	validation_0-rmse:3.36765
[900]	validation_0-rmse:3.36786
[1000]	validation_0-rmse:3.36779
[1100]	validation_0-rmse:3.36778
[1200]	validation_0-rmse:3.36780
[1300]	validation_0-rmse:3.36783
[1400]	validation_0-rmse:3.36784
[1499]	validation_0-rmse:3.36784
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6790
RMSE                : 4.3274
R2                  : 0.9959
MAPE                : 0.0369
Explained Variance  : 0.9961

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:80.37638
[100]	validation_0-rmse:3.66864
[200]	validation_0-rmse:3.49220
[300]	validation_0-rmse:3.48291
[400]	validation_0-rmse:3.48143
[500]	validation_0-rmse:3.48059
[600]	validation_0-rmse:3.47971
[700]	validation_0-rmse:3.47750
[800]	validation_0-rmse:3.47987
[900]	validation_0-rmse:3.47687
[1000]	validation_0-rmse:3.47424
[1100]	validation_0-rmse:3.47519
[1200]	validation_0-rmse:3.47559
[1300]	validation_0-rmse:3.47574
[1400]	validation_0-rmse:3.47574
[1499]	validation_0-rmse:3.47560
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0033
RMSE                : 3.1750
R2                  : 0.9797
MAPE                : 0.0731
Explained Variance  : 0.9799

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.78664
[100]	validation_0-rmse:1.81024
[200]	validation_0-rmse:1.65319
[300]	validation_0-rmse:1.64692
[400]	validation_0-rmse:1.64572
[500]	validation_0-rmse:1.63940
[600]	validation_0-rmse:1.63709
[700]	validation_0-rmse:1.63555
[800]	validation_0-rmse:1.63705
[900]	validation_0-rmse:1.63663
[1000]	validation_0-rmse:1.63698
[1100]	validation_0-rmse:1.63784
[1200]	validation_0-rmse:1.63864
[1300]	validation_0-rmse:1.63867
[1400]	validation_0-rmse:1.63941
[1499]	validation_0-rmse:1.63934
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1922
RMSE                : 3.1036
R2                  : 0.9964
MAPE                : 0.0122
Explained Variance  : 0.9965

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1922
   RMSE: 3.1036
   R2: 0.9964
   MAPE: 0.0122
   Explained Variance: 0.9965

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2026-01-05 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-08 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 153.8 - 163.7 AQI
ğŸ“Š Forecast average: 157.0 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-06 02:31:03 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-06 02:31:03,551 INFO: Closing external client and cleaning up certificates.
Connection closed.
