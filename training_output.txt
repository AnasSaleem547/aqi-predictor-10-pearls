ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-11-28 02:25:55 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-11-28 02:25:58 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/11_28_2025_0225_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-28 02:25:58,389 INFO: Initializing external client
2025-11-28 02:25:58,389 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-28 02:25:59,033 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.89s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9808 records
ğŸ“Š Shape: (9808, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9803 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.3410
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 8.7010
RMSE                : 28.3087
R2                  : 0.8960
MAPE                : 0.0225
Explained Variance  : 0.9013
ğŸ† New best model found! RÂ² = 0.8960

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.5577
ğŸŒ² OOB Score: 0.9993
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4788
RMSE                : 3.1872
R2                  : 0.9988
MAPE                : 0.0224
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9988

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.6636
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6577
RMSE                : 3.2459
R2                  : 0.9778
MAPE                : 0.0200
Explained Variance  : 0.9794

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 0.9069
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.5689
RMSE                : 2.0587
R2                  : 0.9985
MAPE                : 0.0217
Explained Variance  : 0.9986

ğŸ“Š Step 5: Analyzing feature importance (best model from split 2)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-11-27 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 161.0 - 279.4 AQI
ğŸ“ˆ Forecast average: 190.0 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded XGBoost forecasts from 11_27_2025_0225_pkt
âœ… Loaded LightGBM forecasts from 11_27_2025_0225_pkt
ğŸ“Š Reference mean from 2 models: 160.37
ğŸ“ˆ Random Forest forecast mean: 190.05
ğŸ“Š Reference mean: 160.37
ğŸ“Š Ratio (RF/Reference): 1.19
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/11_28_2025_0225_pkt
ğŸ¯ Best RÂ² Score: 0.9988
ğŸ“Š Best MAE: 1.4788
ğŸ“ˆ Best MAPE: 0.0224

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 1.4788466611906113, 'RMSE': 3.187247369863138, 'R2': 0.9988223420206644, 'MAPE': 0.02238614280251171, 'Explained Variance': 0.998860687096327}, 'randomforest_additional/11_28_2025_0225_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-11-28 02:26:47 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-28 02:26:47,320 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-28 02:26:47,323 INFO: Initializing external client
2025-11-28 02:26:47,323 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-28 02:26:47,858 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.70s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9808 records
ğŸ“Š Shape: (9808, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9803 records
ğŸ“Š Final dataset shape: (9803, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1571 samples
   Val:   392 samples
   Test:  1960 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 40.6015
Early stopping, best iteration is:
[51]	valid_0's l2: 38.427
âœ… Model trained with 51 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 13.6631
RMSE                : 34.3274
R2                  : 0.8470
MAPE                : 0.0427
Explained Variance  : 0.8594
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 66.4095460511302
Prediction_local [161.59766119]
Right: 118.05632456681515

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3139 samples
   Val:   784 samples
   Test:  1960 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 20.2744
Early stopping, best iteration is:
[113]	valid_0's l2: 20.1902
âœ… Model trained with 113 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4051
RMSE                : 4.3165
R2                  : 0.9978
MAPE                : 0.0294
Explained Variance  : 0.9978
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4707 samples
   Val:   1176 samples
   Test:  1960 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 9.86865
[200]	valid_0's l2: 9.17472
Early stopping, best iteration is:
[169]	valid_0's l2: 9.12176
âœ… Model trained with 169 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4821
RMSE                : 2.3631
R2                  : 0.9882
MAPE                : 0.0221
Explained Variance  : 0.9884
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6275 samples
   Val:   1568 samples
   Test:  1960 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 2.32779
[200]	valid_0's l2: 2.18979
[300]	valid_0's l2: 2.17627
Early stopping, best iteration is:
[250]	valid_0's l2: 2.16897
âœ… Model trained with 250 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2337
RMSE                : 2.6722
R2                  : 0.9975
MAPE                : 0.0351
Explained Variance  : 0.9975
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 2.3631

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 102.2 - 160.3 AQI
ğŸ“Š Forecast average: 144.3 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-11-27 22:00:00+00:00 to 2025-11-30 21:00:00+00:00
Forecast range: 102.2 - 160.3 AQI
Average forecast: 144.3 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/11_28_2025_0227_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/11_28_2025_0227_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.6960 (avg)
RMSE                : 10.9198 (avg)
R2                  : 0.9576 (avg)
MAPE                : 0.0323 (avg)
Explained Variance  : 0.9608 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-11-28 02:27:08 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/11_28_2025_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-11-28 02:27:08,490 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-11-28 02:27:08,493 INFO: Initializing external client
2025-11-28 02:27:08,493 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-11-28 02:27:09,211 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.58s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 9808 records
ğŸ“Š Shape: (9808, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 9803 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:66.80415
[100]	validation_0-rmse:4.47031
[200]	validation_0-rmse:4.44781
[300]	validation_0-rmse:4.44737
[400]	validation_0-rmse:4.44688
[500]	validation_0-rmse:4.44655
[600]	validation_0-rmse:4.44643
[700]	validation_0-rmse:4.44631
[800]	validation_0-rmse:4.44623
[900]	validation_0-rmse:4.44621
[1000]	validation_0-rmse:4.44621
[1100]	validation_0-rmse:4.44619
[1200]	validation_0-rmse:4.44619
[1300]	validation_0-rmse:4.44619
[1400]	validation_0-rmse:4.44619
[1499]	validation_0-rmse:4.44619
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.1015
RMSE                : 27.0315
R2                  : 0.9051
MAPE                : 0.0311
Explained Variance  : 0.9115

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:125.62842
[100]	validation_0-rmse:4.26655
[200]	validation_0-rmse:3.92633
[300]	validation_0-rmse:3.92339
[400]	validation_0-rmse:3.92322
[500]	validation_0-rmse:3.92283
[600]	validation_0-rmse:3.92309
[700]	validation_0-rmse:3.92322
[800]	validation_0-rmse:3.92328
[900]	validation_0-rmse:3.92335
[1000]	validation_0-rmse:3.92340
[1100]	validation_0-rmse:3.92352
[1200]	validation_0-rmse:3.92354
[1300]	validation_0-rmse:3.92357
[1400]	validation_0-rmse:3.92358
[1499]	validation_0-rmse:3.92359
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5122
RMSE                : 4.7715
R2                  : 0.9974
MAPE                : 0.0307
Explained Variance  : 0.9974

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:86.53496
[100]	validation_0-rmse:4.11613
[200]	validation_0-rmse:3.91749
[300]	validation_0-rmse:3.91969
[400]	validation_0-rmse:3.90691
[500]	validation_0-rmse:3.91254
[600]	validation_0-rmse:3.91560
[700]	validation_0-rmse:3.91848
[800]	validation_0-rmse:3.92044
[900]	validation_0-rmse:3.92089
[1000]	validation_0-rmse:3.92097
[1100]	validation_0-rmse:3.92223
[1200]	validation_0-rmse:3.92274
[1300]	validation_0-rmse:3.92246
[1400]	validation_0-rmse:3.92262
[1499]	validation_0-rmse:3.92282
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4102
RMSE                : 3.7528
R2                  : 0.9703
MAPE                : 0.0338
Explained Variance  : 0.9741

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:71.90282
[100]	validation_0-rmse:1.36132
[200]	validation_0-rmse:1.25554
[300]	validation_0-rmse:1.25590
[400]	validation_0-rmse:1.25680
[500]	validation_0-rmse:1.25476
[600]	validation_0-rmse:1.25542
[700]	validation_0-rmse:1.25543
[800]	validation_0-rmse:1.25501
[900]	validation_0-rmse:1.25484
[1000]	validation_0-rmse:1.25477
[1100]	validation_0-rmse:1.25495
[1200]	validation_0-rmse:1.25470
[1300]	validation_0-rmse:1.25469
[1400]	validation_0-rmse:1.25459
[1499]	validation_0-rmse:1.25458
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0570
RMSE                : 2.5012
R2                  : 0.9978
MAPE                : 0.0324
Explained Variance  : 0.9978

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0570
   RMSE: 2.5012
   R2: 0.9978
   MAPE: 0.0324
   Explained Variance: 0.9978

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-11-27 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-27 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-28 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-29 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-11-30 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 146.1 - 160.1 AQI
ğŸ“Š Forecast average: 153.9 AQI

ğŸ’¾ Step 8: Saving results...

âœ… XGBoost Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: xgboost_additional/11_28_2025_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9978
ğŸ“Š Best MAE: 1.0570
ğŸ“ˆ Best MAPE: 0.0324

âœ… XGBoost training completed successfully!
   Result: (XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric='rmse', feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=8,
             max_leaves=None, min_child_weight=None, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=1500,
             n_jobs=-1, num_parallel_tree=None, ...), {'MAE': 1.0569614171981812, 'RMSE': 2.5012092526770906, 'R2': 0.9978028535842896, 'MAPE': 0.03244936838746071, 'Explained Variance': 0.9978028535842896}, 'xgboost_additional/11_28_2025_0227_pkt')

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-11-28 02:28:04 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-11-28 02:28:04,680 INFO: Closing external client and cleaning up certificates.
Connection closed.
