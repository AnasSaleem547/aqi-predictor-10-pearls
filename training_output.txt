ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-05 02:27:00 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-05 02:27:03 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_05_2026_0227_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-05 02:27:03,136 INFO: Initializing external client
2026-01-05 02:27:03,136 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-05 02:27:04,141 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.06s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10707 records
ğŸ“Š Shape: (10707, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10702 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.1636
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.8681
RMSE                : 30.9291
R2                  : 0.8923
MAPE                : 0.0252
Explained Variance  : 0.8989
ğŸ† New best model found! RÂ² = 0.8923

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.0890
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0135
RMSE                : 3.7054
R2                  : 0.9970
MAPE                : 0.0279
Explained Variance  : 0.9972
ğŸ† New best model found! RÂ² = 0.9970

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.6688
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2885
RMSE                : 2.7133
R2                  : 0.9854
MAPE                : 0.0579
Explained Variance  : 0.9861

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2494
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4533
RMSE                : 2.5132
R2                  : 0.9977
MAPE                : 0.0052
Explained Variance  : 0.9977
ğŸ† New best model found! RÂ² = 0.9977

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2026-01-04 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 160.0 - 286.3 AQI
ğŸ“ˆ Forecast average: 200.4 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 200.39
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.62
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.617
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 76.80 AQI (38.3%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_05_2026_0227_pkt
ğŸ¯ Best RÂ² Score: 0.9977
ğŸ“Š Best MAE: 0.4533
ğŸ“ˆ Best MAPE: 0.0052

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.45333356024168125, 'RMSE': 2.51321352666262, 'R2': 0.9976822961809058, 'MAPE': 0.005220804052799884, 'Explained Variance': 0.9977276272922007}, 'randomforest_additional/01_05_2026_0227_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-05 02:28:13 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-05 02:28:13,782 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-05 02:28:13,785 INFO: Initializing external client
2026-01-05 02:28:13,785 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-05 02:28:14,659 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.80s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10707 records
ğŸ“Š Shape: (10707, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10702 records
ğŸ“Š Final dataset shape: (10702, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1714 samples
   Val:   428 samples
   Test:  2140 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 214.876
[200]	valid_0's l2: 193.56
[300]	valid_0's l2: 185.657
[400]	valid_0's l2: 182.294
[500]	valid_0's l2: 180.62
Early stopping, best iteration is:
[475]	valid_0's l2: 180.62
âœ… Model trained with 475 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.7019
RMSE                : 28.6561
R2                  : 0.9076
MAPE                : 0.0371
Explained Variance  : 0.9129
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 71.40284725759116
Prediction_local [163.87136983]
Right: 140.86563090771386

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3426 samples
   Val:   856 samples
   Test:  2140 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 17.5292
Early stopping, best iteration is:
[135]	valid_0's l2: 17.2482
âœ… Model trained with 135 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1116
RMSE                : 3.5834
R2                  : 0.9972
MAPE                : 0.0287
Explained Variance  : 0.9972
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5138 samples
   Val:   1284 samples
   Test:  2140 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 8.75829
Early stopping, best iteration is:
[77]	valid_0's l2: 8.19757
âœ… Model trained with 77 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8565
RMSE                : 3.4015
R2                  : 0.9771
MAPE                : 0.0834
Explained Variance  : 0.9789
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6850 samples
   Val:   1712 samples
   Test:  2140 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.42222
[200]	valid_0's l2: 3.2756
Early stopping, best iteration is:
[239]	valid_0's l2: 3.22962
âœ… Model trained with 239 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.6443
RMSE                : 3.6496
R2                  : 0.9951
MAPE                : 0.0150
Explained Variance  : 0.9953
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.4015

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 83.0 - 121.7 AQI
ğŸ“Š Forecast average: 114.5 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2026-01-04 22:00:00+00:00 to 2026-01-07 21:00:00+00:00
Forecast range: 83.0 - 121.7 AQI
Average forecast: 114.5 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_05_2026_0228_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_05_2026_0228_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3286 (avg)
RMSE                : 9.8226 (avg)
R2                  : 0.9692 (avg)
MAPE                : 0.0410 (avg)
Explained Variance  : 0.9711 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-05 02:28:37 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_05_2026_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-05 02:28:37,330 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-05 02:28:37,332 INFO: Initializing external client
2026-01-05 02:28:37,332 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-05 02:28:38,161 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.82s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10707 records
ğŸ“Š Shape: (10707, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10702 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:91.89208
[100]	validation_0-rmse:13.19418
[200]	validation_0-rmse:12.97878
[300]	validation_0-rmse:12.97749
[400]	validation_0-rmse:12.97553
[500]	validation_0-rmse:12.97483
[600]	validation_0-rmse:12.97459
[700]	validation_0-rmse:12.97450
[800]	validation_0-rmse:12.97448
[900]	validation_0-rmse:12.97449
[1000]	validation_0-rmse:12.97451
[1100]	validation_0-rmse:12.97450
[1200]	validation_0-rmse:12.97452
[1300]	validation_0-rmse:12.97451
[1400]	validation_0-rmse:12.97450
[1499]	validation_0-rmse:12.97451
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.6208
RMSE                : 28.5714
R2                  : 0.9081
MAPE                : 0.0323
Explained Variance  : 0.9152

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:116.02522
[100]	validation_0-rmse:3.44197
[200]	validation_0-rmse:3.23396
[300]	validation_0-rmse:3.23175
[400]	validation_0-rmse:3.23197
[500]	validation_0-rmse:3.23305
[600]	validation_0-rmse:3.23386
[700]	validation_0-rmse:3.23392
[800]	validation_0-rmse:3.23405
[900]	validation_0-rmse:3.23413
[1000]	validation_0-rmse:3.23425
[1100]	validation_0-rmse:3.23418
[1200]	validation_0-rmse:3.23421
[1300]	validation_0-rmse:3.23423
[1400]	validation_0-rmse:3.23422
[1499]	validation_0-rmse:3.23421
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5798
RMSE                : 4.2031
R2                  : 0.9962
MAPE                : 0.0352
Explained Variance  : 0.9964

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:80.88581
[100]	validation_0-rmse:3.59010
[200]	validation_0-rmse:3.43534
[300]	validation_0-rmse:3.43192
[400]	validation_0-rmse:3.43182
[500]	validation_0-rmse:3.44127
[600]	validation_0-rmse:3.44252
[700]	validation_0-rmse:3.44551
[800]	validation_0-rmse:3.44429
[900]	validation_0-rmse:3.44497
[1000]	validation_0-rmse:3.44495
[1100]	validation_0-rmse:3.44474
[1200]	validation_0-rmse:3.44528
[1300]	validation_0-rmse:3.44565
[1400]	validation_0-rmse:3.44596
[1499]	validation_0-rmse:3.44612
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8478
RMSE                : 3.0338
R2                  : 0.9817
MAPE                : 0.0676
Explained Variance  : 0.9823

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.60953
[100]	validation_0-rmse:1.80989
[200]	validation_0-rmse:1.65554
[300]	validation_0-rmse:1.64542
[400]	validation_0-rmse:1.63947
[500]	validation_0-rmse:1.64047
[600]	validation_0-rmse:1.64157
[700]	validation_0-rmse:1.64073
[800]	validation_0-rmse:1.64088
[900]	validation_0-rmse:1.64173
[1000]	validation_0-rmse:1.64199
[1100]	validation_0-rmse:1.64327
[1200]	validation_0-rmse:1.64356
[1300]	validation_0-rmse:1.64389
[1400]	validation_0-rmse:1.64437
[1499]	validation_0-rmse:1.64490
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.1363
RMSE                : 2.9858
R2                  : 0.9967
MAPE                : 0.0117
Explained Variance  : 0.9968

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.1363
   RMSE: 2.9858
   R2: 0.9967
   MAPE: 0.0117
   Explained Variance: 0.9968

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2026-01-04 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-04 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-05 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-06 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-07 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 80.4 - 117.7 AQI
ğŸ“Š Forecast average: 98.4 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-05 02:29:37 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-05 02:29:37,735 INFO: Closing external client and cleaning up certificates.
Connection closed.
