ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-22 02:24:55 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-22 02:24:58 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_22_2025_0224_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-22 02:24:58,235 INFO: Initializing external client
2025-12-22 02:24:58,235 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-22 02:24:59,021 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.93s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10376 records
ğŸ“Š Shape: (10376, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10371 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.2156
ğŸŒ² OOB Score: 0.9978
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.8238
RMSE                : 30.5966
R2                  : 0.8882
MAPE                : 0.0249
Explained Variance  : 0.8949
ğŸ† New best model found! RÂ² = 0.8882

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.6289
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8945
RMSE                : 3.7270
R2                  : 0.9974
MAPE                : 0.0269
Explained Variance  : 0.9976
ğŸ† New best model found! RÂ² = 0.9974

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5295
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5635
RMSE                : 3.1202
R2                  : 0.9838
MAPE                : 0.0594
Explained Variance  : 0.9856

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2431
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4211
RMSE                : 1.8993
R2                  : 0.9989
MAPE                : 0.0057
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9989

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-21 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 44.3 - 290.0 AQI
ğŸ“ˆ Forecast average: 194.9 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_21_2025_0225_pkt
ğŸ“Š Reference mean from 1 models: 221.33
ğŸ“ˆ Random Forest forecast mean: 194.86
ğŸ“Š Reference mean: 221.33
ğŸ“Š Ratio (RF/Reference): 0.88
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_22_2025_0224_pkt
ğŸ¯ Best RÂ² Score: 0.9989
ğŸ“Š Best MAE: 0.4211
ğŸ“ˆ Best MAPE: 0.0057

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.42107604654934777, 'RMSE': 1.8993001919915875, 'R2': 0.9989121202153206, 'MAPE': 0.00574409437844965, 'Explained Variance': 0.9989320120806444}, 'randomforest_additional/12_22_2025_0224_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-22 02:26:05 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-22 02:26:05,163 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-22 02:26:05,165 INFO: Initializing external client
2025-12-22 02:26:05,165 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-22 02:26:05,984 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Reading data from Hopsworks, using Hopsworks Feature Query Service...   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.01s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10376 records
ğŸ“Š Shape: (10376, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10371 records
ğŸ“Š Final dataset shape: (10371, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1660 samples
   Val:   415 samples
   Test:  2074 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 206.763
[200]	valid_0's l2: 183.425
[300]	valid_0's l2: 174.938
[400]	valid_0's l2: 171.83
[500]	valid_0's l2: 170.571
Early stopping, best iteration is:
[461]	valid_0's l2: 170.571
âœ… Model trained with 461 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.7871
RMSE                : 28.4208
R2                  : 0.9036
MAPE                : 0.0373
Explained Variance  : 0.9090
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 65.75004962335781
Prediction_local [174.31258805]
Right: 326.4787159162214

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3320 samples
   Val:   829 samples
   Test:  2074 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 13.545
[200]	valid_0's l2: 13.2327
[300]	valid_0's l2: 13.1854
Early stopping, best iteration is:
[253]	valid_0's l2: 13.1669
âœ… Model trained with 253 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9509
RMSE                : 3.6001
R2                  : 0.9976
MAPE                : 0.0265
Explained Variance  : 0.9976
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 4979 samples
   Val:   1244 samples
   Test:  2074 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 11.552
[200]	valid_0's l2: 10.2416
Early stopping, best iteration is:
[222]	valid_0's l2: 10.1821
âœ… Model trained with 222 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7340
RMSE                : 3.2646
R2                  : 0.9822
MAPE                : 0.0758
Explained Variance  : 0.9832
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6638 samples
   Val:   1659 samples
   Test:  2074 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.15029
[200]	valid_0's l2: 3.05391
Early stopping, best iteration is:
[247]	valid_0's l2: 3.03334
âœ… Model trained with 247 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4503
RMSE                : 3.1817
R2                  : 0.9969
MAPE                : 0.0155
Explained Variance  : 0.9970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 3.1817

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 63.6 - 108.0 AQI
ğŸ“Š Forecast average: 105.8 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-21 22:00:00+00:00 to 2025-12-24 21:00:00+00:00
Forecast range: 63.6 - 108.0 AQI
Average forecast: 105.8 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_22_2025_0226_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_22_2025_0226_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.2306 (avg)
RMSE                : 9.6168 (avg)
R2                  : 0.9701 (avg)
MAPE                : 0.0388 (avg)
Explained Variance  : 0.9717 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-22 02:26:31 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_22_2025_0226_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-22 02:26:31,384 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-22 02:26:31,387 INFO: Initializing external client
2025-12-22 02:26:31,387 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-22 02:26:32,073 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.92s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10376 records
ğŸ“Š Shape: (10376, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10371 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 1 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.88264
[100]	validation_0-rmse:13.09087
[200]	validation_0-rmse:12.87970
[300]	validation_0-rmse:12.87686
[400]	validation_0-rmse:12.87567
[500]	validation_0-rmse:12.87511
[600]	validation_0-rmse:12.87463
[700]	validation_0-rmse:12.87461
[800]	validation_0-rmse:12.87461
[900]	validation_0-rmse:12.87462
[1000]	validation_0-rmse:12.87462
[1100]	validation_0-rmse:12.87462
[1200]	validation_0-rmse:12.87462
[1300]	validation_0-rmse:12.87461
[1400]	validation_0-rmse:12.87462
[1499]	validation_0-rmse:12.87462
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.7302
RMSE                : 28.3666
R2                  : 0.9039
MAPE                : 0.0325
Explained Variance  : 0.9106

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:119.36212
[100]	validation_0-rmse:3.39529
[200]	validation_0-rmse:3.17170
[300]	validation_0-rmse:3.17005
[400]	validation_0-rmse:3.16883
[500]	validation_0-rmse:3.16838
[600]	validation_0-rmse:3.16868
[700]	validation_0-rmse:3.16838
[800]	validation_0-rmse:3.16798
[900]	validation_0-rmse:3.16810
[1000]	validation_0-rmse:3.16807
[1100]	validation_0-rmse:3.16808
[1200]	validation_0-rmse:3.16804
[1300]	validation_0-rmse:3.16801
[1400]	validation_0-rmse:3.16797
[1499]	validation_0-rmse:3.16796
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.4216
RMSE                : 4.1441
R2                  : 0.9968
MAPE                : 0.0333
Explained Variance  : 0.9970

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.91975
[100]	validation_0-rmse:4.88445
[200]	validation_0-rmse:4.66845
[300]	validation_0-rmse:4.62155
[400]	validation_0-rmse:4.61783
[500]	validation_0-rmse:4.62126
[600]	validation_0-rmse:4.61886
[700]	validation_0-rmse:4.61772
[800]	validation_0-rmse:4.61798
[900]	validation_0-rmse:4.61849
[1000]	validation_0-rmse:4.61678
[1100]	validation_0-rmse:4.61620
[1200]	validation_0-rmse:4.61575
[1300]	validation_0-rmse:4.61568
[1400]	validation_0-rmse:4.61545
[1499]	validation_0-rmse:4.61546
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.2678
RMSE                : 3.6131
R2                  : 0.9782
MAPE                : 0.0732
Explained Variance  : 0.9803

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:82.63319
[100]	validation_0-rmse:1.74414
[200]	validation_0-rmse:1.59498
[300]	validation_0-rmse:1.59325
[400]	validation_0-rmse:1.59363
[500]	validation_0-rmse:1.59352
[600]	validation_0-rmse:1.59388
[700]	validation_0-rmse:1.59458
[800]	validation_0-rmse:1.59537
[900]	validation_0-rmse:1.59649
[1000]	validation_0-rmse:1.59661
[1100]	validation_0-rmse:1.59682
[1200]	validation_0-rmse:1.59671
[1300]	validation_0-rmse:1.59647
[1400]	validation_0-rmse:1.59627
[1499]	validation_0-rmse:1.59610
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.0780
RMSE                : 2.5774
R2                  : 0.9980
MAPE                : 0.0133
Explained Variance  : 0.9980

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.0780
   RMSE: 2.5774
   R2: 0.9980
   MAPE: 0.0133
   Explained Variance: 0.9980

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-21 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-21 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-22 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-23 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-24 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 17.6 - 26.7 AQI
ğŸ“Š Forecast average: 21.4 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-22 02:27:30 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-22 02:27:30,687 INFO: Closing external client and cleaning up certificates.
Connection closed.
