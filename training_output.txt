ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2025-12-27 02:28:37 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2025-12-27 02:28:39 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/12_27_2025_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-27 02:28:39,859 INFO: Initializing external client
2025-12-27 02:28:39,859 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-27 02:28:40,666 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.69s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10493 records
ğŸ“Š Shape: (10493, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10488 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 14.2863
ğŸŒ² OOB Score: 0.9979
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.6873
RMSE                : 30.3923
R2                  : 0.8914
MAPE                : 0.0249
Explained Variance  : 0.8979
ğŸ† New best model found! RÂ² = 0.8914

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.0858
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.8325
RMSE                : 3.5822
R2                  : 0.9976
MAPE                : 0.0261
Explained Variance  : 0.9977
ğŸ† New best model found! RÂ² = 0.9976

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 4.5176
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5914
RMSE                : 3.1551
R2                  : 0.9837
MAPE                : 0.0598
Explained Variance  : 0.9851

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.2752
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.3967
RMSE                : 1.8693
R2                  : 0.9989
MAPE                : 0.0051
Explained Variance  : 0.9989
ğŸ† New best model found! RÂ² = 0.9989

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2025-12-26 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 151.1 - 279.0 AQI
ğŸ“ˆ Forecast average: 188.9 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_25_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 144.48
ğŸ“ˆ Random Forest forecast mean: 188.92
ğŸ“Š Reference mean: 144.48
ğŸ“Š Ratio (RF/Reference): 1.31
âœ… No bias correction needed

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/12_27_2025_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9989
ğŸ“Š Best MAE: 0.3967
ğŸ“ˆ Best MAPE: 0.0051

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.39673339792857193, 'RMSE': 1.869293890768643, 'R2': 0.9988860950975244, 'MAPE': 0.005073926082417493, 'Explained Variance': 0.9989107037072693}, 'randomforest_additional/12_27_2025_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2025-12-27 02:29:45 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-27 02:29:45,786 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-27 02:29:45,788 INFO: Initializing external client
2025-12-27 02:29:45,789 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-27 02:29:46,376 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.87s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10493 records
ğŸ“Š Shape: (10493, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10488 records
ğŸ“Š Final dataset shape: (10488, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1680 samples
   Val:   420 samples
   Test:  2097 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 211.623
[200]	valid_0's l2: 187.362
[300]	valid_0's l2: 178.965
[400]	valid_0's l2: 175.249
[500]	valid_0's l2: 173.877
Early stopping, best iteration is:
[487]	valid_0's l2: 173.863
âœ… Model trained with 487 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.5764
RMSE                : 28.3122
R2                  : 0.9058
MAPE                : 0.0367
Explained Variance  : 0.9113
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 66.60582138674809
Prediction_local [174.74237043]
Right: 323.9179960172027

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3358 samples
   Val:   839 samples
   Test:  2097 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 18.0465
[200]	valid_0's l2: 17.9558
Early stopping, best iteration is:
[156]	valid_0's l2: 17.9117
âœ… Model trained with 156 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0278
RMSE                : 3.5367
R2                  : 0.9976
MAPE                : 0.0276
Explained Variance  : 0.9976
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5036 samples
   Val:   1258 samples
   Test:  2097 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 15.8684
Early stopping, best iteration is:
[69]	valid_0's l2: 10.5654
âœ… Model trained with 69 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0906
RMSE                : 3.7507
R2                  : 0.9769
MAPE                : 0.0883
Explained Variance  : 0.9801
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6713 samples
   Val:   1678 samples
   Test:  2097 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.4776
[200]	valid_0's l2: 3.30484
Early stopping, best iteration is:
[218]	valid_0's l2: 3.29905
âœ… Model trained with 218 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.5072
RMSE                : 3.3394
R2                  : 0.9964
MAPE                : 0.0150
Explained Variance  : 0.9965
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 4 with RMSE: 3.3394

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 58.4 - 91.0 AQI
ğŸ“Š Forecast average: 84.4 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2025-12-26 22:00:00+00:00 to 2025-12-29 21:00:00+00:00
Forecast range: 58.4 - 91.0 AQI
Average forecast: 84.4 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/12_27_2025_0230_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/12_27_2025_0230_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3005 (avg)
RMSE                : 9.7348 (avg)
R2                  : 0.9692 (avg)
MAPE                : 0.0419 (avg)
Explained Variance  : 0.9714 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2025-12-27 02:30:08 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/12_27_2025_0230_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2025-12-27 02:30:08,760 INFO: Closing external client and cleaning up certificates.
Connection closed.
2025-12-27 02:30:08,762 INFO: Initializing external client
2025-12-27 02:30:08,763 INFO: Base URL: https://c.app.hopsworks.ai:443
2025-12-27 02:30:09,364 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.61s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10493 records
ğŸ“Š Shape: (10493, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10488 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 3 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:89.79416
[100]	validation_0-rmse:13.10372
[200]	validation_0-rmse:12.90043
[300]	validation_0-rmse:12.89734
[400]	validation_0-rmse:12.89625
[500]	validation_0-rmse:12.89635
[600]	validation_0-rmse:12.89666
[700]	validation_0-rmse:12.89664
[800]	validation_0-rmse:12.89666
[900]	validation_0-rmse:12.89667
[1000]	validation_0-rmse:12.89661
[1100]	validation_0-rmse:12.89660
[1200]	validation_0-rmse:12.89660
[1300]	validation_0-rmse:12.89660
[1400]	validation_0-rmse:12.89660
[1499]	validation_0-rmse:12.89660
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.4471
RMSE                : 27.9161
R2                  : 0.9084
MAPE                : 0.0323
Explained Variance  : 0.9150

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:114.51036
[100]	validation_0-rmse:3.64348
[200]	validation_0-rmse:3.40589
[300]	validation_0-rmse:3.40437
[400]	validation_0-rmse:3.40417
[500]	validation_0-rmse:3.40453
[600]	validation_0-rmse:3.40515
[700]	validation_0-rmse:3.40535
[800]	validation_0-rmse:3.40556
[900]	validation_0-rmse:3.40572
[1000]	validation_0-rmse:3.40575
[1100]	validation_0-rmse:3.40577
[1200]	validation_0-rmse:3.40582
[1300]	validation_0-rmse:3.40585
[1400]	validation_0-rmse:3.40587
[1499]	validation_0-rmse:3.40589
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.5615
RMSE                : 4.2657
R2                  : 0.9966
MAPE                : 0.0349
Explained Variance  : 0.9968

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:84.15357
[100]	validation_0-rmse:5.94819
[200]	validation_0-rmse:5.61196
[300]	validation_0-rmse:5.58762
[400]	validation_0-rmse:5.57461
[500]	validation_0-rmse:5.57615
[600]	validation_0-rmse:5.57004
[700]	validation_0-rmse:5.56365
[800]	validation_0-rmse:5.56269
[900]	validation_0-rmse:5.56082
[1000]	validation_0-rmse:5.55976
[1100]	validation_0-rmse:5.56107
[1200]	validation_0-rmse:5.56048
[1300]	validation_0-rmse:5.56087
[1400]	validation_0-rmse:5.56106
[1499]	validation_0-rmse:5.56092
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 3.2841
RMSE                : 4.7577
R2                  : 0.9629
MAPE                : 0.0906
Explained Variance  : 0.9711

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:83.17762
[100]	validation_0-rmse:1.87721
[200]	validation_0-rmse:1.73748
[300]	validation_0-rmse:1.73469
[400]	validation_0-rmse:1.73477
[500]	validation_0-rmse:1.73693
[600]	validation_0-rmse:1.73994
[700]	validation_0-rmse:1.73993
[800]	validation_0-rmse:1.74111
[900]	validation_0-rmse:1.74084
[1000]	validation_0-rmse:1.74085
[1100]	validation_0-rmse:1.74066
[1200]	validation_0-rmse:1.74024
[1300]	validation_0-rmse:1.74062
[1400]	validation_0-rmse:1.74052
[1499]	validation_0-rmse:1.74087
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2096
RMSE                : 2.8274
R2                  : 0.9975
MAPE                : 0.0137
Explained Variance  : 0.9975

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.2096
   RMSE: 2.8274
   R2: 0.9975
   MAPE: 0.0137
   Explained Variance: 0.9975

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2025-12-26 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-26 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-27 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 21:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 22:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-28 23:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 00:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 01:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 02:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 03:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 04:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 05:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 06:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 07:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 08:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 09:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 10:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 11:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 12:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 13:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 14:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 15:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 16:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 17:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 18:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 19:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 20:00:00+00:00...
   ğŸ“… Forecasting for 2025-12-29 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 56.0 - 74.4 AQI
ğŸ“Š Forecast average: 64.3 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2025-12-27 02:31:07 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2025-12-27 02:31:07,985 INFO: Closing external client and cleaning up certificates.
Connection closed.
