ğŸ¯ Unified AQI Model Training Pipeline
ğŸ• Pipeline started at: 2026-01-10 02:28:47 PKT
======================================================================

============================================================
ğŸš€ Training RandomForest Model
â° Started at: 2026-01-10 02:28:50 PKT
============================================================
ğŸš€ Starting AQI Random Forest Training Pipeline...
============================================================
ğŸ“ Output directory: randomforest_additional/01_10_2026_0228_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-10 02:28:50,086 INFO: Initializing external client
2026-01-10 02:28:50,086 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-10 02:28:50,811 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.81s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10827 records
ğŸ“Š Shape: (10827, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10822 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training Random Forest models...

ğŸ“ Training on split 1/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 13.8672
ğŸŒ² OOB Score: 0.9981
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 9.5304
RMSE                : 30.3971
R2                  : 0.8968
MAPE                : 0.0242
Explained Variance  : 0.9033
ğŸ† New best model found! RÂ² = 0.8968

ğŸ“ Training on split 2/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 2.1057
ğŸŒ² OOB Score: 0.9995
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.0204
RMSE                : 3.6892
R2                  : 0.9969
MAPE                : 0.0280
Explained Variance  : 0.9971
ğŸ† New best model found! RÂ² = 0.9969

ğŸ“ Training on split 3/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 3.5941
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.4026
RMSE                : 2.7807
R2                  : 0.9844
MAPE                : 0.0630
Explained Variance  : 0.9854

ğŸ“ Training on split 4/4
ğŸ¤– Training Random Forest model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: n_estimators=500, max_depth=25, min_samples_split=6
âœ… Model trained successfully
ğŸ“Š Validation RMSE: 1.1479
ğŸŒ² OOB Score: 0.9997
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 0.4091
RMSE                : 2.5048
R2                  : 0.9974
MAPE                : 0.0043
Explained Variance  : 0.9974
ğŸ† New best model found! RÂ² = 0.9974

ğŸ“Š Step 5: Analyzing feature importance (best model from split 4)...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: SHAP analysis...
ğŸ” Performing SHAP analysis...

ğŸ” Step 7: LIME analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 8: Generating 3-day AQI forecast...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
ğŸ“Š Starting forecast from: 2026-01-09 21:00:00+00:00
ğŸ¯ Forecasting 72 hours ahead...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“Š Forecast range: 161.0 - 284.1 AQI
ğŸ“ˆ Forecast average: 214.4 AQI

ğŸ”§ Step 8b: Applying bias correction...
âœ… Loaded LightGBM forecasts from 12_31_2025_0228_pkt
ğŸ“Š Reference mean from 1 models: 123.59
ğŸ“ˆ Random Forest forecast mean: 214.42
ğŸ“Š Reference mean: 123.59
ğŸ“Š Ratio (RF/Reference): 1.73
âœ… Applied bias correction!
ğŸ“Š Correction factor: 0.576
ğŸ“Š Corrected mean: 123.59
ğŸ“Š Reduction: 90.82 AQI (42.4%)

ğŸ’¾ Step 9: Saving results...

âœ… Random Forest Training Pipeline Completed Successfully!
============================================================
ğŸ“ All results saved to: randomforest_additional/01_10_2026_0228_pkt
ğŸ¯ Best RÂ² Score: 0.9974
ğŸ“Š Best MAE: 0.4091
ğŸ“ˆ Best MAPE: 0.0043

âœ… RandomForest training completed successfully!
   Result: (RandomForestRegressor(max_depth=25, max_features=None, min_samples_leaf=2,
                      min_samples_split=6, n_estimators=500, n_jobs=-1,
                      oob_score=True, random_state=42), {'MAE': 0.40913342705507777, 'RMSE': 2.5048424200330373, 'R2': 0.9973942766570458, 'MAPE': 0.004320362148553592, 'Explained Variance': 0.9974486617587961}, 'randomforest_additional/01_10_2026_0228_pkt')

============================================================
ğŸš€ Training LightGBM Model
â° Started at: 2026-01-10 02:30:00 PKT
============================================================
ğŸš€ Starting AQI LGBM Training Pipeline
============================================================
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-10 02:30:00,560 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-10 02:30:00,562 INFO: Initializing external client
2026-01-10 02:30:00,562 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-10 02:30:01,210 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.76s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10827 records
ğŸ“Š Shape: (10827, 25)
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10822 records
ğŸ“Š Final dataset shape: (10822, 22)
ğŸ¯ Target variable range: 9.0 - 500.0
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ” Processing split 1/4
----------------------------------------
   Train: 1733 samples
   Val:   433 samples
   Test:  2164 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 213.303
[200]	valid_0's l2: 188.005
[300]	valid_0's l2: 179.533
[400]	valid_0's l2: 175.281
[500]	valid_0's l2: 174.379
Early stopping, best iteration is:
[459]	valid_0's l2: 174.049
âœ… Model trained with 459 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 11.5949
RMSE                : 28.6335
R2                  : 0.9084
MAPE                : 0.0367
Explained Variance  : 0.9143
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis...
Intercept 95.84922265054652
Prediction_local [91.01044342]
Right: 97.64846011159332

ğŸ” Processing split 2/4
----------------------------------------
   Train: 3464 samples
   Val:   866 samples
   Test:  2164 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 16.1451
Early stopping, best iteration is:
[117]	valid_0's l2: 15.9549
âœ… Model trained with 117 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.1158
RMSE                : 3.6544
R2                  : 0.9970
MAPE                : 0.0292
Explained Variance  : 0.9970
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 3/4
----------------------------------------
   Train: 5196 samples
   Val:   1298 samples
   Test:  2164 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 7.34516
[200]	valid_0's l2: 7.01016
[300]	valid_0's l2: 6.83046
Early stopping, best iteration is:
[252]	valid_0's l2: 6.82529
âœ… Model trained with 252 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.7113
RMSE                : 3.1345
R2                  : 0.9802
MAPE                : 0.0772
Explained Variance  : 0.9808
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ” Processing split 4/4
----------------------------------------
   Train: 6927 samples
   Val:   1731 samples
   Test:  2164 samples
ğŸ¤– Training LightGBM model...
âœ… Using optimized hyperparameters
Training until validation scores don't improve for 50 rounds
[100]	valid_0's l2: 3.4238
[200]	valid_0's l2: 3.22988
[300]	valid_0's l2: 3.17675
Early stopping, best iteration is:
[254]	valid_0's l2: 3.17675
âœ… Model trained with 254 iterations
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9734
RMSE                : 3.8703
R2                  : 0.9938
MAPE                : 0.0170
Explained Variance  : 0.9941
ğŸ“Š Plotting feature importance...
ğŸ” Performing SHAP analysis...

ğŸ† Best model from split 3 with RMSE: 3.1345

ğŸŒ¤ï¸  Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
âœ… Generated 72 hourly forecasts for 3 days
ğŸ“ˆ Forecast range: 84.1 - 130.9 AQI
ğŸ“Š Forecast average: 102.5 AQI

ğŸ“Š Forecast Summary:
========================================
Forecast period: 2026-01-09 22:00:00+00:00 to 2026-01-12 21:00:00+00:00
Forecast range: 84.1 - 130.9 AQI
Average forecast: 102.5 AQI
ğŸ’¾ Saving model and results...
âœ… Forecasts saved to lgbm_additional/01_10_2026_0230_pkt/aqi_3_day_forecast.csv
âœ… Model and results saved successfully to lgbm_additional/01_10_2026_0230_pkt

ğŸ“Š Overall Cross-Validation Results:
==================================================
MAE                 : 4.3488 (avg)
RMSE                : 9.8232 (avg)
R2                  : 0.9698 (avg)
MAPE                : 0.0400 (avg)
Explained Variance  : 0.9716 (avg)

ğŸ‰ Training pipeline completed successfully!

âœ… LightGBM training completed successfully!
   Result: None

============================================================
ğŸš€ Training XGBoost Model
â° Started at: 2026-01-10 02:30:25 PKT
============================================================
ğŸš€ Starting AQI XGBoost Training Pipeline...
============================================================
ğŸ“ Output directory: xgboost_additional/01_10_2026_0230_pkt

ğŸ“Š Step 1: Fetching data from Hopsworks...
ğŸ“ Attempting to load data from RandomForest CSV first...
âŒ No RandomForest CSV files found, trying general CSV files...
ğŸ”— Connecting to Hopsworks project: anassale
2026-01-10 02:30:25,901 INFO: Closing external client and cleaning up certificates.
Connection closed.
2026-01-10 02:30:25,903 INFO: Initializing external client
2026-01-10 02:30:25,904 INFO: Base URL: https://c.app.hopsworks.ai:443
2026-01-10 02:30:26,450 INFO: Python Engine initialized.

Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1263764
âœ… Successfully connected to Hopsworks
âœ… Latest version of karachifeatures10: 1
ğŸ“¥ Retrieving features from: karachifeatures10 (v1)
ğŸ“‹ Retrieving all records...
Reading data from Hopsworks, using Hopsworks Feature Query Service.   Reading data from Hopsworks, using Hopsworks Feature Query Service..   Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.75s) 
ğŸ”„ Sorting data by datetime for chronological order...
âœ… Successfully retrieved 10827 records
ğŸ“Š Shape: (10827, 25)

ğŸ”§ Step 2: Preprocessing data...
ğŸ”§ Preprocessing data...
   ğŸ§¹ Handling missing values...
   ğŸ“Š Ensuring engineered features are properly calculated...
   â• Creating lag features for pollutants...
   â• Creating lag features for pm2_5_nowcast...
   â• Creating lag features for pm10_nowcast...
   â• Creating lag features for co_ppm_8hr_avg...
   â• Creating lag features for no2_ppb...
   âœ… After preprocessing: 10822 records

ğŸ“Š Step 3: Creating time series splits...
ğŸ“Š Creating time series splits...
âš ï¸ Skipping split - insufficient training data: 2 samples
âœ… Created 4 time series splits

ğŸ¤– Step 4: Training XGBoost models...

ğŸ“ˆ Training split 1/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:89.49635
[100]	validation_0-rmse:12.78462
[200]	validation_0-rmse:12.55478
[300]	validation_0-rmse:12.55383
[400]	validation_0-rmse:12.55305
[500]	validation_0-rmse:12.55238
[600]	validation_0-rmse:12.55196
[700]	validation_0-rmse:12.55180
[800]	validation_0-rmse:12.55166
[900]	validation_0-rmse:12.55158
[1000]	validation_0-rmse:12.55153
[1100]	validation_0-rmse:12.55153
[1200]	validation_0-rmse:12.55153
[1300]	validation_0-rmse:12.55152
[1400]	validation_0-rmse:12.55151
[1499]	validation_0-rmse:12.55151
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 10.4927
RMSE                : 28.0052
R2                  : 0.9124
MAPE                : 0.0322
Explained Variance  : 0.9191

ğŸ“ˆ Training split 2/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:114.00059
[100]	validation_0-rmse:3.63672
[200]	validation_0-rmse:3.43085
[300]	validation_0-rmse:3.42898
[400]	validation_0-rmse:3.43033
[500]	validation_0-rmse:3.42928
[600]	validation_0-rmse:3.42967
[700]	validation_0-rmse:3.43001
[800]	validation_0-rmse:3.42979
[900]	validation_0-rmse:3.42997
[1000]	validation_0-rmse:3.42990
[1100]	validation_0-rmse:3.42993
[1200]	validation_0-rmse:3.42994
[1300]	validation_0-rmse:3.42999
[1400]	validation_0-rmse:3.42998
[1499]	validation_0-rmse:3.42998
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 2.6692
RMSE                : 4.3489
R2                  : 0.9957
MAPE                : 0.0370
Explained Variance  : 0.9960

ğŸ“ˆ Training split 3/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:78.71732
[100]	validation_0-rmse:3.65407
[200]	validation_0-rmse:3.44438
[300]	validation_0-rmse:3.44152
[400]	validation_0-rmse:3.43865
[500]	validation_0-rmse:3.43822
[600]	validation_0-rmse:3.44101
[700]	validation_0-rmse:3.44356
[800]	validation_0-rmse:3.43961
[900]	validation_0-rmse:3.43884
[1000]	validation_0-rmse:3.43946
[1100]	validation_0-rmse:3.43932
[1200]	validation_0-rmse:3.43955
[1300]	validation_0-rmse:3.43905
[1400]	validation_0-rmse:3.43886
[1499]	validation_0-rmse:3.43851
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.9133
RMSE                : 3.3004
R2                  : 0.9780
MAPE                : 0.0770
Explained Variance  : 0.9787

ğŸ“ˆ Training split 4/4...
ğŸ¤– Training XGBoost model...
âœ… Using optimized hyperparameters from Optuna
ğŸ¯ Key params: max_depth=8, learning_rate=0.050, n_estimators=1500
[0]	validation_0-rmse:85.95241
[100]	validation_0-rmse:1.78632
[200]	validation_0-rmse:1.63009
[300]	validation_0-rmse:1.63280
[400]	validation_0-rmse:1.63685
[500]	validation_0-rmse:1.64238
[600]	validation_0-rmse:1.64209
[700]	validation_0-rmse:1.64231
[800]	validation_0-rmse:1.64201
[900]	validation_0-rmse:1.64168
[1000]	validation_0-rmse:1.64284
[1100]	validation_0-rmse:1.64229
[1200]	validation_0-rmse:1.64248
[1300]	validation_0-rmse:1.64294
[1400]	validation_0-rmse:1.64295
[1499]	validation_0-rmse:1.64345
Traceback (most recent call last):
  File "/home/runner/work/aqi-predictor-10-pearls/aqi-predictor-10-pearls/aqi_xgboost_training_pipeline.py", line 896, in main
    best_model.save_model(model_path)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1113, in save_model
    meta["_estimator_type"] = self._get_type()
                              ^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/xgboost/sklearn.py", line 1104, in _get_type
    raise TypeError(
TypeError: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.
âœ… Model trained successfully
ğŸ“Š Evaluating model performance...

ğŸ“ˆ Model Performance Metrics:
==================================================
MAE                 : 1.2826
RMSE                : 3.1748
R2                  : 0.9958
MAPE                : 0.0126
Explained Variance  : 0.9959

ğŸ† Best model from split 4
ğŸ“Š Best metrics:
   MAE: 1.2826
   RMSE: 3.1748
   R2: 0.9958
   MAPE: 0.0126
   Explained Variance: 0.9959

ğŸ“Š Step 5: Feature importance analysis...
ğŸ“Š Plotting feature importance...

ğŸ” Step 6: Model interpretability analysis...
ğŸ” Performing SHAP analysis...
ğŸ” Performing LIME analysis (fast mode)...

ğŸ”® Step 7: Generating 3-day AQI forecasts...
ğŸ”® Generating 3-day AQI forecast using recursive approach...
   ğŸ“… Forecasting for 2026-01-09 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-09 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-10 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 21:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 22:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-11 23:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 00:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 01:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 02:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 03:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 04:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 05:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 06:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 07:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 08:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 09:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 10:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 11:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 12:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 13:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 14:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 15:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 16:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 17:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 18:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 19:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 20:00:00+00:00...
   ğŸ“… Forecasting for 2026-01-12 21:00:00+00:00...
âœ… Generated 72 hourly forecasts for 3 days
ï¿½ Forecast range: 80.5 - 100.5 AQI
ğŸ“Š Forecast average: 89.4 AQI

ğŸ’¾ Step 8: Saving results...

âŒ Error in training pipeline: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.

âœ… XGBoost training completed successfully!
   Result: (None, None, None)

======================================================================
ğŸ“Š TRAINING SUMMARY
======================================================================
â° Completed at: 2026-01-10 02:31:27 PKT
âœ… Successful: 3/3 models
   âœ… RandomForest
   âœ… LightGBM
   âœ… XGBoost

ğŸ‰ All models trained successfully!
2026-01-10 02:31:27,051 INFO: Closing external client and cleaning up certificates.
Connection closed.
